From 6f987a98117fa6beb9c53dacbff1df99d3486683 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 2 Jan 2024 11:55:53 -0600
Subject: [PATCH 01/22] vulkan: Add a vk_get_subgroup_size() helper

No reason to duplicate this logic between pipelines and shader objects.

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_pipeline.c | 48 ++++++++++++++++++++------------
 src/vulkan/runtime/vk_pipeline.h |  7 +++++
 2 files changed, 37 insertions(+), 18 deletions(-)

diff --git a/src/vulkan/runtime/vk_pipeline.c b/src/vulkan/runtime/vk_pipeline.c
index 50a87e13a3c73..a1fa7745fdede 100644
--- a/src/vulkan/runtime/vk_pipeline.c
+++ b/src/vulkan/runtime/vk_pipeline.c
@@ -76,14 +76,37 @@ get_builtin_nir(const VkPipelineShaderStageCreateInfo *info)
 }
 
 static uint32_t
-get_required_subgroup_size(const VkPipelineShaderStageCreateInfo *info)
+get_required_subgroup_size(const void *info_pNext)
 {
    const VkPipelineShaderStageRequiredSubgroupSizeCreateInfo *rss_info =
-      vk_find_struct_const(info->pNext,
+      vk_find_struct_const(info_pNext,
                            PIPELINE_SHADER_STAGE_REQUIRED_SUBGROUP_SIZE_CREATE_INFO);
    return rss_info != NULL ? rss_info->requiredSubgroupSize : 0;
 }
 
+enum gl_subgroup_size
+vk_get_subgroup_size(uint32_t spirv_version,
+                     gl_shader_stage stage,
+                     const void *info_pNext,
+                     bool allow_varying,
+                     bool require_full)
+{
+   uint32_t req_subgroup_size = get_required_subgroup_size(info_pNext);
+   if (req_subgroup_size > 0) {
+      assert(util_is_power_of_two_nonzero(req_subgroup_size));
+      assert(req_subgroup_size >= 8 && req_subgroup_size <= 128);
+      return req_subgroup_size;
+   } else if (allow_varying || spirv_version >= 0x10600) {
+      /* Starting with SPIR-V 1.6, varying subgroup size the default */
+      return SUBGROUP_SIZE_VARYING;
+   } else if (require_full) {
+      assert(stage == MESA_SHADER_COMPUTE);
+      return SUBGROUP_SIZE_FULL_SUBGROUPS;
+   } else {
+      return SUBGROUP_SIZE_API_CONSTANT;
+   }
+}
+
 VkResult
 vk_pipeline_shader_stage_to_nir(struct vk_device *device,
                                 const VkPipelineShaderStageCreateInfo *info,
@@ -127,22 +150,11 @@ vk_pipeline_shader_stage_to_nir(struct vk_device *device,
       spirv_size = minfo->codeSize;
    }
 
-   enum gl_subgroup_size subgroup_size;
-   uint32_t req_subgroup_size = get_required_subgroup_size(info);
-   if (req_subgroup_size > 0) {
-      assert(util_is_power_of_two_nonzero(req_subgroup_size));
-      assert(req_subgroup_size >= 8 && req_subgroup_size <= 128);
-      subgroup_size = req_subgroup_size;
-   } else if (info->flags & VK_PIPELINE_SHADER_STAGE_CREATE_ALLOW_VARYING_SUBGROUP_SIZE_BIT ||
-              vk_spirv_version(spirv_data, spirv_size) >= 0x10600) {
-      /* Starting with SPIR-V 1.6, varying subgroup size the default */
-      subgroup_size = SUBGROUP_SIZE_VARYING;
-   } else if (info->flags & VK_PIPELINE_SHADER_STAGE_CREATE_REQUIRE_FULL_SUBGROUPS_BIT) {
-      assert(stage == MESA_SHADER_COMPUTE);
-      subgroup_size = SUBGROUP_SIZE_FULL_SUBGROUPS;
-   } else {
-      subgroup_size = SUBGROUP_SIZE_API_CONSTANT;
-   }
+   enum gl_subgroup_size subgroup_size = vk_get_subgroup_size(
+      vk_spirv_version(spirv_data, spirv_size),
+      stage, info->pNext,
+      info->flags & VK_PIPELINE_SHADER_STAGE_CREATE_ALLOW_VARYING_SUBGROUP_SIZE_BIT,
+      info->flags & VK_PIPELINE_SHADER_STAGE_CREATE_REQUIRE_FULL_SUBGROUPS_BIT);
 
    nir_shader *nir = vk_spirv_to_nir(device, spirv_data, spirv_size, stage,
                                      info->pName, subgroup_size,
diff --git a/src/vulkan/runtime/vk_pipeline.h b/src/vulkan/runtime/vk_pipeline.h
index 1ca32a1428ee8..b003e41442662 100644
--- a/src/vulkan/runtime/vk_pipeline.h
+++ b/src/vulkan/runtime/vk_pipeline.h
@@ -60,6 +60,13 @@ vk_pipeline_shader_stage_to_nir(struct vk_device *device,
                                 const struct nir_shader_compiler_options *nir_options,
                                 void *mem_ctx, struct nir_shader **nir_out);
 
+enum gl_subgroup_size
+vk_get_subgroup_size(uint32_t spirv_version,
+                     gl_shader_stage stage,
+                     const void *info_pNext,
+                     bool allow_varying,
+                     bool require_full);
+
 struct vk_pipeline_robustness_state {
    VkPipelineRobustnessBufferBehaviorEXT storage_buffers;
    VkPipelineRobustnessBufferBehaviorEXT uniform_buffers;
-- 
GitLab


From 6d58dc84ca99a2a9a26560ab49e3c66f0afcaa98 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 2 Jan 2024 16:48:18 -0600
Subject: [PATCH 02/22] vulkan: Move the descriptor set limit to vk_limits.h

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_limits.h          | 2 ++
 src/vulkan/runtime/vk_pipeline_layout.c | 2 +-
 src/vulkan/runtime/vk_pipeline_layout.h | 5 ++---
 3 files changed, 5 insertions(+), 4 deletions(-)

diff --git a/src/vulkan/runtime/vk_limits.h b/src/vulkan/runtime/vk_limits.h
index 8294b764975a1..e58e02fa95f04 100644
--- a/src/vulkan/runtime/vk_limits.h
+++ b/src/vulkan/runtime/vk_limits.h
@@ -24,6 +24,8 @@
 #ifndef VK_LIMITS_H
 #define VK_LIMITS_H
 
+#define MESA_VK_MAX_DESCRIPTOR_SETS 32
+
 #define MESA_VK_MAX_VERTEX_BINDINGS 32
 #define MESA_VK_MAX_VERTEX_ATTRIBUTES 32
 
diff --git a/src/vulkan/runtime/vk_pipeline_layout.c b/src/vulkan/runtime/vk_pipeline_layout.c
index df2311ccb8a82..3a04e7dc52a8f 100644
--- a/src/vulkan/runtime/vk_pipeline_layout.c
+++ b/src/vulkan/runtime/vk_pipeline_layout.c
@@ -37,7 +37,7 @@ vk_pipeline_layout_init(struct vk_device *device,
                         const VkPipelineLayoutCreateInfo *pCreateInfo)
 {
    assert(pCreateInfo->sType == VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO);
-   assert(pCreateInfo->setLayoutCount <= VK_MESA_PIPELINE_LAYOUT_MAX_SETS);
+   assert(pCreateInfo->setLayoutCount <= MESA_VK_MAX_DESCRIPTOR_SETS);
 
    vk_object_base_init(device, &layout->base, VK_OBJECT_TYPE_PIPELINE_LAYOUT);
 
diff --git a/src/vulkan/runtime/vk_pipeline_layout.h b/src/vulkan/runtime/vk_pipeline_layout.h
index 17fde98379517..4bffdc906c7f4 100644
--- a/src/vulkan/runtime/vk_pipeline_layout.h
+++ b/src/vulkan/runtime/vk_pipeline_layout.h
@@ -23,6 +23,7 @@
 #ifndef VK_PIPELINE_LAYOUT_H
 #define VK_PIPELINE_LAYOUT_H
 
+#include "vk_limits.h"
 #include "vk_object.h"
 
 #include "util/u_atomic.h"
@@ -33,8 +34,6 @@ extern "C" {
 
 struct vk_descriptor_set_layout;
 
-#define VK_MESA_PIPELINE_LAYOUT_MAX_SETS 32
-
 struct vk_pipeline_layout {
    struct vk_object_base base;
 
@@ -61,7 +60,7 @@ struct vk_pipeline_layout {
    uint32_t set_count;
 
    /** Array of pointers to descriptor set layouts, indexed by set index */
-   struct vk_descriptor_set_layout *set_layouts[VK_MESA_PIPELINE_LAYOUT_MAX_SETS];
+   struct vk_descriptor_set_layout *set_layouts[MESA_VK_MAX_DESCRIPTOR_SETS];
 
    /** Destroy callback
     *
-- 
GitLab


From a47a7b82ed6d726b5c9a92124e5541d66e9f6d61 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Thu, 5 Oct 2023 16:24:36 +0200
Subject: [PATCH 03/22] vk/graphics_state: Remove bogus assert in
 CmdSetSampleMaskEXT

We're supposed to just ignore samples above what we support, and there's
no VU matching this assert. Fixes a crash in
dEQP-VK.pipeline.shader_object_unlinked_spirv.extended_dynamic_state.misc.sample_shading_dynamic_sample_count.

Reviewed-by: Faith Ekstrand <faith.ekstrand@collabora.com>
Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_graphics_state.c | 1 -
 1 file changed, 1 deletion(-)

diff --git a/src/vulkan/runtime/vk_graphics_state.c b/src/vulkan/runtime/vk_graphics_state.c
index db2bd178e7b2c..c8ea2545a151c 100644
--- a/src/vulkan/runtime/vk_graphics_state.c
+++ b/src/vulkan/runtime/vk_graphics_state.c
@@ -2646,7 +2646,6 @@ vk_common_CmdSetSampleMaskEXT(VkCommandBuffer commandBuffer,
    VK_FROM_HANDLE(vk_command_buffer, cmd, commandBuffer);
    struct vk_dynamic_graphics_state *dyn = &cmd->dynamic_graphics_state;
 
-   assert(samples <= MESA_VK_MAX_SAMPLES);
    VkSampleMask sample_mask = *pSampleMask & BITFIELD_MASK(MESA_VK_MAX_SAMPLES);
 
    SET_DYN_VALUE(dyn, MS_SAMPLE_MASK, ms.sample_mask, sample_mask);
-- 
GitLab


From 9b65b740862e441361e1cd86a7e09b7fa649549f Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Thu, 12 Oct 2023 13:03:50 +0200
Subject: [PATCH 04/22] vk/graphics_state: Add stubs required by
 VK_EXT_shader_objects

Because these functions were introduced by VK_EXT_shader_objects, we
technically have to expose them even though they have to do with NV
extensions that no one else supports.

Reviewed-by: Faith Ekstrand <faith.ekstrand@collabora.com>
Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_graphics_state.c | 75 ++++++++++++++++++++++++++
 1 file changed, 75 insertions(+)

diff --git a/src/vulkan/runtime/vk_graphics_state.c b/src/vulkan/runtime/vk_graphics_state.c
index c8ea2545a151c..0efcb1a5bde06 100644
--- a/src/vulkan/runtime/vk_graphics_state.c
+++ b/src/vulkan/runtime/vk_graphics_state.c
@@ -3122,6 +3122,81 @@ vk_common_CmdSetRenderingInputAttachmentIndicesKHR(
    SET_DYN_VALUE(dyn, INPUT_ATTACHMENT_MAP, ial.stencil_att, stencil_att);
 }
 
+/* These are stubs required by VK_EXT_shader_object */
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetViewportWScalingEnableNV(
+    VkCommandBuffer                             commandBuffer,
+    VkBool32                                    viewportWScalingEnable)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetCoverageReductionModeNV(
+    VkCommandBuffer                             commandBuffer,
+    VkCoverageReductionModeNV                   coverageReductionMode)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetCoverageToColorEnableNV(
+    VkCommandBuffer                             commandBuffer,
+    VkBool32                                    coverageToColorEnable)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetCoverageToColorLocationNV(
+    VkCommandBuffer                             commandBuffer,
+    uint32_t                                    coverageToColorLocation)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetCoverageModulationModeNV(
+    VkCommandBuffer                             commandBuffer,
+    VkCoverageModulationModeNV                  coverageModulationMode)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetCoverageModulationTableEnableNV(
+    VkCommandBuffer                             commandBuffer,
+    VkBool32                                    coverageModulationTableEnable)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetCoverageModulationTableNV(
+    VkCommandBuffer                             commandBuffer,
+    uint32_t                                    coverageModulationTableCount,
+    const float*                                pCoverageModulationTable)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetRepresentativeFragmentTestEnableNV(
+    VkCommandBuffer                             commandBuffer,
+    VkBool32                                    representativeFragmentTestEnable)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetShadingRateImageEnableNV(
+    VkCommandBuffer                             commandBuffer,
+    VkBool32                                    shadingRateImageEnable)
+{
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdSetViewportSwizzleNV(
+    VkCommandBuffer                             commandBuffer,
+    uint32_t                                    firstViewport,
+    uint32_t                                    viewportCount,
+    const VkViewportSwizzleNV*                  pViewportSwizzles)
+{
+}
+
 const char *
 vk_dynamic_graphic_state_to_str(enum mesa_vk_dynamic_graphics_state state)
 {
-- 
GitLab


From 0435ba56d5b770fc7d201cf43ec767cb12e0e059 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 2 Jan 2024 10:41:11 -0600
Subject: [PATCH 05/22] vulkan: Add runtime code for VK_EXT_shader_object

This adds a new base vk_shader object along with vtables for creating,
binding, and working with shader objects.

Unlike other parts of the runtime, the new shader object code is a bit
more sanitized and opinionated than just handing you the Vulkan
entrypoints.  For one thing, the create_shaders() calback takes a NIR
shader, not SPIR-V.  Conversion of SPIR-V into NIR, handling of magic
meta NIR shaders, etc. is all done in common code.  [De]serialization is
done via `struct blob` and the common code does a checksum of the binary
and handles rejecting invalid binaries based on shaderBinaryUUID and
shaderBinaryVersion.  This should make life a bit easier for driver
authors as well as provides a bit nicer interface for building the
common pipeline implementation on top of shader objects.

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/meson.build |   2 +
 src/vulkan/runtime/vk_device.h |   4 +
 src/vulkan/runtime/vk_shader.c | 561 +++++++++++++++++++++++++++++++++
 src/vulkan/runtime/vk_shader.h | 184 +++++++++++
 4 files changed, 751 insertions(+)
 create mode 100644 src/vulkan/runtime/vk_shader.c
 create mode 100644 src/vulkan/runtime/vk_shader.h

diff --git a/src/vulkan/runtime/meson.build b/src/vulkan/runtime/meson.build
index 6bfa1b499b1b9..29b17f7737f78 100644
--- a/src/vulkan/runtime/meson.build
+++ b/src/vulkan/runtime/meson.build
@@ -96,6 +96,8 @@ vulkan_runtime_files = files(
   'vk_sampler.h',
   'vk_semaphore.c',
   'vk_semaphore.h',
+  'vk_shader.c',
+  'vk_shader.h',
   'vk_shader_module.c',
   'vk_shader_module.h',
   'vk_standard_sample_locations.c',
diff --git a/src/vulkan/runtime/vk_device.h b/src/vulkan/runtime/vk_device.h
index 79ddc81f68c77..5bdd3a3ddd8e3 100644
--- a/src/vulkan/runtime/vk_device.h
+++ b/src/vulkan/runtime/vk_device.h
@@ -38,6 +38,7 @@ extern "C" {
 #endif
 
 struct vk_command_buffer_ops;
+struct vk_device_shader_ops;
 struct vk_sync;
 
 enum vk_queue_submit_mode {
@@ -130,6 +131,9 @@ struct vk_device {
    /** Command buffer vtable when using the common command pool */
    const struct vk_command_buffer_ops *command_buffer_ops;
 
+   /** Shader vtable for VK_EXT_shader_object and common pipelines */
+   const struct vk_device_shader_ops *shader_ops;
+
    /** Driver provided callback for capturing traces
     * 
     * Triggers for this callback are:
diff --git a/src/vulkan/runtime/vk_shader.c b/src/vulkan/runtime/vk_shader.c
new file mode 100644
index 0000000000000..54187b425a0a5
--- /dev/null
+++ b/src/vulkan/runtime/vk_shader.c
@@ -0,0 +1,561 @@
+/*
+ * Copyright © 2024 Collabora, Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#include "vk_shader.h"
+
+#include "vk_alloc.h"
+#include "vk_command_buffer.h"
+#include "vk_common_entrypoints.h"
+#include "vk_descriptor_set_layout.h"
+#include "vk_device.h"
+#include "vk_nir.h"
+#include "vk_physical_device.h"
+#include "vk_pipeline.h"
+
+#include "util/mesa-sha1.h"
+
+void *
+vk_shader_zalloc(struct vk_device *device,
+                 const struct vk_shader_ops *ops,
+                 gl_shader_stage stage,
+                 const VkAllocationCallbacks *alloc,
+                 size_t size)
+{
+   /* For internal allocations, we need to allocate from the device scope
+    * because they might be put in pipeline caches.  Importantly, it is
+    * impossible for the client to get at this pointer and we apply this
+    * heuristic before we account for allocation fallbacks so this will only
+    * ever happen for internal shader objectx.
+    */
+   const VkSystemAllocationScope alloc_scope =
+      alloc == &device->alloc ? VK_SYSTEM_ALLOCATION_SCOPE_DEVICE
+                              : VK_SYSTEM_ALLOCATION_SCOPE_OBJECT;
+
+   struct vk_shader *shader = vk_zalloc2(&device->alloc, alloc, size, 8,
+                                         alloc_scope);
+   if (shader == NULL)
+      return NULL;
+
+   vk_object_base_init(device, &shader->base, VK_OBJECT_TYPE_SHADER_EXT);
+   shader->ops = ops;
+   shader->stage = stage;
+
+   return shader;
+}
+
+void
+vk_shader_free(struct vk_device *device,
+               const VkAllocationCallbacks *alloc,
+               struct vk_shader *shader)
+{
+   vk_object_base_finish(&shader->base);
+   vk_free2(&device->alloc, alloc, shader);
+}
+
+int
+vk_shader_cmp_graphics_stages(gl_shader_stage a, gl_shader_stage b)
+{
+   static const int stage_order[MESA_SHADER_MESH + 1] = {
+      [MESA_SHADER_VERTEX] = 1,
+      [MESA_SHADER_TESS_CTRL] = 2,
+      [MESA_SHADER_TESS_EVAL] = 3,
+      [MESA_SHADER_GEOMETRY] = 4,
+      [MESA_SHADER_TASK] = 5,
+      [MESA_SHADER_MESH] = 6,
+      [MESA_SHADER_FRAGMENT] = 7,
+   };
+
+   assert(a < ARRAY_SIZE(stage_order) && stage_order[a] > 0);
+   assert(b < ARRAY_SIZE(stage_order) && stage_order[b] > 0);
+
+   return stage_order[a] - stage_order[b];
+}
+
+struct stage_idx {
+   gl_shader_stage stage;
+   uint32_t idx;
+};
+
+static int
+cmp_stage_idx(const void *_a, const void *_b)
+{
+   const struct stage_idx *a = _a, *b = _b;
+   return vk_shader_cmp_graphics_stages(a->stage, b->stage);
+}
+
+static nir_shader *
+vk_shader_to_nir(struct vk_device *device,
+                 const VkShaderCreateInfoEXT *info,
+                 const struct vk_pipeline_robustness_state *rs)
+{
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+
+   gl_shader_stage stage = vk_to_mesa_shader_stage(info->stage);
+   const nir_shader_compiler_options *nir_options =
+      ops->get_nir_options(device->physical, stage, rs);
+   struct spirv_to_nir_options spirv_options =
+      ops->get_spirv_options(device->physical, stage, rs);
+
+   enum gl_subgroup_size subgroup_size = vk_get_subgroup_size(
+      vk_spirv_version(info->pCode, info->codeSize),
+      stage, info->pNext,
+      info->flags & VK_SHADER_CREATE_ALLOW_VARYING_SUBGROUP_SIZE_BIT_EXT,
+      info->flags &VK_SHADER_CREATE_REQUIRE_FULL_SUBGROUPS_BIT_EXT);
+
+   nir_shader *nir = vk_spirv_to_nir(device,
+                                     info->pCode, info->codeSize,
+                                     stage, info->pName,
+                                     subgroup_size,
+                                     info->pSpecializationInfo,
+                                     &spirv_options, nir_options,
+                                     false /* internal */, NULL);
+   if (nir == NULL)
+      return NULL;
+
+   if (ops->preprocess_nir != NULL)
+      ops->preprocess_nir(device->physical, nir);
+
+   return nir;
+}
+
+struct set_layouts {
+   struct vk_descriptor_set_layout *set_layouts[MESA_VK_MAX_DESCRIPTOR_SETS];
+};
+
+static void
+vk_shader_compile_info_init(struct vk_shader_compile_info *info,
+                            struct set_layouts *set_layouts,
+                            const VkShaderCreateInfoEXT *vk_info,
+                            const struct vk_pipeline_robustness_state *rs,
+                            nir_shader *nir)
+{
+   for (uint32_t sl = 0; sl < vk_info->setLayoutCount; sl++) {
+      set_layouts->set_layouts[sl] =
+         vk_descriptor_set_layout_from_handle(vk_info->pSetLayouts[sl]);
+   }
+
+   *info = (struct vk_shader_compile_info) {
+      .stage = nir->info.stage,
+      .flags = vk_info->flags,
+      .next_stage_mask = vk_info->nextStage,
+      .nir = nir,
+      .robustness = rs,
+      .set_layout_count = vk_info->setLayoutCount,
+      .set_layouts = set_layouts->set_layouts,
+      .push_constant_range_count = vk_info->pushConstantRangeCount,
+      .push_constant_ranges = vk_info->pPushConstantRanges,
+   };
+}
+
+struct vk_shader_bin_header {
+   char mesavkshaderbin[16];
+   VkDriverId driver_id;
+   uint8_t uuid[VK_UUID_SIZE];
+   uint32_t version;
+   uint64_t size;
+   uint8_t sha1[SHA1_DIGEST_LENGTH];
+   uint32_t _pad;
+};
+static_assert(sizeof(struct vk_shader_bin_header) == 72,
+              "This struct has no holes");
+
+static void
+vk_shader_bin_header_init(struct vk_shader_bin_header *header,
+                          struct vk_physical_device *device)
+{
+   *header = (struct vk_shader_bin_header) {
+      .mesavkshaderbin = "MesaVkShaderBin",
+      .driver_id = device->properties.driverID,
+   };
+
+   memcpy(header->uuid, device->properties.shaderBinaryUUID, VK_UUID_SIZE);
+   header->version = device->properties.shaderBinaryVersion;
+}
+
+static VkResult
+vk_shader_serialize(struct vk_device *device,
+                    struct vk_shader *shader,
+                    struct blob *blob)
+{
+   struct vk_shader_bin_header header;
+   vk_shader_bin_header_init(&header, device->physical);
+
+   ASSERTED intptr_t header_offset = blob_reserve_bytes(blob, sizeof(header));
+   assert(header_offset == 0);
+
+   bool success = shader->ops->serialize(device, shader, blob);
+   if (!success || blob->out_of_memory)
+      return VK_INCOMPLETE;
+
+   /* Finalize and write the header */
+   header.size = blob->size;
+   if (blob->data != NULL) {
+      assert(sizeof(header) <= blob->size);
+
+      struct mesa_sha1 sha1_ctx;
+      _mesa_sha1_init(&sha1_ctx);
+
+      /* Hash the header with a zero SHA1 */
+      _mesa_sha1_update(&sha1_ctx, &header, sizeof(header));
+
+      /* Hash the serialized data */
+      _mesa_sha1_update(&sha1_ctx, blob->data + sizeof(header),
+                        blob->size - sizeof(header));
+
+      _mesa_sha1_final(&sha1_ctx, header.sha1);
+
+      blob_overwrite_bytes(blob, header_offset, &header, sizeof(header));
+   }
+
+   return VK_SUCCESS;
+}
+
+static VkResult
+vk_shader_deserialize(struct vk_device *device,
+                      size_t data_size, const void *data,
+                      const VkAllocationCallbacks* pAllocator,
+                      struct vk_shader **shader_out)
+{
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+
+   struct blob_reader blob;
+   blob_reader_init(&blob, data, data_size);
+
+   struct vk_shader_bin_header header, ref_header;
+   blob_copy_bytes(&blob, &header, sizeof(header));
+   if (blob.overrun)
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   vk_shader_bin_header_init(&ref_header, device->physical);
+
+   if (memcmp(header.mesavkshaderbin, ref_header.mesavkshaderbin,
+              sizeof(header.mesavkshaderbin)))
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   if (header.driver_id != ref_header.driver_id)
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   if (memcmp(header.uuid, ref_header.uuid, sizeof(header.uuid)))
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   /* From the Vulkan 1.3.276 spec:
+    *
+    *    "Guaranteed compatibility of shader binaries is expressed through a
+    *    combination of the shaderBinaryUUID and shaderBinaryVersion members
+    *    of the VkPhysicalDeviceShaderObjectPropertiesEXT structure queried
+    *    from a physical device. Binary shaders retrieved from a physical
+    *    device with a certain shaderBinaryUUID are guaranteed to be
+    *    compatible with all other physical devices reporting the same
+    *    shaderBinaryUUID and the same or higher shaderBinaryVersion."
+    *
+    * We handle the version check here on behalf of the driver and then pass
+    * the version into the driver's deserialize callback.
+    *
+    * If a driver doesn't want to mess with versions, they can always make the
+    * UUID a hash and always report version 0 and that will make this check
+    * effectively a no-op.
+    */
+   if (header.version > ref_header.version)
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   /* Reject shader binaries that are the wrong size. */
+   if (header.size != data_size)
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   assert(blob.current == (uint8_t *)data + sizeof(header));
+   blob.end = (uint8_t *)data + data_size;
+
+   struct mesa_sha1 sha1_ctx;
+   _mesa_sha1_init(&sha1_ctx);
+
+   /* Hash the header with a zero SHA1 */
+   struct vk_shader_bin_header sha1_header = header;
+   memset(sha1_header.sha1, 0, sizeof(sha1_header.sha1));
+   _mesa_sha1_update(&sha1_ctx, &sha1_header, sizeof(sha1_header));
+
+   /* Hash the serialized data */
+   _mesa_sha1_update(&sha1_ctx, (uint8_t *)data + sizeof(header),
+                     data_size - sizeof(header));
+
+   _mesa_sha1_final(&sha1_ctx, ref_header.sha1);
+   if (memcmp(header.sha1, ref_header.sha1, sizeof(header.sha1)))
+      return vk_error(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   /* We've now verified that the header matches and that the data has the
+    * right SHA1 hash so it's safe to call into the driver.
+    */
+   return ops->deserialize(device, &blob, header.version,
+                           pAllocator, shader_out);
+}
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_GetShaderBinaryDataEXT(VkDevice _device,
+                                 VkShaderEXT _shader,
+                                 size_t *pDataSize,
+                                 void *pData)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_shader, shader, _shader);
+   VkResult result;
+
+   /* From the Vulkan 1.3.275 spec:
+    *
+    *    "If pData is NULL, then the size of the binary shader code of the
+    *    shader object, in bytes, is returned in pDataSize. Otherwise,
+    *    pDataSize must point to a variable set by the user to the size of the
+    *    buffer, in bytes, pointed to by pData, and on return the variable is
+    *    overwritten with the amount of data actually written to pData. If
+    *    pDataSize is less than the size of the binary shader code, nothing is
+    *    written to pData, and VK_INCOMPLETE will be returned instead of
+    *    VK_SUCCESS."
+    *
+    * This is annoying.  Unlike basically every other Vulkan data return
+    * method, we're not allowed to overwrite the client-provided memory region
+    * on VK_INCOMPLETE.  This means we either need to query the blob size
+    * up-front by serializing twice or we need to serialize into temporary
+    * memory and memcpy into the client-provided region.  We choose the first
+    * approach.
+    *
+    * In the common case, this means that vk_shader_ops::serialize will get
+    * called 3 times: Once for the client to get the size, once for us to
+    * validate the client's size, and once to actually write the data.  It's a
+    * bit heavy-weight but this shouldn't be in a hot path and this is better
+    * for memory efficiency.  Also, the vk_shader_ops::serialize should be
+    * pretty fast on a null blob.
+    */
+   struct blob blob;
+   blob_init_fixed(&blob, NULL, SIZE_MAX);
+   result = vk_shader_serialize(device, shader, &blob);
+   assert(result == VK_SUCCESS);
+
+   if (result != VK_SUCCESS) {
+      *pDataSize = 0;
+      return result;
+   } else if (pData == NULL) {
+      *pDataSize = blob.size;
+      return VK_SUCCESS;
+   } else if (blob.size > *pDataSize) {
+      /* No data written */
+      *pDataSize = 0;
+      return VK_INCOMPLETE;
+   }
+
+   blob_init_fixed(&blob, pData, *pDataSize);
+   result = vk_shader_serialize(device, shader, &blob);
+   assert(result == VK_SUCCESS);
+
+   *pDataSize = blob.size;
+
+   return result;
+}
+
+#define VK_MAX_LINKED_SHADER_STAGES 5
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_CreateShadersEXT(VkDevice _device,
+                           uint32_t createInfoCount,
+                           const VkShaderCreateInfoEXT *pCreateInfos,
+                           const VkAllocationCallbacks *pAllocator,
+                           VkShaderEXT *pShaders)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+   VkResult first_fail_or_success = VK_SUCCESS;
+
+   struct vk_pipeline_robustness_state rs = {
+      .storage_buffers = VK_PIPELINE_ROBUSTNESS_BUFFER_BEHAVIOR_DISABLED_EXT,
+      .uniform_buffers = VK_PIPELINE_ROBUSTNESS_BUFFER_BEHAVIOR_DISABLED_EXT,
+      .vertex_inputs = VK_PIPELINE_ROBUSTNESS_BUFFER_BEHAVIOR_DISABLED_EXT,
+      .images = VK_PIPELINE_ROBUSTNESS_IMAGE_BEHAVIOR_DISABLED_EXT,
+   };
+
+   /* From the Vulkan 1.3.274 spec:
+    *
+    *    "When this function returns, whether or not it succeeds, it is
+    *    guaranteed that every element of pShaders will have been overwritten
+    *    by either VK_NULL_HANDLE or a valid VkShaderEXT handle."
+    *
+    * Zeroing up-front makes the error path easier.
+    */
+   memset(pShaders, 0, createInfoCount * sizeof(*pShaders));
+
+   bool has_linked_spirv = false;
+   for (uint32_t i = 0; i < createInfoCount; i++) {
+      if (pCreateInfos[i].codeType == VK_SHADER_CODE_TYPE_SPIRV_EXT &&
+          (pCreateInfos[i].flags & VK_SHADER_CREATE_LINK_STAGE_BIT_EXT))
+         has_linked_spirv = true;
+   }
+
+   uint32_t linked_count = 0;
+   struct stage_idx linked[VK_MAX_LINKED_SHADER_STAGES];
+
+   for (uint32_t i = 0; i < createInfoCount; i++) {
+      const VkShaderCreateInfoEXT *vk_info = &pCreateInfos[i];
+      VkResult result = VK_SUCCESS;
+
+      switch (vk_info->codeType) {
+      case VK_SHADER_CODE_TYPE_BINARY_EXT: {
+         /* This isn't required by Vulkan but we're allowed to fail binary
+          * import for basically any reason.  This seems like a pretty good
+          * reason.
+          */
+         if (has_linked_spirv &&
+             (vk_info->flags & VK_SHADER_CREATE_LINK_STAGE_BIT_EXT)) {
+            result = vk_errorf(device, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT,
+                               "Cannot mix linked binary and SPIR-V");
+            break;
+         }
+
+         struct vk_shader *shader;
+         result = vk_shader_deserialize(device, vk_info->codeSize,
+                                        vk_info->pCode, pAllocator,
+                                        &shader);
+         if (result != VK_SUCCESS)
+            break;
+
+         pShaders[i] = vk_shader_to_handle(shader);
+         break;
+      }
+
+      case VK_SHADER_CODE_TYPE_SPIRV_EXT: {
+         if (vk_info->flags & VK_SHADER_CREATE_LINK_STAGE_BIT_EXT) {
+            /* Stash it and compile later */
+            assert(linked_count < ARRAY_SIZE(linked));
+            linked[linked_count++] = (struct stage_idx) {
+               .stage = vk_to_mesa_shader_stage(vk_info->stage),
+               .idx = i,
+            };
+         } else {
+            nir_shader *nir = vk_shader_to_nir(device, vk_info, &rs);
+            if (nir == NULL) {
+               result = vk_errorf(device, VK_ERROR_UNKNOWN,
+                                  "Failed to compile shader to NIR");
+               break;
+            }
+
+            struct vk_shader_compile_info info;
+            struct set_layouts set_layouts;
+            vk_shader_compile_info_init(&info, &set_layouts,
+                                        vk_info, &rs, nir);
+
+            struct vk_shader *shader;
+            result = ops->compile(device, 1, &info, NULL /* state */,
+                                  pAllocator, &shader);
+            if (result != VK_SUCCESS)
+               break;
+
+            pShaders[i] = vk_shader_to_handle(shader);
+         }
+         break;
+      }
+
+      default:
+         unreachable("Unknown shader code type");
+      }
+
+      if (first_fail_or_success == VK_SUCCESS)
+         first_fail_or_success = result;
+   }
+
+   if (linked_count > 0) {
+      struct set_layouts set_layouts[VK_MAX_LINKED_SHADER_STAGES];
+      struct vk_shader_compile_info infos[VK_MAX_LINKED_SHADER_STAGES];
+      VkResult result = VK_SUCCESS;
+
+      /* Sort so we guarantee the driver always gets them in-order */
+      qsort(linked, linked_count, sizeof(*linked), cmp_stage_idx);
+
+      /* Memset for easy error handling */
+      memset(infos, 0, sizeof(infos));
+
+      for (uint32_t l = 0; l < linked_count; l++) {
+         const VkShaderCreateInfoEXT *vk_info = &pCreateInfos[linked[l].idx];
+
+         nir_shader *nir = vk_shader_to_nir(device, vk_info, &rs);
+         if (nir == NULL) {
+            result = vk_errorf(device, VK_ERROR_UNKNOWN,
+                               "Failed to compile shader to NIR");
+            break;
+         }
+
+         vk_shader_compile_info_init(&infos[l], &set_layouts[l],
+                                     vk_info, &rs, nir);
+      }
+
+      if (result == VK_SUCCESS) {
+         struct vk_shader *shaders[VK_MAX_LINKED_SHADER_STAGES];
+
+         result = ops->compile(device, linked_count, infos, NULL /* state */,
+                               pAllocator, shaders);
+         if (result == VK_SUCCESS) {
+            for (uint32_t l = 0; l < linked_count; l++)
+               pShaders[linked[l].idx] = vk_shader_to_handle(shaders[l]);
+         }
+      } else {
+         for (uint32_t l = 0; l < linked_count; l++) {
+            if (infos[l].nir != NULL)
+               ralloc_free(infos[l].nir);
+         }
+      }
+
+      if (first_fail_or_success == VK_SUCCESS)
+         first_fail_or_success = result;
+   }
+
+   return first_fail_or_success;
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_DestroyShaderEXT(VkDevice _device,
+                           VkShaderEXT _shader,
+                           const VkAllocationCallbacks *pAllocator)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_shader, shader, _shader);
+
+   if (shader == NULL)
+      return;
+
+   vk_shader_destroy(device, shader, pAllocator);
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdBindShadersEXT(VkCommandBuffer commandBuffer,
+                            uint32_t stageCount,
+                            const VkShaderStageFlagBits *pStages,
+                            const VkShaderEXT *pShaders)
+{
+   VK_FROM_HANDLE(vk_command_buffer, cmd_buffer, commandBuffer);
+   struct vk_device *device = cmd_buffer->base.device;
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+
+   STACK_ARRAY(gl_shader_stage, stages, stageCount);
+   STACK_ARRAY(struct vk_shader *, shaders, stageCount);
+
+   for (uint32_t i = 0; i < stageCount; i++) {
+      stages[i] = vk_to_mesa_shader_stage(pStages[i]);
+      shaders[i] = pShaders != NULL ? vk_shader_from_handle(pShaders[i]) : NULL;
+   }
+
+   ops->cmd_bind_shaders(cmd_buffer, stageCount, stages, shaders);
+}
diff --git a/src/vulkan/runtime/vk_shader.h b/src/vulkan/runtime/vk_shader.h
new file mode 100644
index 0000000000000..0ee6e7681c343
--- /dev/null
+++ b/src/vulkan/runtime/vk_shader.h
@@ -0,0 +1,184 @@
+/*
+ * Copyright © 2024 Collabora, Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef VK_SHADER_H
+#define VK_SHADER_H
+
+#include "compiler/spirv/nir_spirv.h"
+#include "vk_limits.h"
+#include "vk_pipeline_cache.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct blob;
+struct nir_shader;
+struct vk_command_buffer;
+struct vk_device;
+struct vk_descriptor_set_layout;
+struct vk_dynamic_graphics_state;
+struct vk_graphics_pipeline_state;
+struct vk_physical_device;
+struct vk_pipeline;
+struct vk_pipeline_robustness_state;
+
+int vk_shader_cmp_graphics_stages(gl_shader_stage a, gl_shader_stage b);
+
+struct vk_shader_compile_info {
+   gl_shader_stage stage;
+   VkShaderCreateFlagsEXT flags;
+   VkShaderStageFlags next_stage_mask;
+   struct nir_shader *nir;
+
+   const struct vk_pipeline_robustness_state *robustness;
+
+   uint32_t set_layout_count;
+   struct vk_descriptor_set_layout * const *set_layouts;
+
+   uint32_t push_constant_range_count;
+   const VkPushConstantRange *push_constant_ranges;
+};
+
+struct vk_shader_ops;
+
+struct vk_shader {
+   struct vk_object_base base;
+
+   const struct vk_shader_ops *ops;
+
+   gl_shader_stage stage;
+};
+
+VK_DEFINE_NONDISP_HANDLE_CASTS(vk_shader, base, VkShaderEXT,
+                               VK_OBJECT_TYPE_SHADER_EXT);
+
+struct vk_shader_ops {
+   /** Destroy a vk_shader_object */
+   void (*destroy)(struct vk_device *device,
+                   struct vk_shader *shader,
+                   const VkAllocationCallbacks* pAllocator);
+
+   /** Serialize a vk_shader_object to a blob
+    *
+    * This function shouldn't need to do any validation of the blob data
+    * beyond basic sanity checking.  The common implementation of
+    * vkGetShaderBinaryEXT verifies the blobUUID and version of input data as
+    * well as a size and checksum to ensure integrity.  This callback is only
+    * invoked after validation of the input binary data.
+    */
+   bool (*serialize)(struct vk_device *device,
+                     const struct vk_shader *shader,
+                     struct blob *blob);
+};
+
+void *vk_shader_zalloc(struct vk_device *device,
+                       const struct vk_shader_ops *ops,
+                       gl_shader_stage stage,
+                       const VkAllocationCallbacks *alloc,
+                       size_t size);
+void vk_shader_free(struct vk_device *device,
+                    const VkAllocationCallbacks *alloc,
+                    struct vk_shader *shader);
+
+static inline void
+vk_shader_destroy(struct vk_device *device,
+                  struct vk_shader *shader,
+                  const VkAllocationCallbacks *alloc)
+{
+   shader->ops->destroy(device, shader, alloc);
+}
+
+struct vk_device_shader_ops {
+   /** Retrieves a NIR compiler options struct
+    *
+    * NIR compiler options are only allowed to vary based on physical device,
+    * stage, and robustness state.
+    */
+   const struct nir_shader_compiler_options *(*get_nir_options)(
+      struct vk_physical_device *device,
+      gl_shader_stage stage,
+      const struct vk_pipeline_robustness_state *rs);
+
+   /** Retrieves a SPIR-V options struct
+    *
+    * SPIR-V options are only allowed to vary based on physical device, stage,
+    * and robustness state.
+    */
+   struct spirv_to_nir_options (*get_spirv_options)(
+      struct vk_physical_device *device,
+      gl_shader_stage stage,
+      const struct vk_pipeline_robustness_state *rs);
+
+   /** Preprocesses a NIR shader
+    *
+    * This callback is optional.
+    *
+    * If non-NULL, this callback is invoked after the SPIR-V is parsed into
+    * NIR and before it is handed to compile().  The driver should do as much
+    * generic optimization and lowering as it can here.  Importantly, the
+    * preprocess step only knows about the NIR input and the physical device,
+    * not any enabled device features or pipeline state.  This allows us to
+    * potentially cache this shader and re-use it across pipelines.
+    */
+   void (*preprocess_nir)(struct vk_physical_device *device, nir_shader *nir);
+
+   /** Compile (and potentially link) a set of shaders
+    *
+    * Unlike vkCreateShadersEXT, this callback will only ever be called with
+    * multiple shaders if VK_SHADER_CREATE_LINK_STAGE_BIT_EXT is set on all of
+    * them.  We also guarantee that the shaders occur in the call in Vulkan
+    * pipeline stage order as dictated by vk_shader_cmp_graphics_stages().
+    *
+    * This callback consumes all input NIR shaders, regardless of whether or
+    * not it was successful.
+    */
+   VkResult (*compile)(struct vk_device *device,
+                       uint32_t shader_count,
+                       struct vk_shader_compile_info *infos,
+                       const struct vk_graphics_pipeline_state *state,
+                       const VkAllocationCallbacks* pAllocator,
+                       struct vk_shader **shaders_out);
+
+   /** Create a vk_shader from a binary blob */
+   VkResult (*deserialize)(struct vk_device *device,
+                           struct blob_reader *blob,
+                           uint32_t binary_version,
+                           const VkAllocationCallbacks* pAllocator,
+                           struct vk_shader **shader_out);
+
+   /** Bind a set of shaders
+    *
+    * This is roughly equivalent to vkCmdBindShadersEXT()
+    */
+   void (*cmd_bind_shaders)(struct vk_command_buffer *cmd_buffer,
+                            uint32_t stage_count,
+                            const gl_shader_stage *stages,
+                            struct vk_shader ** const shaders);
+};
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* VK_SHADER_H */
-- 
GitLab


From 8ba3f7afdd37bc90de6d582de49656958ff33e24 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 9 Jan 2024 22:06:20 -0600
Subject: [PATCH 06/22] vulkan: Add a
 vk_render_pass_state_has_attachment_info() helper

We already have a helper like this internally.  Give it a better name
and expose it.

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/freedreno/vulkan/tu_pipeline.cc    |  3 +--
 src/intel/vulkan/anv_pipeline.c        |  2 +-
 src/vulkan/runtime/vk_graphics_state.c | 18 ++++++------------
 src/vulkan/runtime/vk_graphics_state.h |  6 ++++++
 4 files changed, 14 insertions(+), 15 deletions(-)

diff --git a/src/freedreno/vulkan/tu_pipeline.cc b/src/freedreno/vulkan/tu_pipeline.cc
index e434665dd3bfe..45489055ec4b9 100644
--- a/src/freedreno/vulkan/tu_pipeline.cc
+++ b/src/freedreno/vulkan/tu_pipeline.cc
@@ -3272,8 +3272,7 @@ tu_pipeline_builder_emit_state(struct tu_pipeline_builder *builder,
               builder->graphics_state.rs);
    bool attachments_valid =
       builder->graphics_state.rp &&
-      !(builder->graphics_state.rp->attachment_aspects &
-                              VK_IMAGE_ASPECT_METADATA_BIT);
+      vk_render_pass_state_has_attachment_info(builder->graphics_state.rp);
    struct vk_color_blend_state dummy_cb = {};
    const struct vk_color_blend_state *cb = builder->graphics_state.cb;
    if (attachments_valid &&
diff --git a/src/intel/vulkan/anv_pipeline.c b/src/intel/vulkan/anv_pipeline.c
index 9ef14b87bc3f8..e39c8f268e604 100644
--- a/src/intel/vulkan/anv_pipeline.c
+++ b/src/intel/vulkan/anv_pipeline.c
@@ -576,7 +576,7 @@ populate_mesh_prog_key(struct anv_pipeline_stage *stage,
 static uint32_t
 rp_color_mask(const struct vk_render_pass_state *rp)
 {
-   if (rp == NULL || rp->attachment_aspects == VK_IMAGE_ASPECT_METADATA_BIT)
+   if (rp == NULL || !vk_render_pass_state_has_attachment_info(rp))
       return ((1u << MAX_RTS) - 1);
 
    uint32_t color_mask = 0;
diff --git a/src/vulkan/runtime/vk_graphics_state.c b/src/vulkan/runtime/vk_graphics_state.c
index 0efcb1a5bde06..0b5ec547448ac 100644
--- a/src/vulkan/runtime/vk_graphics_state.c
+++ b/src/vulkan/runtime/vk_graphics_state.c
@@ -1111,12 +1111,6 @@ vk_dynamic_graphics_state_init_cal(struct vk_dynamic_graphics_state *dst,
       typed_memcpy(dst->cal.color_map, cal->color_map, MESA_VK_MAX_COLOR_ATTACHMENTS);
 }
 
-static bool
-vk_render_pass_state_is_complete(const struct vk_render_pass_state *rp)
-{
-   return rp->attachment_aspects != VK_IMAGE_ASPECT_METADATA_BIT;
-}
-
 static void
 vk_pipeline_flags_init(struct vk_graphics_pipeline_state *state,
                        VkPipelineCreateFlags2KHR driver_rp_flags,
@@ -1176,7 +1170,7 @@ vk_render_pass_state_init(struct vk_render_pass_state *rp,
     * it's complete and we don't need a new one.  The one caveat here is that
     * we may need to add in some rendering flags.
     */
-   if (old_rp != NULL && vk_render_pass_state_is_complete(old_rp)) {
+   if (old_rp != NULL && vk_render_pass_state_has_attachment_info(old_rp)) {
       *rp = *old_rp;
       return;
    }
@@ -1477,8 +1471,8 @@ vk_graphics_pipeline_state_fill(const struct vk_device *device,
        * to NULL so it gets replaced with the new version.
        */
       if (state->rp != NULL &&
-          !vk_render_pass_state_is_complete(state->rp) &&
-          vk_render_pass_state_is_complete(&rp))
+          !vk_render_pass_state_has_attachment_info(state->rp) &&
+          !vk_render_pass_state_has_attachment_info(&rp))
          state->rp = NULL;
    }
 
@@ -1569,7 +1563,7 @@ vk_graphics_pipeline_state_fill(const struct vk_device *device,
        */
       if ((rp.attachment_aspects & (VK_IMAGE_ASPECT_DEPTH_BIT |
                                     VK_IMAGE_ASPECT_STENCIL_BIT)) ||
-          !vk_render_pass_state_is_complete(&rp))
+          !vk_render_pass_state_has_attachment_info(&rp))
          needs |= MESA_VK_GRAPHICS_STATE_DEPTH_STENCIL_BIT;
 
       needs |= MESA_VK_GRAPHICS_STATE_INPUT_ATTACHMENT_MAP_BIT;
@@ -1759,8 +1753,8 @@ vk_graphics_pipeline_state_merge(struct vk_graphics_pipeline_state *dst,
     * incomplete (view mask only).  See vk_render_pass_state_init().
     */
    if (dst->rp != NULL && src->rp != NULL &&
-       !vk_render_pass_state_is_complete(dst->rp) &&
-       vk_render_pass_state_is_complete(src->rp))
+       !vk_render_pass_state_has_attachment_info(dst->rp) &&
+       vk_render_pass_state_has_attachment_info(src->rp))
       dst->rp = src->rp;
 
 #define MERGE(STATE, type, state) \
diff --git a/src/vulkan/runtime/vk_graphics_state.h b/src/vulkan/runtime/vk_graphics_state.h
index d18b4d5eb8440..7d9339f119efc 100644
--- a/src/vulkan/runtime/vk_graphics_state.h
+++ b/src/vulkan/runtime/vk_graphics_state.h
@@ -728,6 +728,12 @@ struct vk_render_pass_state {
    uint8_t depth_stencil_attachment_samples;
 };
 
+static inline bool
+vk_render_pass_state_has_attachment_info(const struct vk_render_pass_state *rp)
+{
+   return rp->attachment_aspects != VK_IMAGE_ASPECT_METADATA_BIT;
+}
+
 static inline VkImageAspectFlags
 vk_pipeline_flags_feedback_loops(VkPipelineCreateFlags2KHR flags)
 {
-- 
GitLab


From c67ebbe18fd284306f44c45dd965c83dfb0af3af Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 9 Jan 2024 22:07:14 -0600
Subject: [PATCH 07/22] vulkan: Rework vk_render_pass_state::attachments

The new bitfield has a separat flag for each of the color attachments.

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/freedreno/vulkan/tu_pipeline.cc    | 40 ++++++++++++++------------
 src/imagination/vulkan/pvr_pipeline.c  | 14 +++++----
 src/vulkan/runtime/vk_graphics_state.c | 19 ++++++------
 src/vulkan/runtime/vk_graphics_state.h | 27 ++++++++++++++---
 4 files changed, 62 insertions(+), 38 deletions(-)

diff --git a/src/freedreno/vulkan/tu_pipeline.cc b/src/freedreno/vulkan/tu_pipeline.cc
index 45489055ec4b9..d84470fbbcd04 100644
--- a/src/freedreno/vulkan/tu_pipeline.cc
+++ b/src/freedreno/vulkan/tu_pipeline.cc
@@ -2783,13 +2783,13 @@ tu_calc_bandwidth(struct tu_bandwidth *bandwidth,
 
    bandwidth->color_bandwidth_per_sample = total_bpp / 8;
 
-   if (rp->attachment_aspects & VK_IMAGE_ASPECT_DEPTH_BIT) {
+   if (rp->attachments & MESA_VK_RP_ATTACHMENT_DEPTH_BIT) {
       bandwidth->depth_cpp_per_sample = util_format_get_component_bits(
             vk_format_to_pipe_format(rp->depth_attachment_format),
             UTIL_FORMAT_COLORSPACE_ZS, 0) / 8;
    }
 
-   if (rp->attachment_aspects & VK_IMAGE_ASPECT_STENCIL_BIT) {
+   if (rp->attachments & MESA_VK_RP_ATTACHMENT_STENCIL_BIT) {
       bandwidth->stencil_cpp_per_sample = util_format_get_component_bits(
             vk_format_to_pipe_format(rp->stencil_attachment_format),
             UTIL_FORMAT_COLORSPACE_ZS, 1) / 8;
@@ -3149,7 +3149,7 @@ tu6_emit_rb_depth_cntl(struct tu_cs *cs,
                        const struct vk_render_pass_state *rp,
                        const struct vk_rasterization_state *rs)
 {
-   if (rp->attachment_aspects & VK_IMAGE_ASPECT_DEPTH_BIT) {
+   if (rp->attachments & MESA_VK_RP_ATTACHMENT_DEPTH_BIT) {
       bool depth_test = ds->depth.test_enable;
       enum adreno_compare_func zfunc = tu6_compare_func(ds->depth.compare_op);
 
@@ -3276,8 +3276,8 @@ tu_pipeline_builder_emit_state(struct tu_pipeline_builder *builder,
    struct vk_color_blend_state dummy_cb = {};
    const struct vk_color_blend_state *cb = builder->graphics_state.cb;
    if (attachments_valid &&
-       !(builder->graphics_state.rp->attachment_aspects &
-         VK_IMAGE_ASPECT_COLOR_BIT)) {
+       !(builder->graphics_state.rp->attachments &
+         MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS)) {
       /* If there are no color attachments, then the original blend state may
        * be NULL and the common code sanitizes it to always be NULL. In this
        * case we want to emit an empty blend/bandwidth/etc.  rather than
@@ -3305,8 +3305,8 @@ tu_pipeline_builder_emit_state(struct tu_pipeline_builder *builder,
                         builder->graphics_state.rp);
    DRAW_STATE(blend_constants, TU_DYNAMIC_STATE_BLEND_CONSTANTS, cb);
    if (attachments_valid &&
-       !(builder->graphics_state.rp->attachment_aspects &
-         VK_IMAGE_ASPECT_COLOR_BIT)) {
+       !(builder->graphics_state.rp->attachments &
+         MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS)) {
       /* Don't actually make anything dynamic as that may mean a partially-set
        * state group where the group is NULL which angers common code.
        */
@@ -3538,10 +3538,10 @@ tu_pipeline_builder_parse_depth_stencil(
    const VkPipelineDepthStencilStateCreateInfo *ds_info =
       builder->create_info->pDepthStencilState;
 
-   if ((builder->graphics_state.rp->attachment_aspects &
-        VK_IMAGE_ASPECT_METADATA_BIT) ||
-       (builder->graphics_state.rp->attachment_aspects &
-        VK_IMAGE_ASPECT_DEPTH_BIT)) {
+   if ((builder->graphics_state.rp->attachments ==
+        MESA_VK_RP_ATTACHMENT_INFO_INVALID) ||
+       (builder->graphics_state.rp->attachments &
+        MESA_VK_RP_ATTACHMENT_DEPTH_BIT)) {
       pipeline->ds.raster_order_attachment_access =
          ds_info && (ds_info->flags &
          (VK_PIPELINE_DEPTH_STENCIL_STATE_CREATE_RASTERIZATION_ORDER_ATTACHMENT_DEPTH_ACCESS_BIT_EXT |
@@ -3576,11 +3576,13 @@ tu_pipeline_builder_parse_multisample_and_color_blend(
    static const VkPipelineColorBlendStateCreateInfo dummy_blend_info = {};
 
    const VkPipelineColorBlendStateCreateInfo *blend_info =
-      (builder->graphics_state.rp->attachment_aspects &
-       VK_IMAGE_ASPECT_COLOR_BIT) ? builder->create_info->pColorBlendState :
-      &dummy_blend_info;
+      (builder->graphics_state.rp->attachments &
+       MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS)
+      ? builder->create_info->pColorBlendState
+      : &dummy_blend_info;
 
-   if (builder->graphics_state.rp->attachment_aspects & VK_IMAGE_ASPECT_COLOR_BIT) {
+   if (builder->graphics_state.rp->attachments &
+       MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS)
       pipeline->output.raster_order_attachment_access =
          blend_info && (blend_info->flags &
             VK_PIPELINE_COLOR_BLEND_STATE_CREATE_RASTERIZATION_ORDER_ATTACHMENT_ACCESS_BIT_EXT);
@@ -3847,16 +3849,16 @@ tu_fill_render_pass_state(struct vk_render_pass_state *rp,
    const uint32_t a = subpass->depth_stencil_attachment.attachment;
    rp->depth_attachment_format = VK_FORMAT_UNDEFINED;
    rp->stencil_attachment_format = VK_FORMAT_UNDEFINED;
-   rp->attachment_aspects = 0;
+   rp->attachments = 0;
    if (a != VK_ATTACHMENT_UNUSED) {
       VkFormat ds_format = pass->attachments[a].format;
       if (vk_format_has_depth(ds_format)) {
          rp->depth_attachment_format = ds_format;
-         rp->attachment_aspects |= VK_IMAGE_ASPECT_DEPTH_BIT;
+         rp->attachments |= MESA_VK_RP_ATTACHMENT_DEPTH_BIT;
       }
       if (vk_format_has_stencil(ds_format)) {
          rp->stencil_attachment_format = ds_format;
-         rp->attachment_aspects |= VK_IMAGE_ASPECT_STENCIL_BIT;
+         rp->attachments |= MESA_VK_RP_ATTACHMENT_STENCIL_BIT;
       }
    }
 
@@ -3868,7 +3870,7 @@ tu_fill_render_pass_state(struct vk_render_pass_state *rp,
       }
 
       rp->color_attachment_formats[i] = pass->attachments[a].format;
-      rp->attachment_aspects |= VK_IMAGE_ASPECT_COLOR_BIT;
+      rp->attachments |= MESA_VK_RP_ATTACHMENT_COLOR0_BIT << a;
    }
 }
 
diff --git a/src/imagination/vulkan/pvr_pipeline.c b/src/imagination/vulkan/pvr_pipeline.c
index 482a56ebc6bc8..46fcd6d489dc7 100644
--- a/src/imagination/vulkan/pvr_pipeline.c
+++ b/src/imagination/vulkan/pvr_pipeline.c
@@ -2250,22 +2250,26 @@ pvr_create_renderpass_state(const VkGraphicsPipelineCreateInfo *const info)
    const struct pvr_render_subpass *const subpass =
       &pass->subpasses[info->subpass];
 
-   VkImageAspectFlags attachment_aspects = VK_IMAGE_ASPECT_NONE;
+   enum vk_rp_attachment_flags attachments = 0;
 
    assert(info->subpass < pass->subpass_count);
 
    for (uint32_t i = 0; i < subpass->color_count; i++) {
-      attachment_aspects |=
-         pass->attachments[subpass->color_attachments[i]].aspects;
+      if (pass->attachments[subpass->color_attachments[i]].aspects)
+         attachments |= MESA_VK_RP_ATTACHMENT_COLOR_0_BIT << i;
    }
 
    if (subpass->depth_stencil_attachment != VK_ATTACHMENT_UNUSED) {
-      attachment_aspects |=
+      VkImageAspectFlags ds_aspects =
          pass->attachments[subpass->depth_stencil_attachment].aspects;
+      if (ds_aspects & VK_IMAGE_ASPECT_DEPTH_BIT)
+         attachments |= MESA_VK_RP_ATTACHMENT_DEPTH_BIT;
+      if (ds_aspects & VK_IMAGE_ASPECT_STENCIL_BIT)
+         attachments |= MESA_VK_RP_ATTACHMENT_STENCIL_BIT;
    }
 
    return (struct vk_render_pass_state){
-      .attachment_aspects = attachment_aspects,
+      .attachments = attachments,
 
       /* TODO: This is only needed for VK_KHR_create_renderpass2 (or core 1.2),
        * which is not currently supported.
diff --git a/src/vulkan/runtime/vk_graphics_state.c b/src/vulkan/runtime/vk_graphics_state.c
index 0b5ec547448ac..e01cfbef46574 100644
--- a/src/vulkan/runtime/vk_graphics_state.c
+++ b/src/vulkan/runtime/vk_graphics_state.c
@@ -1211,7 +1211,7 @@ vk_render_pass_state_init(struct vk_render_pass_state *rp,
     */
    if (info->renderPass == VK_NULL_HANDLE &&
        !(lib & VK_GRAPHICS_PIPELINE_LIBRARY_FRAGMENT_OUTPUT_INTERFACE_BIT_EXT)) {
-      rp->attachment_aspects = VK_IMAGE_ASPECT_METADATA_BIT;
+      rp->attachments = MESA_VK_RP_ATTACHMENT_INFO_INVALID;
       return;
    }
 
@@ -1220,16 +1220,16 @@ vk_render_pass_state_init(struct vk_render_pass_state *rp,
    for (uint32_t i = 0; i < r_info->colorAttachmentCount; i++) {
       rp->color_attachment_formats[i] = r_info->pColorAttachmentFormats[i];
       if (r_info->pColorAttachmentFormats[i] != VK_FORMAT_UNDEFINED)
-         rp->attachment_aspects |= VK_IMAGE_ASPECT_COLOR_BIT;
+         rp->attachments |= MESA_VK_RP_ATTACHMENT_COLOR_0_BIT << i;
    }
 
    rp->depth_attachment_format = r_info->depthAttachmentFormat;
    if (r_info->depthAttachmentFormat != VK_FORMAT_UNDEFINED)
-      rp->attachment_aspects |= VK_IMAGE_ASPECT_DEPTH_BIT;
+      rp->attachments |= MESA_VK_RP_ATTACHMENT_DEPTH_BIT;
 
    rp->stencil_attachment_format = r_info->stencilAttachmentFormat;
    if (r_info->stencilAttachmentFormat != VK_FORMAT_UNDEFINED)
-      rp->attachment_aspects |= VK_IMAGE_ASPECT_STENCIL_BIT;
+      rp->attachments |= MESA_VK_RP_ATTACHMENT_STENCIL_BIT;
 
    const VkAttachmentSampleCountInfoAMD *asc_info =
       vk_get_pipeline_sample_count_info_amd(info);
@@ -1561,18 +1561,17 @@ vk_graphics_pipeline_state_fill(const struct vk_device *device,
        * where we only have fragment shader state and no render pass, the
        * vk_render_pass_state will be incomplete.
        */
-      if ((rp.attachment_aspects & (VK_IMAGE_ASPECT_DEPTH_BIT |
-                                    VK_IMAGE_ASPECT_STENCIL_BIT)) ||
-          !vk_render_pass_state_has_attachment_info(&rp))
+      if (!vk_render_pass_state_has_attachment_info(&rp) ||
+          (rp.attachments & (MESA_VK_RP_ATTACHMENT_DEPTH_BIT |
+                             MESA_VK_RP_ATTACHMENT_STENCIL_BIT)))
          needs |= MESA_VK_GRAPHICS_STATE_DEPTH_STENCIL_BIT;
 
       needs |= MESA_VK_GRAPHICS_STATE_INPUT_ATTACHMENT_MAP_BIT;
    }
 
    if (lib & VK_GRAPHICS_PIPELINE_LIBRARY_FRAGMENT_OUTPUT_INTERFACE_BIT_EXT) {
-      if (rp.attachment_aspects & (VK_IMAGE_ASPECT_COLOR_BIT)) {
+      if (rp.attachments & MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS)
          needs |= MESA_VK_GRAPHICS_STATE_COLOR_BLEND_BIT;
-      }
 
       needs |= MESA_VK_GRAPHICS_STATE_MULTISAMPLE_BIT;
 
@@ -1978,7 +1977,7 @@ vk_dynamic_graphics_state_fill(struct vk_dynamic_graphics_state *dyn,
     * the other blend states will be initialized. Normally this would be
     * initialized with the other blend states.
     */
-   if (!p->rp || !(p->rp->attachment_aspects & VK_IMAGE_ASPECT_COLOR_BIT)) {
+   if (!p->rp || !(p->rp->attachments & MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS)) {
       dyn->cb.attachment_count = 0;
       BITSET_SET(dyn->set, MESA_VK_DYNAMIC_CB_ATTACHMENT_COUNT);
    }
diff --git a/src/vulkan/runtime/vk_graphics_state.h b/src/vulkan/runtime/vk_graphics_state.h
index 7d9339f119efc..e55fea832cd2a 100644
--- a/src/vulkan/runtime/vk_graphics_state.h
+++ b/src/vulkan/runtime/vk_graphics_state.h
@@ -667,6 +667,25 @@ struct vk_color_blend_state {
    float blend_constants[4];
 };
 
+enum vk_rp_attachment_flags {
+   MESA_VK_RP_ATTACHMENT_COLOR_0_BIT               = (1 << 0),
+   MESA_VK_RP_ATTACHMENT_COLOR_1_BIT               = (1 << 1),
+   MESA_VK_RP_ATTACHMENT_COLOR_2_BIT               = (1 << 2),
+   MESA_VK_RP_ATTACHMENT_COLOR_3_BIT               = (1 << 3),
+   MESA_VK_RP_ATTACHMENT_COLOR_4_BIT               = (1 << 4),
+   MESA_VK_RP_ATTACHMENT_COLOR_5_BIT               = (1 << 5),
+   MESA_VK_RP_ATTACHMENT_COLOR_6_BIT               = (1 << 6),
+   MESA_VK_RP_ATTACHMENT_COLOR_7_BIT               = (1 << 7),
+   MESA_VK_RP_ATTACHMENT_ANY_COLOR_BITS            = 0xff,
+
+   MESA_VK_RP_ATTACHMENT_DEPTH_BIT                 = (1 << 8),
+   MESA_VK_RP_ATTACHMENT_STENCIL_BIT               = (1 << 9),
+
+   MESA_VK_RP_ATTACHMENT_INFO_INVALID = 0xffff,
+};
+static_assert(MESA_VK_MAX_COLOR_ATTACHMENTS == 8,
+              "This enum must match the global runtime limit");
+
 /***/
 struct vk_input_attachment_location_state {
    /** VkRenderingInputAttachmentIndexInfoKHR::pColorAttachmentLocations
@@ -701,10 +720,10 @@ struct vk_color_attachment_location_state {
 struct vk_render_pass_state {
    /** Set of image aspects bound as color/depth/stencil attachments
     *
-    * Set to VK_IMAGE_ASPECT_METADATA_BIT to indicate that attachment info
-    * is invalid.
+    * Set to MESA_VK_RP_ATTACHMENT_INFO_INVALID to indicate that attachment
+    * info is invalid.
     */
-   VkImageAspectFlags attachment_aspects;
+   enum vk_rp_attachment_flags attachments;
 
    /** VkPipelineRenderingCreateInfo::viewMask */
    uint32_t view_mask;
@@ -731,7 +750,7 @@ struct vk_render_pass_state {
 static inline bool
 vk_render_pass_state_has_attachment_info(const struct vk_render_pass_state *rp)
 {
-   return rp->attachment_aspects != VK_IMAGE_ASPECT_METADATA_BIT;
+   return rp->attachments != MESA_VK_RP_ATTACHMENT_INFO_INVALID;
 }
 
 static inline VkImageAspectFlags
-- 
GitLab


From 5b664f68fd965f1e6387d34b2dddfdfdca596366 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 9 Jan 2024 22:19:49 -0600
Subject: [PATCH 08/22] vulkan: Add a new dynamic state for render pass
 attachments

This is useful for implementing VK_EXT_dynamic_rendering_unused_attachments

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_graphics_state.c | 19 +++++++++++++++++--
 src/vulkan/runtime/vk_graphics_state.h | 14 ++++++++++++++
 2 files changed, 31 insertions(+), 2 deletions(-)

diff --git a/src/vulkan/runtime/vk_graphics_state.c b/src/vulkan/runtime/vk_graphics_state.c
index e01cfbef46574..2179accf550b0 100644
--- a/src/vulkan/runtime/vk_graphics_state.c
+++ b/src/vulkan/runtime/vk_graphics_state.c
@@ -133,8 +133,10 @@ get_dynamic_state_groups(BITSET_WORD *dynamic,
    if (groups & MESA_VK_GRAPHICS_STATE_INPUT_ATTACHMENT_MAP_BIT)
       BITSET_SET(dynamic, MESA_VK_DYNAMIC_INPUT_ATTACHMENT_MAP);
 
-   if (groups & MESA_VK_GRAPHICS_STATE_RENDER_PASS_BIT)
+   if (groups & MESA_VK_GRAPHICS_STATE_RENDER_PASS_BIT) {
+      BITSET_SET(dynamic, MESA_VK_DYNAMIC_RP_ATTACHMENTS);
       BITSET_SET(dynamic, MESA_VK_DYNAMIC_ATTACHMENT_FEEDBACK_LOOP_ENABLE);
+   }
 }
 
 static enum mesa_vk_graphics_state_groups
@@ -1247,7 +1249,9 @@ static void
 vk_dynamic_graphics_state_init_rp(struct vk_dynamic_graphics_state *dst,
                                   const BITSET_WORD *needed,
                                   const struct vk_render_pass_state *rp)
-{ }
+{
+   dst->rp.attachments = rp->attachments;
+}
 
 #define FOREACH_STATE_GROUP(f)                           \
    f(MESA_VK_GRAPHICS_STATE_VERTEX_INPUT_BIT,            \
@@ -2188,6 +2192,8 @@ vk_dynamic_graphics_state_copy(struct vk_dynamic_graphics_state *dst,
    if (IS_SET_IN_SRC(CB_BLEND_CONSTANTS))
       COPY_ARRAY(CB_BLEND_CONSTANTS, cb.blend_constants, 4);
 
+   COPY_IF_SET(RP_ATTACHMENTS, rp.attachments);
+
    if (IS_SET_IN_SRC(COLOR_ATTACHMENT_MAP)) {
       COPY_ARRAY(COLOR_ATTACHMENT_MAP, cal.color_map,
                  MESA_VK_MAX_COLOR_ATTACHMENTS);
@@ -3010,6 +3016,15 @@ vk_cmd_set_cb_attachment_count(struct vk_command_buffer *cmd,
    SET_DYN_VALUE(dyn, CB_ATTACHMENT_COUNT, cb.attachment_count, attachment_count);
 }
 
+void
+vk_cmd_set_rp_attachments(struct vk_command_buffer *cmd,
+                          enum vk_rp_attachment_flags attachments)
+{
+   struct vk_dynamic_graphics_state *dyn = &cmd->dynamic_graphics_state;
+
+   SET_DYN_VALUE(dyn, RP_ATTACHMENTS, rp.attachments, attachments);
+}
+
 VKAPI_ATTR void VKAPI_CALL
 vk_common_CmdSetDiscardRectangleEnableEXT(VkCommandBuffer commandBuffer,
                                           VkBool32 discardRectangleEnable)
diff --git a/src/vulkan/runtime/vk_graphics_state.h b/src/vulkan/runtime/vk_graphics_state.h
index e55fea832cd2a..38b88356134ae 100644
--- a/src/vulkan/runtime/vk_graphics_state.h
+++ b/src/vulkan/runtime/vk_graphics_state.h
@@ -102,6 +102,7 @@ enum mesa_vk_dynamic_graphics_state {
    MESA_VK_DYNAMIC_CB_BLEND_EQUATIONS,
    MESA_VK_DYNAMIC_CB_WRITE_MASKS,
    MESA_VK_DYNAMIC_CB_BLEND_CONSTANTS,
+   MESA_VK_DYNAMIC_RP_ATTACHMENTS,
    MESA_VK_DYNAMIC_ATTACHMENT_FEEDBACK_LOOP_ENABLE,
    MESA_VK_DYNAMIC_COLOR_ATTACHMENT_MAP,
    MESA_VK_DYNAMIC_INPUT_ATTACHMENT_MAP,
@@ -890,6 +891,10 @@ struct vk_dynamic_graphics_state {
    /** Color blend state */
    struct vk_color_blend_state cb;
 
+   struct {
+      enum vk_rp_attachment_flags attachments;
+   } rp;
+
    /** MESA_VK_DYNAMIC_ATTACHMENT_FEEDBACK_LOOP_ENABLE */
    VkImageAspectFlags feedback_loops;
 
@@ -1230,6 +1235,15 @@ void
 vk_cmd_set_cb_attachment_count(struct vk_command_buffer *cmd,
                                uint32_t attachment_count);
 
+/* Set render pass attachments on a command buffer.
+ *
+ * This is required for VK_EXT_shader_object in order to disable attachments
+ * based on bound shaders.
+ */
+void
+vk_cmd_set_rp_attachments(struct vk_command_buffer *cmd,
+                          enum vk_rp_attachment_flags attachments);
+
 const char *
 vk_dynamic_graphic_state_to_str(enum mesa_vk_dynamic_graphics_state state);
 
-- 
GitLab


From e38ec890ddc9d4212520618381c8285f3613e373 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Wed, 3 Jan 2024 11:59:21 -0600
Subject: [PATCH 09/22] vulkan: Add a vk_pipeline base struct

We need to be able to thunk through a destroy callback if we want to
have different kinds of pipelines implemented in different parts of the
stack.

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_pipeline.c | 104 +++++++++++++++++++++++++++++++
 src/vulkan/runtime/vk_pipeline.h |  55 +++++++++++++++-
 2 files changed, 158 insertions(+), 1 deletion(-)

diff --git a/src/vulkan/runtime/vk_pipeline.c b/src/vulkan/runtime/vk_pipeline.c
index a1fa7745fdede..ac7b66a13f46f 100644
--- a/src/vulkan/runtime/vk_pipeline.c
+++ b/src/vulkan/runtime/vk_pipeline.c
@@ -23,6 +23,8 @@
 
 #include "vk_pipeline.h"
 
+#include "vk_common_entrypoints.h"
+#include "vk_command_buffer.h"
 #include "vk_device.h"
 #include "vk_log.h"
 #include "vk_nir.h"
@@ -315,3 +317,105 @@ vk_pipeline_robustness_state_fill(const struct vk_device *device,
    if (rs->images == VK_PIPELINE_ROBUSTNESS_IMAGE_BEHAVIOR_DEVICE_DEFAULT_EXT)
       rs->images = vk_device_default_robust_image_behavior(device);
 }
+
+void *
+vk_pipeline_zalloc(struct vk_device *device,
+                   const struct vk_pipeline_ops *ops,
+                   VkPipelineBindPoint bind_point,
+                   VkPipelineCreateFlags2KHR flags,
+                   const VkAllocationCallbacks *alloc,
+                   size_t size)
+{
+   struct vk_pipeline *pipeline;
+
+   pipeline = vk_object_zalloc(device, alloc, size, VK_OBJECT_TYPE_PIPELINE);
+   if (pipeline == NULL)
+      return NULL;
+
+   pipeline->ops = ops;
+   pipeline->bind_point = bind_point;
+   pipeline->flags = flags;
+
+   return pipeline;
+}
+
+void
+vk_pipeline_free(struct vk_device *device,
+                 const VkAllocationCallbacks *alloc,
+                 struct vk_pipeline *pipeline)
+{
+   vk_object_free(device, alloc, &pipeline->base);
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_DestroyPipeline(VkDevice _device,
+                          VkPipeline _pipeline,
+                          const VkAllocationCallbacks *pAllocator)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_pipeline, pipeline, _pipeline);
+
+   if (pipeline == NULL)
+      return;
+
+   pipeline->ops->destroy(device, pipeline, pAllocator);
+}
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_GetPipelineExecutablePropertiesKHR(
+   VkDevice _device,
+   const VkPipelineInfoKHR *pPipelineInfo,
+   uint32_t *pExecutableCount,
+   VkPipelineExecutablePropertiesKHR *pProperties)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_pipeline, pipeline, pPipelineInfo->pipeline);
+
+   return pipeline->ops->get_executable_properties(device, pipeline,
+                                                   pExecutableCount,
+                                                   pProperties);
+}
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_GetPipelineExecutableStatisticsKHR(
+    VkDevice _device,
+    const VkPipelineExecutableInfoKHR *pExecutableInfo,
+    uint32_t *pStatisticCount,
+    VkPipelineExecutableStatisticKHR *pStatistics)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_pipeline, pipeline, pExecutableInfo->pipeline);
+
+   return pipeline->ops->get_executable_statistics(device, pipeline,
+                                                   pExecutableInfo->executableIndex,
+                                                   pStatisticCount, pStatistics);
+}
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_GetPipelineExecutableInternalRepresentationsKHR(
+    VkDevice _device,
+    const VkPipelineExecutableInfoKHR *pExecutableInfo,
+    uint32_t *pInternalRepresentationCount,
+    VkPipelineExecutableInternalRepresentationKHR* pInternalRepresentations)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_pipeline, pipeline, pExecutableInfo->pipeline);
+
+   return pipeline->ops->get_internal_representations(device, pipeline,
+                                                      pExecutableInfo->executableIndex,
+                                                      pInternalRepresentationCount,
+                                                      pInternalRepresentations);
+}
+
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdBindPipeline(VkCommandBuffer commandBuffer,
+                          VkPipelineBindPoint pipelineBindPoint,
+                          VkPipeline _pipeline)
+{
+   VK_FROM_HANDLE(vk_command_buffer, cmd_buffer, commandBuffer);
+   VK_FROM_HANDLE(vk_pipeline, pipeline, _pipeline);
+
+   assert(pipeline->bind_point == pipelineBindPoint);
+
+   pipeline->ops->cmd_bind(cmd_buffer, pipeline);
+}
diff --git a/src/vulkan/runtime/vk_pipeline.h b/src/vulkan/runtime/vk_pipeline.h
index b003e41442662..62ae730e1e440 100644
--- a/src/vulkan/runtime/vk_pipeline.h
+++ b/src/vulkan/runtime/vk_pipeline.h
@@ -24,7 +24,7 @@
 #ifndef VK_PIPELINE_H
 #define VK_PIPELINE_H
 
-#include "vulkan/vulkan_core.h"
+#include "vk_object.h"
 #include "vk_util.h"
 
 #include <stdbool.h>
@@ -32,6 +32,7 @@
 struct nir_shader;
 struct nir_shader_compiler_options;
 struct spirv_to_nir_options;
+struct vk_command_buffer;
 struct vk_device;
 
 #ifdef __cplusplus
@@ -146,6 +147,58 @@ vk_graph_pipeline_create_flags(const VkExecutionGraphPipelineCreateInfoAMDX *inf
 }
 #endif
 
+struct vk_pipeline_ops;
+
+struct vk_pipeline {
+   struct vk_object_base base;
+
+   const struct vk_pipeline_ops *ops;
+
+   VkPipelineBindPoint bind_point;
+   VkPipelineCreateFlags2KHR flags;
+};
+
+VK_DEFINE_NONDISP_HANDLE_CASTS(vk_pipeline, base, VkPipeline,
+                               VK_OBJECT_TYPE_PIPELINE);
+
+struct vk_pipeline_ops {
+   void (*destroy)(struct vk_device *device,
+                   struct vk_pipeline *pipeline,
+                   const VkAllocationCallbacks *pAllocator);
+
+   VkResult (*get_executable_properties)(struct vk_device *device,
+                                         struct vk_pipeline *pipeline,
+                                         uint32_t *executable_count,
+                                         VkPipelineExecutablePropertiesKHR *properties);
+
+   VkResult (*get_executable_statistics)(struct vk_device *device,
+                                         struct vk_pipeline *pipeline,
+                                         uint32_t executable_index,
+                                         uint32_t *statistic_count,
+                                         VkPipelineExecutableStatisticKHR *statistics);
+
+   VkResult (*get_internal_representations)(
+      struct vk_device *device,
+      struct vk_pipeline *pipeline,
+      uint32_t executable_index,
+      uint32_t *internal_representation_count,
+      VkPipelineExecutableInternalRepresentationKHR* internal_representations);
+
+   void (*cmd_bind)(struct vk_command_buffer *cmd_buffer,
+                    struct vk_pipeline *pipeline);
+};
+
+void *vk_pipeline_zalloc(struct vk_device *device,
+                         const struct vk_pipeline_ops *ops,
+                         VkPipelineBindPoint bind_point,
+                         VkPipelineCreateFlags2KHR flags,
+                         const VkAllocationCallbacks *alloc,
+                         size_t size);
+
+void vk_pipeline_free(struct vk_device *device,
+                      const VkAllocationCallbacks *alloc,
+                      struct vk_pipeline *pipeline);
+
 #ifdef __cplusplus
 }
 #endif
-- 
GitLab


From e388bf261b92818fe4eb9444634764e4b934b880 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Wed, 3 Jan 2024 14:55:51 -0600
Subject: [PATCH 10/22] vulkan: Add push constant ranges to vk_pipeline_layout

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_limits.h          | 36 +++++++++++++++++++++++++
 src/vulkan/runtime/vk_pipeline_layout.c |  6 +++++
 src/vulkan/runtime/vk_pipeline_layout.h |  6 +++++
 3 files changed, 48 insertions(+)

diff --git a/src/vulkan/runtime/vk_limits.h b/src/vulkan/runtime/vk_limits.h
index e58e02fa95f04..e4756794b0802 100644
--- a/src/vulkan/runtime/vk_limits.h
+++ b/src/vulkan/runtime/vk_limits.h
@@ -26,6 +26,42 @@
 
 #define MESA_VK_MAX_DESCRIPTOR_SETS 32
 
+/* From the Vulkan 1.3.274 spec:
+ *
+ *    VUID-VkPipelineLayoutCreateInfo-pPushConstantRanges-00292
+ *
+ *    "Any two elements of pPushConstantRanges must not include the same
+ *    stage in stageFlags"
+ *
+ * and
+ *
+ *    VUID-VkPushConstantRange-stageFlags-requiredbitmask
+ *
+ *    "stageFlags must not be 0"
+ *
+ * This means that the number of push constant ranges is effectively bounded
+ * by the number of possible shader stages.  Not the number of stages that can
+ * be compiled together (a pipeline layout can be used in multiple pipelnes
+ * wth different sets of shaders) but the total number of stage bits supported
+ * by the implementation.  Currently, those are
+ *
+ *    - VK_SHADER_STAGE_VERTEX_BIT
+ *    - VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT
+ *    - VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT
+ *    - VK_SHADER_STAGE_GEOMETRY_BIT
+ *    - VK_SHADER_STAGE_FRAGMENT_BIT
+ *    - VK_SHADER_STAGE_COMPUTE_BIT
+ *    - VK_SHADER_STAGE_RAYGEN_BIT_KHR
+ *    - VK_SHADER_STAGE_ANY_HIT_BIT_KHR
+ *    - VK_SHADER_STAGE_CLOSEST_HIT_BIT_KHR
+ *    - VK_SHADER_STAGE_MISS_BIT_KHR
+ *    - VK_SHADER_STAGE_INTERSECTION_BIT_KHR
+ *    - VK_SHADER_STAGE_CALLABLE_BIT_KHR
+ *    - VK_SHADER_STAGE_TASK_BIT_EXT
+ *    - VK_SHADER_STAGE_MESH_BIT_EXT
+ */
+#define MESA_VK_MAX_PUSH_CONSTANT_RANGES 14
+
 #define MESA_VK_MAX_VERTEX_BINDINGS 32
 #define MESA_VK_MAX_VERTEX_ATTRIBUTES 32
 
diff --git a/src/vulkan/runtime/vk_pipeline_layout.c b/src/vulkan/runtime/vk_pipeline_layout.c
index 3a04e7dc52a8f..77653464835c8 100644
--- a/src/vulkan/runtime/vk_pipeline_layout.c
+++ b/src/vulkan/runtime/vk_pipeline_layout.c
@@ -55,6 +55,12 @@ vk_pipeline_layout_init(struct vk_device *device,
       else
          layout->set_layouts[s] = NULL;
    }
+
+   assert(pCreateInfo->pushConstantRangeCount <
+          MESA_VK_MAX_PUSH_CONSTANT_RANGES);
+   layout->push_range_count = pCreateInfo->pushConstantRangeCount;
+   for (uint32_t r = 0; r < pCreateInfo->pushConstantRangeCount; r++)
+      layout->push_ranges[r] = pCreateInfo->pPushConstantRanges[r];
 }
 
 void *
diff --git a/src/vulkan/runtime/vk_pipeline_layout.h b/src/vulkan/runtime/vk_pipeline_layout.h
index 4bffdc906c7f4..f71110c20a596 100644
--- a/src/vulkan/runtime/vk_pipeline_layout.h
+++ b/src/vulkan/runtime/vk_pipeline_layout.h
@@ -62,6 +62,12 @@ struct vk_pipeline_layout {
    /** Array of pointers to descriptor set layouts, indexed by set index */
    struct vk_descriptor_set_layout *set_layouts[MESA_VK_MAX_DESCRIPTOR_SETS];
 
+   /** Number of push constant ranges in this pipeline layout */
+   uint32_t push_range_count;
+
+   /** Array of push constant ranges */
+   VkPushConstantRange push_ranges[MESA_VK_MAX_PUSH_CONSTANT_RANGES];
+
    /** Destroy callback
     *
     * Will be initially set to vk_pipeline_layout_destroy() but may be set to
-- 
GitLab


From 4f439e718ef2ba93c79f9c03babd964439feda00 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 11 Jan 2024 11:08:57 -0600
Subject: [PATCH 11/22] vulkan: Add a BLAKE3 hash to vk_descriptor_set_layout

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_descriptor_set_layout.h | 8 ++++++++
 1 file changed, 8 insertions(+)

diff --git a/src/vulkan/runtime/vk_descriptor_set_layout.h b/src/vulkan/runtime/vk_descriptor_set_layout.h
index 857ec1a7beaea..b01f30157e4fa 100644
--- a/src/vulkan/runtime/vk_descriptor_set_layout.h
+++ b/src/vulkan/runtime/vk_descriptor_set_layout.h
@@ -25,6 +25,7 @@
 
 #include "vk_object.h"
 
+#include "util/mesa-blake3.h"
 #include "util/u_atomic.h"
 
 #ifdef __cplusplus
@@ -34,6 +35,13 @@ extern "C" {
 struct vk_descriptor_set_layout {
    struct vk_object_base base;
 
+   /* BLAKE3 hash of the descriptor set layout.  This is used by the common
+    * pipeline code to properly cache shaders, including handling pipeline
+    * layouts.  It must be populated by the driver or you risk pipeline cache
+    * collisions.
+    */
+   blake3_hash blake3;
+
    void (*destroy)(struct vk_device *device,
                    struct vk_descriptor_set_layout *layout);
 
-- 
GitLab


From 6b32cf04a820538863baea87b5a4083d56a73e84 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 2 Jan 2024 17:37:47 -0600
Subject: [PATCH 12/22] vulkan: Add generic graphics and compute VkPipeline
 implementations

These implementations are built on top of vk_shader.  For the most part,
the driver shouldn't notice a difference between draws consuming
pipelines vs. draws consuming shaders.  The only real difference is
that, when vk_driver_shader_ops::compile() is called for pipelines, a
struct vk_graphics_pipeline_state is provided.  For shader objects, the
state object will be NULL indicating that all state is unknown.  Besides
that, all the rest of the differences between Vulkan 1.0 pipelines,
VK_EXT_graphics_pipeline_library, and VK_EXT_shader_object are handled
by the Vulkan runtime code.

Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
---
 src/vulkan/runtime/vk_command_buffer.h |    8 +
 src/vulkan/runtime/vk_limits.h         |    3 +
 src/vulkan/runtime/vk_pipeline.c       | 1731 +++++++++++++++++++++++-
 src/vulkan/runtime/vk_pipeline.h       |    4 +
 src/vulkan/runtime/vk_shader.c         |   11 +-
 src/vulkan/runtime/vk_shader.h         |   72 +
 6 files changed, 1827 insertions(+), 2 deletions(-)

diff --git a/src/vulkan/runtime/vk_command_buffer.h b/src/vulkan/runtime/vk_command_buffer.h
index e49b3077d34a9..6ae637ca5ab4d 100644
--- a/src/vulkan/runtime/vk_command_buffer.h
+++ b/src/vulkan/runtime/vk_command_buffer.h
@@ -185,6 +185,14 @@ struct vk_command_buffer {
    struct vk_attachment_state _attachments[8];
 
    VkRenderPassSampleLocationsBeginInfoEXT *pass_sample_locations;
+
+   /**
+    * Bitmask of shader stages bound via a vk_pipeline since the last call to
+    * vkBindShadersEXT().
+    *
+    * Used by the common vk_pipeline implementation
+    */
+   VkShaderStageFlags pipeline_shader_stages;
 };
 
 VK_DEFINE_HANDLE_CASTS(vk_command_buffer, base, VkCommandBuffer,
diff --git a/src/vulkan/runtime/vk_limits.h b/src/vulkan/runtime/vk_limits.h
index e4756794b0802..50bfde0c0ebee 100644
--- a/src/vulkan/runtime/vk_limits.h
+++ b/src/vulkan/runtime/vk_limits.h
@@ -24,6 +24,9 @@
 #ifndef VK_LIMITS_H
 #define VK_LIMITS_H
 
+/* Maximun number of shader stages in a single graphics pipeline */
+#define MESA_VK_MAX_GRAPHICS_PIPELINE_STAGES 5
+
 #define MESA_VK_MAX_DESCRIPTOR_SETS 32
 
 /* From the Vulkan 1.3.274 spec:
diff --git a/src/vulkan/runtime/vk_pipeline.c b/src/vulkan/runtime/vk_pipeline.c
index ac7b66a13f46f..28c66dd6f0fff 100644
--- a/src/vulkan/runtime/vk_pipeline.c
+++ b/src/vulkan/runtime/vk_pipeline.c
@@ -23,18 +23,23 @@
 
 #include "vk_pipeline.h"
 
+#include "vk_alloc.h"
 #include "vk_common_entrypoints.h"
 #include "vk_command_buffer.h"
+#include "vk_descriptor_set_layout.h"
 #include "vk_device.h"
+#include "vk_graphics_state.h"
 #include "vk_log.h"
 #include "vk_nir.h"
+#include "vk_physical_device.h"
+#include "vk_pipeline_layout.h"
+#include "vk_shader.h"
 #include "vk_shader_module.h"
 #include "vk_util.h"
 
 #include "nir_serialize.h"
 
 #include "util/mesa-sha1.h"
-#include "util/mesa-blake3.h"
 
 bool
 vk_pipeline_shader_stage_is_null(const VkPipelineShaderStageCreateInfo *info)
@@ -419,3 +424,1727 @@ vk_common_CmdBindPipeline(VkCommandBuffer commandBuffer,
 
    pipeline->ops->cmd_bind(cmd_buffer, pipeline);
 }
+
+static const struct vk_pipeline_cache_object_ops pipeline_shader_cache_ops;
+
+static struct vk_shader *
+vk_shader_from_cache_obj(struct vk_pipeline_cache_object *object)
+{
+   assert(object->ops == &pipeline_shader_cache_ops);
+   return container_of(object, struct vk_shader, pipeline.cache_obj);
+}
+
+static bool
+vk_pipeline_shader_serialize(struct vk_pipeline_cache_object *object,
+                             struct blob *blob)
+{
+   struct vk_shader *shader = vk_shader_from_cache_obj(object);
+   struct vk_device *device = shader->base.device;
+
+   return shader->ops->serialize(device, shader, blob);
+}
+
+static void
+vk_shader_init_cache_obj(struct vk_device *device, struct vk_shader *shader,
+                         const void *key_data, size_t key_size)
+{
+   assert(key_size == sizeof(shader->pipeline.cache_key));
+   memcpy(&shader->pipeline.cache_key, key_data,
+          sizeof(shader->pipeline.cache_key));
+
+   vk_pipeline_cache_object_init(device, &shader->pipeline.cache_obj,
+                                 &pipeline_shader_cache_ops,
+                                 &shader->pipeline.cache_key,
+                                 sizeof(shader->pipeline.cache_key));
+}
+
+static struct vk_pipeline_cache_object *
+vk_pipeline_shader_deserialize(struct vk_pipeline_cache *cache,
+                               const void *key_data, size_t key_size,
+                               struct blob_reader *blob)
+{
+   struct vk_device *device = cache->base.device;
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+
+   /* TODO: Do we really want to always use the latest version? */
+   const uint32_t version = device->physical->properties.shaderBinaryVersion;
+
+   struct vk_shader *shader;
+   VkResult result = ops->deserialize(device, blob, version,
+                                      &device->alloc, &shader);
+   if (result != VK_SUCCESS) {
+      assert(result == VK_ERROR_OUT_OF_HOST_MEMORY);
+      return NULL;
+   }
+
+   vk_shader_init_cache_obj(device, shader, key_data, key_size);
+
+   return &shader->pipeline.cache_obj;
+}
+
+static void
+vk_pipeline_shader_destroy(struct vk_device *device,
+                           struct vk_pipeline_cache_object *object)
+{
+   struct vk_shader *shader = vk_shader_from_cache_obj(object);
+   assert(shader->base.device == device);
+
+   vk_shader_destroy(device, shader, &device->alloc);
+}
+
+static const struct vk_pipeline_cache_object_ops pipeline_shader_cache_ops = {
+   .serialize = vk_pipeline_shader_serialize,
+   .deserialize = vk_pipeline_shader_deserialize,
+   .destroy = vk_pipeline_shader_destroy,
+};
+
+static struct vk_shader *
+vk_shader_ref(struct vk_shader *shader)
+{
+   vk_pipeline_cache_object_ref(&shader->pipeline.cache_obj);
+   return shader;
+}
+
+static void
+vk_shader_unref(struct vk_device *device, struct vk_shader *shader)
+{
+   vk_pipeline_cache_object_unref(device, &shader->pipeline.cache_obj);
+}
+
+struct vk_pipeline_tess_info {
+   unsigned tcs_vertices_out : 8;
+   unsigned primitive_mode : 2; /* tess_primitive_mode */
+   unsigned spacing : 2; /*gl_tess_spacing*/
+   bool ccw : 1;
+   bool point_mode : 1;
+   unsigned _pad : 18;
+};
+static_assert(sizeof(struct vk_pipeline_tess_info) == 4,
+              "This struct has no holes");
+
+static void
+vk_pipeline_gather_nir_tess_info(const nir_shader *nir,
+                                 struct vk_pipeline_tess_info *info)
+{
+   info->tcs_vertices_out  = nir->info.tess.tcs_vertices_out;
+   info->primitive_mode    = nir->info.tess._primitive_mode;
+   info->spacing           = nir->info.tess.spacing;
+   info->ccw               = nir->info.tess.ccw;
+   info->point_mode        = nir->info.tess.point_mode;
+}
+
+static void
+vk_pipeline_replace_nir_tess_info(nir_shader *nir,
+                                  const struct vk_pipeline_tess_info *info)
+{
+   nir->info.tess.tcs_vertices_out  = info->tcs_vertices_out;
+   nir->info.tess._primitive_mode   = info->primitive_mode;
+   nir->info.tess.spacing           = info->spacing;
+   nir->info.tess.ccw               = info->ccw;
+   nir->info.tess.point_mode        = info->point_mode;
+}
+
+static void
+vk_pipeline_tess_info_merge(struct vk_pipeline_tess_info *dst,
+                            const struct vk_pipeline_tess_info *src)
+{
+   /* The Vulkan 1.0.38 spec, section 21.1 Tessellator says:
+    *
+    *    "PointMode. Controls generation of points rather than triangles
+    *     or lines. This functionality defaults to disabled, and is
+    *     enabled if either shader stage includes the execution mode.
+    *
+    * and about Triangles, Quads, IsoLines, VertexOrderCw, VertexOrderCcw,
+    * PointMode, SpacingEqual, SpacingFractionalEven, SpacingFractionalOdd,
+    * and OutputVertices, it says:
+    *
+    *    "One mode must be set in at least one of the tessellation
+    *     shader stages."
+    *
+    * So, the fields can be set in either the TCS or TES, but they must
+    * agree if set in both.
+    */
+   assert(dst->tcs_vertices_out == 0 ||
+          src->tcs_vertices_out == 0 ||
+          dst->tcs_vertices_out == src->tcs_vertices_out);
+   dst->tcs_vertices_out |= src->tcs_vertices_out;
+
+   static_assert(TESS_SPACING_UNSPECIFIED == 0);
+   assert(dst->spacing == TESS_SPACING_UNSPECIFIED ||
+          src->spacing == TESS_SPACING_UNSPECIFIED ||
+          dst->spacing == src->spacing);
+   dst->spacing |= src->spacing;
+
+   static_assert(TESS_PRIMITIVE_UNSPECIFIED == 0);
+   assert(dst->primitive_mode == TESS_PRIMITIVE_UNSPECIFIED ||
+          src->primitive_mode == TESS_PRIMITIVE_UNSPECIFIED ||
+          dst->primitive_mode == src->primitive_mode);
+   dst->primitive_mode |= src->primitive_mode;
+   dst->ccw |= src->ccw;
+   dst->point_mode |= src->point_mode;
+}
+
+struct vk_pipeline_precomp_shader {
+   struct vk_pipeline_cache_object cache_obj;
+
+   /* Key for this cache_obj in the pipeline cache.
+    *
+    * This is always the output of vk_pipeline_hash_shader_stage() so it must
+    * be a SHA1 hash.
+    */
+   uint8_t cache_key[SHA1_DIGEST_LENGTH];
+
+   gl_shader_stage stage;
+
+   struct vk_pipeline_robustness_state rs;
+
+   /* Tessellation info if the shader is a tessellation shader */
+   struct vk_pipeline_tess_info tess;
+
+   /* Hash of the vk_pipeline_precomp_shader
+    *
+    * This is the hash of the final compiled NIR together with tess info and
+    * robustness state.  It's used as a key for final binary lookups.  By
+    * having this as a separate key, we can de-duplicate cases where you have
+    * different SPIR-V or specialization constants but end up compiling the
+    * same NIR shader in the end anyway.
+    */
+   blake3_hash blake3;
+
+   struct blob nir_blob;
+};
+
+static struct vk_pipeline_precomp_shader *
+vk_pipeline_precomp_shader_ref(struct vk_pipeline_precomp_shader *shader)
+{
+   vk_pipeline_cache_object_ref(&shader->cache_obj);
+   return shader;
+}
+
+static void
+vk_pipeline_precomp_shader_unref(struct vk_device *device,
+                                 struct vk_pipeline_precomp_shader *shader)
+{
+   vk_pipeline_cache_object_unref(device, &shader->cache_obj);
+}
+
+static const struct vk_pipeline_cache_object_ops pipeline_precomp_shader_cache_ops;
+
+static struct vk_pipeline_precomp_shader *
+vk_pipeline_precomp_shader_from_cache_obj(struct vk_pipeline_cache_object *obj)
+{
+   assert(obj->ops == & pipeline_precomp_shader_cache_ops);
+   return container_of(obj, struct vk_pipeline_precomp_shader, cache_obj);
+}
+
+static struct vk_pipeline_precomp_shader *
+vk_pipeline_precomp_shader_create(struct vk_device *device,
+                                  const void *key_data, size_t key_size,
+                                  const struct vk_pipeline_robustness_state *rs,
+                                  nir_shader *nir)
+{
+   struct blob blob;
+   blob_init(&blob);
+
+   nir_serialize(&blob, nir, false);
+
+   if (blob.out_of_memory)
+      goto fail_blob;
+
+   struct vk_pipeline_precomp_shader *shader =
+      vk_zalloc(&device->alloc, sizeof(*shader), 8,
+                VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
+   if (shader == NULL)
+      goto fail_blob;
+
+   assert(sizeof(shader->cache_key) == key_size);
+   memcpy(shader->cache_key, key_data, sizeof(shader->cache_key));
+
+   vk_pipeline_cache_object_init(device, &shader->cache_obj,
+                                 &pipeline_precomp_shader_cache_ops,
+                                 shader->cache_key,
+                                 sizeof(shader->cache_key));
+
+   shader->stage = nir->info.stage;
+   shader->rs = *rs;
+
+   vk_pipeline_gather_nir_tess_info(nir, &shader->tess);
+
+   struct mesa_blake3 blake3_ctx;
+   _mesa_blake3_init(&blake3_ctx);
+   _mesa_blake3_update(&blake3_ctx, rs, sizeof(*rs));
+   _mesa_blake3_update(&blake3_ctx, blob.data, blob.size);
+   _mesa_blake3_final(&blake3_ctx, shader->blake3);
+
+   shader->nir_blob = blob;
+
+   return shader;
+
+fail_blob:
+   blob_finish(&blob);
+
+   return NULL;
+}
+
+static bool
+vk_pipeline_precomp_shader_serialize(struct vk_pipeline_cache_object *obj,
+                                     struct blob *blob)
+{
+   struct vk_pipeline_precomp_shader *shader =
+      vk_pipeline_precomp_shader_from_cache_obj(obj);
+
+   blob_write_uint32(blob, shader->stage);
+   blob_write_bytes(blob, &shader->rs, sizeof(shader->rs));
+   blob_write_bytes(blob, &shader->tess, sizeof(shader->tess));
+   blob_write_bytes(blob, shader->blake3, sizeof(shader->blake3));
+   blob_write_uint64(blob, shader->nir_blob.size);
+   blob_write_bytes(blob, shader->nir_blob.data, shader->nir_blob.size);
+
+   return !blob->out_of_memory;
+}
+
+static struct vk_pipeline_cache_object *
+vk_pipeline_precomp_shader_deserialize(struct vk_pipeline_cache *cache,
+                                       const void *key_data, size_t key_size,
+                                       struct blob_reader *blob)
+{
+   struct vk_device *device = cache->base.device;
+
+   struct vk_pipeline_precomp_shader *shader =
+      vk_zalloc(&device->alloc, sizeof(*shader), 8,
+                VK_SYSTEM_ALLOCATION_SCOPE_DEVICE);
+   if (shader == NULL)
+      return NULL;
+
+   assert(sizeof(shader->cache_key) == key_size);
+   memcpy(shader->cache_key, key_data, sizeof(shader->cache_key));
+
+   vk_pipeline_cache_object_init(device, &shader->cache_obj,
+                                 &pipeline_precomp_shader_cache_ops,
+                                 shader->cache_key,
+                                 sizeof(shader->cache_key));
+
+   shader->stage = blob_read_uint32(blob);
+   blob_copy_bytes(blob, &shader->rs, sizeof(shader->rs));
+   blob_copy_bytes(blob, &shader->tess, sizeof(shader->tess));
+   blob_copy_bytes(blob, shader->blake3, sizeof(shader->blake3));
+
+   uint64_t nir_size = blob_read_uint64(blob);
+   if (blob->overrun || nir_size > SIZE_MAX)
+      goto fail_shader;
+
+   const void *nir_data = blob_read_bytes(blob, nir_size);
+   if (blob->overrun)
+      goto fail_shader;
+
+   blob_init(&shader->nir_blob);
+   blob_write_bytes(&shader->nir_blob, nir_data, nir_size);
+   if (shader->nir_blob.out_of_memory)
+      goto fail_nir_blob;
+
+   return &shader->cache_obj;
+
+fail_nir_blob:
+   blob_finish(&shader->nir_blob);
+fail_shader:
+   vk_pipeline_cache_object_finish(&shader->cache_obj);
+   vk_free(&device->alloc, shader);
+
+   return NULL;
+}
+
+static void
+vk_pipeline_precomp_shader_destroy(struct vk_device *device,
+                                   struct vk_pipeline_cache_object *obj)
+{
+   struct vk_pipeline_precomp_shader *shader =
+      vk_pipeline_precomp_shader_from_cache_obj(obj);
+
+   blob_finish(&shader->nir_blob);
+   vk_pipeline_cache_object_finish(&shader->cache_obj);
+   vk_free(&device->alloc, shader);
+}
+
+static nir_shader *
+vk_pipeline_precomp_shader_get_nir(const struct vk_pipeline_precomp_shader *shader,
+                                   const struct nir_shader_compiler_options *nir_options)
+{
+   struct blob_reader blob;
+   blob_reader_init(&blob, shader->nir_blob.data, shader->nir_blob.size);
+
+   nir_shader *nir = nir_deserialize(NULL, nir_options, &blob);
+   if (blob.overrun) {
+      ralloc_free(nir);
+      return NULL;
+   }
+
+   return nir;
+}
+
+static const struct vk_pipeline_cache_object_ops pipeline_precomp_shader_cache_ops = {
+   .serialize = vk_pipeline_precomp_shader_serialize,
+   .deserialize = vk_pipeline_precomp_shader_deserialize,
+   .destroy = vk_pipeline_precomp_shader_destroy,
+};
+
+static VkResult
+vk_pipeline_precompile_shader(struct vk_device *device,
+                              struct vk_pipeline_cache *cache,
+                              VkPipelineCreateFlags2KHR pipeline_flags,
+                              const void *pipeline_info_pNext,
+                              const VkPipelineShaderStageCreateInfo *info,
+                              struct vk_pipeline_precomp_shader **ps_out)
+{
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+   VkResult result;
+
+   struct vk_pipeline_robustness_state rs;
+   vk_pipeline_robustness_state_fill(device, &rs,
+                                     pipeline_info_pNext,
+                                     info->pNext);
+
+   uint8_t stage_sha1[SHA1_DIGEST_LENGTH];
+   vk_pipeline_hash_shader_stage(info, &rs, stage_sha1);
+
+   if (cache != NULL) {
+      struct vk_pipeline_cache_object *cache_obj =
+         vk_pipeline_cache_lookup_object(cache, stage_sha1, sizeof(stage_sha1),
+                                         &pipeline_precomp_shader_cache_ops,
+                                         NULL /* cache_hit */);
+      if (cache_obj != NULL) {
+         *ps_out = vk_pipeline_precomp_shader_from_cache_obj(cache_obj);
+         return VK_SUCCESS;
+      }
+   }
+
+   if (pipeline_flags &
+       VK_PIPELINE_CREATE_2_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT_KHR)
+      return VK_PIPELINE_COMPILE_REQUIRED;
+
+   const struct nir_shader_compiler_options *nir_options =
+      ops->get_nir_options(device->physical, info->stage, &rs);
+
+   const struct spirv_to_nir_options spirv_options =
+      ops->get_spirv_options(device->physical, info->stage, &rs);
+
+   nir_shader *nir;
+   result = vk_pipeline_shader_stage_to_nir(device, info, &spirv_options,
+                                            nir_options, NULL, &nir);
+   if (result != VK_SUCCESS)
+      return result;
+
+   if (ops->preprocess_nir != NULL)
+      ops->preprocess_nir(device->physical, nir);
+
+   struct vk_pipeline_precomp_shader *shader =
+      vk_pipeline_precomp_shader_create(device, stage_sha1,
+                                        sizeof(stage_sha1),
+                                        &rs, nir);
+   ralloc_free(nir);
+   if (shader == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   if (cache != NULL) {
+      struct vk_pipeline_cache_object *cache_obj = &shader->cache_obj;
+      cache_obj = vk_pipeline_cache_add_object(cache, cache_obj);
+      shader = vk_pipeline_precomp_shader_from_cache_obj(cache_obj);
+   }
+
+   *ps_out = shader;
+
+   return VK_SUCCESS;
+}
+
+struct vk_pipeline_stage {
+   gl_shader_stage stage;
+
+   struct vk_pipeline_precomp_shader *precomp;
+   struct vk_shader *shader;
+};
+
+static int
+cmp_vk_pipeline_stages(const void *_a, const void *_b)
+{
+   const struct vk_pipeline_stage *a = _a, *b = _b;
+   return vk_shader_cmp_graphics_stages(a->stage, b->stage);
+}
+
+static bool
+vk_pipeline_stage_is_null(const struct vk_pipeline_stage *stage)
+{
+   return stage->precomp == NULL && stage->shader == NULL;
+}
+
+static void
+vk_pipeline_stage_finish(struct vk_device *device,
+                         struct vk_pipeline_stage *stage)
+{
+   if (stage->precomp != NULL)
+      vk_pipeline_precomp_shader_unref(device, stage->precomp);
+
+   if (stage->shader)
+      vk_shader_unref(device, stage->shader);
+}
+
+static struct vk_pipeline_stage
+vk_pipeline_stage_clone(const struct vk_pipeline_stage *in)
+{
+   struct vk_pipeline_stage out = {
+      .stage = in->stage,
+   };
+
+   if (in->precomp)
+      out.precomp = vk_pipeline_precomp_shader_ref(in->precomp);
+
+   if (in->shader)
+      out.shader = vk_shader_ref(in->shader);
+
+   return out;
+}
+
+struct vk_graphics_pipeline {
+   struct vk_pipeline base;
+
+   union {
+      struct {
+         struct vk_graphics_pipeline_all_state all_state;
+         struct vk_graphics_pipeline_state state;
+      } lib;
+
+      struct {
+         struct vk_vertex_input_state _dynamic_vi;
+         struct vk_sample_locations_state _dynamic_sl;
+         struct vk_dynamic_graphics_state dynamic;
+      } linked;
+   };
+
+   uint32_t set_layout_count;
+   struct vk_descriptor_set_layout *set_layouts[MESA_VK_MAX_DESCRIPTOR_SETS];
+
+   uint32_t stage_count;
+   struct vk_pipeline_stage stages[MESA_VK_MAX_GRAPHICS_PIPELINE_STAGES];
+};
+
+static void
+vk_graphics_pipeline_destroy(struct vk_device *device,
+                             struct vk_pipeline *pipeline,
+                             const VkAllocationCallbacks *pAllocator)
+{
+   struct vk_graphics_pipeline *gfx_pipeline =
+      container_of(pipeline, struct vk_graphics_pipeline, base);
+
+   for (uint32_t i = 0; i < gfx_pipeline->stage_count; i++)
+      vk_pipeline_stage_finish(device, &gfx_pipeline->stages[i]);
+
+   for (uint32_t i = 0; i < gfx_pipeline->set_layout_count; i++) {
+      if (gfx_pipeline->set_layouts[i] != NULL)
+         vk_descriptor_set_layout_unref(device, gfx_pipeline->set_layouts[i]);
+   }
+
+   vk_pipeline_free(device, pAllocator, pipeline);
+}
+
+static bool
+vk_device_supports_stage(struct vk_device *device,
+                         gl_shader_stage stage)
+{
+   const struct vk_features *features = &device->physical->supported_features;
+
+   switch (stage) {
+   case MESA_SHADER_VERTEX:
+   case MESA_SHADER_FRAGMENT:
+   case MESA_SHADER_COMPUTE:
+      return true;
+   case MESA_SHADER_TESS_CTRL:
+   case MESA_SHADER_TESS_EVAL:
+      return features->tessellationShader;
+   case MESA_SHADER_GEOMETRY:
+      return features->geometryShader;
+   case MESA_SHADER_TASK:
+      return features->taskShader;
+   case MESA_SHADER_MESH:
+      return features->meshShader;
+   default:
+      return false;
+   }
+}
+
+static const gl_shader_stage all_gfx_stages[] = {
+   MESA_SHADER_VERTEX,
+   MESA_SHADER_TESS_CTRL,
+   MESA_SHADER_TESS_EVAL,
+   MESA_SHADER_GEOMETRY,
+   MESA_SHADER_TASK,
+   MESA_SHADER_MESH,
+   MESA_SHADER_FRAGMENT,
+};
+
+static void
+vk_graphics_pipeline_cmd_bind(struct vk_command_buffer *cmd_buffer,
+                              struct vk_pipeline *pipeline)
+{
+   struct vk_device *device = cmd_buffer->base.device;
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+
+   struct vk_graphics_pipeline *gfx_pipeline = NULL;
+   struct vk_shader *stage_shader[PIPE_SHADER_MESH_TYPES] = { NULL, };
+   if (pipeline != NULL) {
+      assert(pipeline->bind_point == VK_PIPELINE_BIND_POINT_GRAPHICS);
+      assert(!(pipeline->flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR));
+      gfx_pipeline = container_of(pipeline, struct vk_graphics_pipeline, base);
+
+      for (uint32_t i = 0; i < gfx_pipeline->stage_count; i++) {
+         struct vk_shader *shader = gfx_pipeline->stages[i].shader;
+         stage_shader[shader->stage] = shader;
+      }
+   }
+
+   uint32_t stage_count = 0;
+   gl_shader_stage stages[ARRAY_SIZE(all_gfx_stages)];
+   struct vk_shader *shaders[ARRAY_SIZE(all_gfx_stages)];
+
+   VkShaderStageFlags vk_stages = 0;
+   for (uint32_t i = 0; i < ARRAY_SIZE(all_gfx_stages); i++) {
+      gl_shader_stage stage = all_gfx_stages[i];
+      if (!vk_device_supports_stage(device, stage)) {
+         assert(stage_shader[stage] == NULL);
+         continue;
+      }
+
+      vk_stages |= mesa_to_vk_shader_stage(stage);
+
+      stages[stage_count] = stage;
+      shaders[stage_count] = stage_shader[stage];
+      stage_count++;
+   }
+   ops->cmd_bind_shaders(cmd_buffer, stage_count, stages, shaders);
+
+   if (gfx_pipeline != NULL) {
+      cmd_buffer->pipeline_shader_stages |= vk_stages;
+      ops->cmd_set_dynamic_graphics_state(cmd_buffer,
+                                          &gfx_pipeline->linked.dynamic);
+   } else {
+      cmd_buffer->pipeline_shader_stages &= ~vk_stages;
+   }
+}
+
+static VkShaderCreateFlagsEXT
+vk_pipeline_to_shader_flags(VkPipelineCreateFlags2KHR pipeline_flags,
+                            gl_shader_stage stage)
+{
+   VkShaderCreateFlagsEXT shader_flags = 0;
+
+   if (pipeline_flags & VK_PIPELINE_CREATE_2_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_KHR)
+      shader_flags |= VK_SHADER_CREATE_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_MESA;
+
+   if (stage == MESA_SHADER_FRAGMENT) {
+      if (pipeline_flags & VK_PIPELINE_CREATE_2_RENDERING_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_KHR)
+         shader_flags |= VK_SHADER_CREATE_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_EXT;
+
+      if (pipeline_flags & VK_PIPELINE_CREATE_2_RENDERING_FRAGMENT_DENSITY_MAP_ATTACHMENT_BIT_EXT)
+         shader_flags |= VK_SHADER_CREATE_FRAGMENT_DENSITY_MAP_ATTACHMENT_BIT_EXT;
+   }
+
+   if (stage == MESA_SHADER_COMPUTE) {
+      if (pipeline_flags & VK_PIPELINE_CREATE_2_DISPATCH_BASE_BIT_KHR)
+         shader_flags |= VK_SHADER_CREATE_DISPATCH_BASE_BIT_EXT;
+   }
+
+   return shader_flags;
+}
+
+static VkResult
+vk_graphics_pipeline_compile_shaders(struct vk_device *device,
+                                     struct vk_pipeline_cache *cache,
+                                     struct vk_graphics_pipeline *pipeline,
+                                     struct vk_pipeline_layout *pipeline_layout,
+                                     const struct vk_graphics_pipeline_state *state,
+                                     uint32_t stage_count,
+                                     struct vk_pipeline_stage *stages,
+                                     VkPipelineCreationFeedback *stage_feedbacks)
+{
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+   VkResult result;
+
+   if (stage_count == 0)
+      return VK_SUCCESS;
+
+   /* If we're linking, throw away any previously compiled shaders as they
+    * likely haven't been properly linked.  We keep the precompiled shaders
+    * and we still look it up in the cache so it may still be fast.
+    */
+   if (pipeline->base.flags & VK_PIPELINE_CREATE_2_LINK_TIME_OPTIMIZATION_BIT_EXT) {
+      for (uint32_t i = 0; i < stage_count; i++) {
+         if (stages[i].shader != NULL) {
+            vk_shader_unref(device, stages[i].shader);
+            stages[i].shader = NULL;
+         }
+      }
+   }
+
+   bool have_all_shaders = true;
+   VkShaderStageFlags all_stages = 0;
+   struct vk_pipeline_precomp_shader *tcs_precomp = NULL, *tes_precomp = NULL;
+   for (uint32_t i = 0; i < stage_count; i++) {
+      all_stages |= mesa_to_vk_shader_stage(stages[i].stage);
+
+      if (stages[i].shader == NULL)
+         have_all_shaders = false;
+
+      if (stages[i].stage == MESA_SHADER_TESS_CTRL)
+         tcs_precomp = stages[i].precomp;
+
+      if (stages[i].stage == MESA_SHADER_TESS_EVAL)
+         tes_precomp = stages[i].precomp;
+   }
+
+   /* If we already have a shader for each stage, there's nothing to do. */
+   if (have_all_shaders)
+      return VK_SUCCESS;
+
+   struct vk_pipeline_tess_info tess_info = { ._pad = 0 };
+   if (tcs_precomp != NULL && tes_precomp != NULL) {
+      tess_info = tcs_precomp->tess;
+      vk_pipeline_tess_info_merge(&tess_info, &tes_precomp->tess);
+   }
+
+   struct mesa_blake3 blake3_ctx;
+   _mesa_blake3_init(&blake3_ctx);
+   for (uint32_t i = 0; i < pipeline->set_layout_count; i++) {
+      if (pipeline->set_layouts[i] != NULL) {
+         _mesa_blake3_update(&blake3_ctx, pipeline->set_layouts[i]->blake3,
+                           sizeof(pipeline->set_layouts[i]->blake3));
+      }
+   }
+   if (pipeline_layout != NULL) {
+      _mesa_blake3_update(&blake3_ctx, &pipeline_layout->push_ranges,
+                        sizeof(pipeline_layout->push_ranges[0]) *
+                           pipeline_layout->push_range_count);
+   }
+   blake3_hash layout_blake3;
+   _mesa_blake3_final(&blake3_ctx, layout_blake3);
+
+   /* Partition the shaders */
+   uint32_t part_count;
+   uint32_t partition[MESA_VK_MAX_GRAPHICS_PIPELINE_STAGES + 1] = { 0 };
+   if (pipeline->base.flags & VK_PIPELINE_CREATE_2_LINK_TIME_OPTIMIZATION_BIT_EXT) {
+      partition[1] = stage_count;
+      part_count = 1;
+   } else if (ops->link_geom_stages) {
+      if (stages[0].stage == MESA_SHADER_FRAGMENT) {
+         assert(stage_count == 1);
+         partition[1] = stage_count;
+         part_count = 1;
+      } else if (stages[stage_count - 1].stage == MESA_SHADER_FRAGMENT) {
+         /* In this case we have both */
+         assert(stage_count > 1);
+         partition[1] = stage_count - 1;
+         partition[2] = stage_count;
+         part_count = 2;
+      } else {
+         /* In this case we only have geometry */
+         partition[1] = stage_count;
+         part_count = 1;
+      }
+   } else {
+      /* Otherwise, we're don't want to link anything */
+      part_count = stage_count;
+      for (uint32_t i = 0; i < stage_count; i++)
+         partition[i + 1] = i + 1;
+   }
+
+   for (uint32_t p = 0; p < part_count; p++) {
+      const int64_t part_start = os_time_get_nano();
+
+      struct vk_shader_pipeline_cache_key shader_key = { 0 };
+
+      _mesa_blake3_init(&blake3_ctx);
+
+      VkShaderStageFlags part_stages = 0;
+      for (uint32_t i = partition[p]; i < partition[p + 1]; i++) {
+         const struct vk_pipeline_stage *stage = &stages[i];
+
+         part_stages |= mesa_to_vk_shader_stage(stage->stage);
+         _mesa_blake3_update(&blake3_ctx, stage->precomp->blake3,
+                           sizeof(stage->precomp->blake3));
+
+         VkShaderCreateFlagsEXT shader_flags =
+            vk_pipeline_to_shader_flags(pipeline->base.flags, stage->stage);
+         _mesa_blake3_update(&blake3_ctx, &shader_flags, sizeof(shader_flags));
+      }
+
+      blake3_hash state_blake3;
+      ops->hash_graphics_state(device->physical, state,
+                               part_stages, state_blake3);
+
+      _mesa_blake3_update(&blake3_ctx, state_blake3, sizeof(state_blake3));
+      _mesa_blake3_update(&blake3_ctx, layout_blake3, sizeof(layout_blake3));
+
+      if (part_stages & (VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT |
+                         VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT))
+         _mesa_blake3_update(&blake3_ctx, &tess_info, sizeof(tess_info));
+
+      /* The set of geometry stages used together is used to generate the
+       * nextStage mask as well as VK_SHADER_CREATE_NO_TASK_SHADER_BIT_EXT.
+       */
+      const VkShaderStageFlags geom_stages =
+         all_stages & ~VK_SHADER_STAGE_FRAGMENT_BIT;
+      _mesa_blake3_update(&blake3_ctx, &geom_stages, sizeof(geom_stages));
+
+      _mesa_blake3_final(&blake3_ctx, shader_key.blake3);
+
+      if (cache != NULL) {
+         /* From the Vulkan 1.3.278 spec:
+          *
+          *    "VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT
+          *    indicates that a readily usable pipeline or pipeline stage was
+          *    found in the pipelineCache specified by the application in the
+          *    pipeline creation command.
+          *
+          *    [...]
+          *
+          *    Note
+          *
+          *    Implementations are encouraged to provide a meaningful signal
+          *    to applications using this bit. The intention is to communicate
+          *    to the application that the pipeline or pipeline stage was
+          *    created “as fast as it gets” using the pipeline cache provided
+          *    by the application. If an implementation uses an internal
+          *    cache, it is discouraged from setting this bit as the feedback
+          *    would be unactionable."
+          *
+          * The cache_hit value returned by vk_pipeline_cache_lookup_object()
+          * is only set to true when the shader is found in the provided
+          * pipeline cache.  It is left false if we fail to find it in the
+          * memory cache but find it in the disk cache even though that's
+          * still a cache hit from the perspective of the compile pipeline.
+          */
+         bool all_shaders_found = true;
+         bool all_cache_hits = true;
+         for (uint32_t i = partition[p]; i < partition[p + 1]; i++) {
+            struct vk_pipeline_stage *stage = &stages[i];
+
+            shader_key.stage = stage->stage;
+
+            bool cache_hit = false;
+            struct vk_pipeline_cache_object *cache_obj =
+               vk_pipeline_cache_lookup_object(cache, &shader_key,
+                                               sizeof(shader_key),
+                                               &pipeline_shader_cache_ops,
+                                               &cache_hit);
+            if (cache_obj != NULL) {
+               stage->shader = vk_shader_from_cache_obj(cache_obj);
+            } else {
+               all_shaders_found = false;
+            }
+
+            if (cache_obj == NULL && !cache_hit)
+               all_cache_hits = false;
+         }
+
+         if (all_cache_hits) {
+            /* The pipeline cache only really helps if we hit for everything
+             * in the partition.  Otherwise, we have to go re-compile it all
+             * anyway.
+             */
+            for (uint32_t i = partition[p]; i < partition[p + 1]; i++) {
+               struct vk_pipeline_stage *stage = &stages[i];
+
+               stage_feedbacks[stage->stage].flags |=
+                  VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT;
+            }
+         }
+
+         if (all_shaders_found) {
+            /* Update duration to take cache lookups into account */
+            const int64_t part_end = os_time_get_nano();
+            for (uint32_t i = partition[p]; i < partition[p + 1]; i++) {
+               struct vk_pipeline_stage *stage = &stages[i];
+               stage_feedbacks[stage->stage].duration += part_end - part_start;
+            }
+            continue;
+         }
+      }
+
+      if (pipeline->base.flags &
+          VK_PIPELINE_CREATE_2_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT_KHR)
+         return VK_PIPELINE_COMPILE_REQUIRED;
+
+      struct vk_shader_compile_info infos[MESA_VK_MAX_GRAPHICS_PIPELINE_STAGES];
+      for (uint32_t i = partition[p]; i < partition[p + 1]; i++) {
+         struct vk_pipeline_stage *stage = &stages[i];
+
+         VkShaderCreateFlagsEXT shader_flags =
+            vk_pipeline_to_shader_flags(pipeline->base.flags, stage->stage);
+
+         if (partition[p + 1] - partition[p] > 1)
+            shader_flags |= VK_SHADER_CREATE_LINK_STAGE_BIT_EXT;
+
+         if ((part_stages & VK_SHADER_STAGE_MESH_BIT_EXT) &&
+             !(geom_stages & VK_SHADER_STAGE_TASK_BIT_EXT))
+            shader_flags = VK_SHADER_CREATE_NO_TASK_SHADER_BIT_EXT;
+
+         VkShaderStageFlags next_stage;
+         if (stage->stage == MESA_SHADER_FRAGMENT) {
+            next_stage = 0;
+         } else if (i + 1 < stage_count) {
+            /* We hash geom_stages above so this is safe */
+            next_stage = mesa_to_vk_shader_stage(stages[i + 1].stage);
+         } else {
+            /* We're the last geometry stage */
+            next_stage = VK_SHADER_STAGE_FRAGMENT_BIT;
+         }
+
+         const struct nir_shader_compiler_options *nir_options =
+            ops->get_nir_options(device->physical, stage->stage,
+                                 &stage->precomp->rs);
+
+         nir_shader *nir =
+            vk_pipeline_precomp_shader_get_nir(stage->precomp, nir_options);
+         if (nir == NULL) {
+            for (uint32_t j = partition[p]; j < i; j++)
+               ralloc_free(infos[i].nir);
+
+            return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+         }
+
+         if (stage->stage == MESA_SHADER_TESS_CTRL ||
+             stage->stage == MESA_SHADER_TESS_EVAL)
+            vk_pipeline_replace_nir_tess_info(nir, &tess_info);
+
+         const VkPushConstantRange *push_range = NULL;
+         if (pipeline_layout != NULL) {
+            for (uint32_t r = 0; r < pipeline_layout->push_range_count; r++) {
+               if (pipeline_layout->push_ranges[r].stageFlags &
+                   mesa_to_vk_shader_stage(stage->stage)) {
+                  assert(push_range == NULL);
+                  push_range = &pipeline_layout->push_ranges[r];
+               }
+            }
+         }
+
+         infos[i] = (struct vk_shader_compile_info) {
+            .stage = stage->stage,
+            .flags = shader_flags,
+            .next_stage_mask = next_stage,
+            .nir = nir,
+            .robustness = &stage->precomp->rs,
+            .set_layout_count = pipeline->set_layout_count,
+            .set_layouts = pipeline->set_layouts,
+            .push_constant_range_count = push_range != NULL,
+            .push_constant_ranges = push_range != NULL ? push_range : NULL,
+         };
+      }
+
+      /* vk_shader_ops::compile() consumes the NIR regardless of whether or
+       * not it succeeds and only generates shaders on success. Once this
+       * returns, we own the shaders but not the NIR in infos.
+       */
+      struct vk_shader *shaders[MESA_VK_MAX_GRAPHICS_PIPELINE_STAGES];
+      result = ops->compile(device, partition[p + 1] - partition[p],
+                            &infos[partition[p]],
+                            state,
+                            &device->alloc,
+                            &shaders[partition[p]]);
+      if (result != VK_SUCCESS)
+         return result;
+
+      const int64_t part_end = os_time_get_nano();
+      for (uint32_t i = partition[p]; i < partition[p + 1]; i++) {
+         struct vk_pipeline_stage *stage = &stages[i];
+
+         if (stage->shader == NULL) {
+            shader_key.stage = stage->stage;
+            vk_shader_init_cache_obj(device, shaders[i], &shader_key,
+                                     sizeof(shader_key));
+
+            struct vk_pipeline_cache_object *cache_obj =
+               &shaders[i]->pipeline.cache_obj;
+            if (cache != NULL)
+               cache_obj = vk_pipeline_cache_add_object(cache, cache_obj);
+
+            stage->shader = vk_shader_from_cache_obj(cache_obj);
+         } else {
+            /* This can fail to happen if only some of the shaders were found
+             * in the pipeline cache.  In this case, we just throw away the
+             * shader as vk_pipeline_cache_add_object() would throw it away
+             * for us anyway.
+             */
+            vk_shader_destroy(device, shaders[i], &device->alloc);
+         }
+
+         stage_feedbacks[stage->stage].duration += part_end - part_start;
+      }
+   }
+
+   return VK_SUCCESS;
+}
+
+static VkResult
+vk_graphics_pipeline_get_executable_properties(
+   struct vk_device *device,
+   struct vk_pipeline *pipeline,
+   uint32_t *executable_count,
+   VkPipelineExecutablePropertiesKHR *properties)
+{
+   struct vk_graphics_pipeline *gfx_pipeline =
+      container_of(pipeline, struct vk_graphics_pipeline, base);
+   VkResult result;
+
+   if (properties == NULL) {
+      *executable_count = 0;
+      for (uint32_t i = 0; i < gfx_pipeline->stage_count; i++) {
+         struct vk_shader *shader = gfx_pipeline->stages[i].shader;
+
+         uint32_t shader_exec_count = 0;
+         result = shader->ops->get_executable_properties(device, shader,
+                                                         &shader_exec_count,
+                                                         NULL);
+         assert(result == VK_SUCCESS);
+         *executable_count += shader_exec_count;
+      }
+   } else {
+      uint32_t arr_len = *executable_count;
+      *executable_count = 0;
+      for (uint32_t i = 0; i < gfx_pipeline->stage_count; i++) {
+         struct vk_shader *shader = gfx_pipeline->stages[i].shader;
+
+         uint32_t shader_exec_count = arr_len - *executable_count;
+         result = shader->ops->get_executable_properties(device, shader,
+                                                         &shader_exec_count,
+                                                         &properties[*executable_count]);
+         if (result != VK_SUCCESS)
+            return result;
+
+         *executable_count += shader_exec_count;
+      }
+   }
+
+   return VK_SUCCESS;
+}
+
+static inline struct vk_shader *
+vk_graphics_pipeline_executable_shader(struct vk_device *device,
+                                       struct vk_graphics_pipeline *gfx_pipeline,
+                                       uint32_t *executable_index)
+{
+   for (uint32_t i = 0; i < gfx_pipeline->stage_count; i++) {
+      struct vk_shader *shader = gfx_pipeline->stages[i].shader;
+
+      uint32_t shader_exec_count = 0;
+      shader->ops->get_executable_properties(device, shader,
+                                             &shader_exec_count, NULL);
+
+      if (*executable_index < shader_exec_count)
+         return shader;
+      else
+         *executable_index -= shader_exec_count;
+   }
+
+   return NULL;
+}
+
+static VkResult
+vk_graphics_pipeline_get_executable_statistics(
+   struct vk_device *device,
+   struct vk_pipeline *pipeline,
+   uint32_t executable_index,
+   uint32_t *statistic_count,
+   VkPipelineExecutableStatisticKHR *statistics)
+{
+   struct vk_graphics_pipeline *gfx_pipeline =
+      container_of(pipeline, struct vk_graphics_pipeline, base);
+
+   struct vk_shader *shader =
+      vk_graphics_pipeline_executable_shader(device, gfx_pipeline,
+                                             &executable_index);
+   if (shader == NULL) {
+      *statistic_count = 0;
+      return VK_SUCCESS;
+   }
+
+   return shader->ops->get_executable_statistics(device, shader,
+                                                 executable_index,
+                                                 statistic_count,
+                                                 statistics);
+}
+
+static VkResult
+vk_graphics_pipeline_get_internal_representations(
+   struct vk_device *device,
+   struct vk_pipeline *pipeline,
+   uint32_t executable_index,
+   uint32_t *internal_representation_count,
+   VkPipelineExecutableInternalRepresentationKHR* internal_representations)
+{
+   struct vk_graphics_pipeline *gfx_pipeline =
+      container_of(pipeline, struct vk_graphics_pipeline, base);
+
+   struct vk_shader *shader =
+      vk_graphics_pipeline_executable_shader(device, gfx_pipeline,
+                                             &executable_index);
+   if (shader == NULL) {
+      *internal_representation_count = 0;
+      return VK_SUCCESS;
+   }
+
+   return shader->ops->get_executable_internal_representations(
+      device, shader, executable_index,
+      internal_representation_count, internal_representations);
+}
+
+static const struct vk_pipeline_ops vk_graphics_pipeline_ops = {
+   .destroy = vk_graphics_pipeline_destroy,
+   .get_executable_statistics = vk_graphics_pipeline_get_executable_statistics,
+   .get_executable_properties = vk_graphics_pipeline_get_executable_properties,
+   .get_internal_representations = vk_graphics_pipeline_get_internal_representations,
+   .cmd_bind = vk_graphics_pipeline_cmd_bind,
+};
+
+static VkResult
+vk_create_graphics_pipeline(struct vk_device *device,
+                            struct vk_pipeline_cache *cache,
+                            const VkGraphicsPipelineCreateInfo *pCreateInfo,
+                            const VkAllocationCallbacks *pAllocator,
+                            VkPipeline *pPipeline)
+{
+   VK_FROM_HANDLE(vk_pipeline_layout, pipeline_layout, pCreateInfo->layout);
+   const int64_t pipeline_start = os_time_get_nano();
+   VkResult result;
+
+   const VkPipelineCreateFlags2KHR pipeline_flags =
+      vk_graphics_pipeline_create_flags(pCreateInfo);
+
+   const VkPipelineCreationFeedbackCreateInfo *feedback_info =
+      vk_find_struct_const(pCreateInfo->pNext,
+                           PIPELINE_CREATION_FEEDBACK_CREATE_INFO);
+
+   const VkPipelineLibraryCreateInfoKHR *libs_info =
+      vk_find_struct_const(pCreateInfo->pNext,
+                           PIPELINE_LIBRARY_CREATE_INFO_KHR);
+
+   struct vk_graphics_pipeline *pipeline =
+      vk_pipeline_zalloc(device, &vk_graphics_pipeline_ops,
+                         VK_PIPELINE_BIND_POINT_GRAPHICS,
+                         pipeline_flags, pAllocator, sizeof(*pipeline));
+   if (pipeline == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   struct vk_pipeline_stage stages[PIPE_SHADER_MESH_TYPES];
+   memset(stages, 0, sizeof(stages));
+
+   VkPipelineCreationFeedback stage_feedbacks[PIPE_SHADER_MESH_TYPES];
+   memset(stage_feedbacks, 0, sizeof(stage_feedbacks));
+
+   struct vk_graphics_pipeline_state state_tmp, *state;
+   struct vk_graphics_pipeline_all_state all_state_tmp, *all_state;
+   if (pipeline->base.flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR) {
+      /* For pipeline libraries, the state is stored in the pipeline */
+      state = &pipeline->lib.state;
+      all_state = &pipeline->lib.all_state;
+   } else {
+      /* For linked pipelines, we throw the state away at the end of pipeline
+       * creation and only keep the dynamic state.
+       */
+      memset(&state_tmp, 0, sizeof(state_tmp));
+      state = &state_tmp;
+      all_state = &all_state_tmp;
+   }
+
+   /* If we have libraries, import them first. */
+   if (libs_info) {
+      for (uint32_t i = 0; i < libs_info->libraryCount; i++) {
+         VK_FROM_HANDLE(vk_pipeline, lib_pipeline, libs_info->pLibraries[i]);
+         assert(lib_pipeline->bind_point == VK_PIPELINE_BIND_POINT_GRAPHICS);
+         assert(lib_pipeline->flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR);
+         struct vk_graphics_pipeline *lib_gfx_pipeline =
+            container_of(lib_pipeline, struct vk_graphics_pipeline, base);
+
+         vk_graphics_pipeline_state_merge(state, &lib_gfx_pipeline->lib.state);
+
+         pipeline->set_layout_count = MAX2(pipeline->set_layout_count,
+                                           lib_gfx_pipeline->set_layout_count);
+         for (uint32_t i = 0; i < lib_gfx_pipeline->set_layout_count; i++) {
+            if (lib_gfx_pipeline->set_layouts[i] == NULL)
+               continue;
+
+            if (pipeline->set_layouts[i] == NULL) {
+               pipeline->set_layouts[i] =
+                  vk_descriptor_set_layout_ref(lib_gfx_pipeline->set_layouts[i]);
+            }
+         }
+
+         for (uint32_t i = 0; i < lib_gfx_pipeline->stage_count; i++) {
+            const struct vk_pipeline_stage *lib_stage =
+               &lib_gfx_pipeline->stages[i];
+
+            /* We shouldn't have duplicated stages in the imported pipeline
+             * but it's cheap enough to protect against it so we may as well.
+             */
+            assert(lib_stage->stage < ARRAY_SIZE(stages));
+            assert(vk_pipeline_stage_is_null(&stages[lib_stage->stage]));
+            if (!vk_pipeline_stage_is_null(&stages[lib_stage->stage]))
+               continue;
+
+            stages[lib_stage->stage] = vk_pipeline_stage_clone(lib_stage);
+         }
+      }
+   }
+
+   result = vk_graphics_pipeline_state_fill(device, state,
+                                            pCreateInfo,
+                                            NULL /* driver_rp */,
+                                            0 /* driver_rp_flags */,
+                                            all_state,
+                                            NULL, 0, NULL);
+   if (result != VK_SUCCESS)
+      goto fail_stages;
+
+   if (!(pipeline->base.flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR)) {
+      pipeline->linked.dynamic.vi = &pipeline->linked._dynamic_vi;
+      pipeline->linked.dynamic.ms.sample_locations =
+         &pipeline->linked._dynamic_sl;
+      vk_dynamic_graphics_state_fill(&pipeline->linked.dynamic, &state_tmp);
+   }
+
+   if (pipeline_layout != NULL) {
+      pipeline->set_layout_count = MAX2(pipeline->set_layout_count,
+                                        pipeline_layout->set_count);
+      for (uint32_t i = 0; i < pipeline_layout->set_count; i++) {
+         if (pipeline_layout->set_layouts[i] == NULL)
+            continue;
+
+         if (pipeline->set_layouts[i] == NULL) {
+            pipeline->set_layouts[i] =
+               vk_descriptor_set_layout_ref(pipeline_layout->set_layouts[i]);
+         }
+      }
+   }
+
+   for (uint32_t i = 0; i < pCreateInfo->stageCount; i++) {
+      const VkPipelineShaderStageCreateInfo *stage_info =
+         &pCreateInfo->pStages[i];
+
+      const int64_t stage_start = os_time_get_nano();
+
+      assert(util_bitcount(stage_info->stage) == 1);
+      if (!(state->shader_stages & stage_info->stage))
+         continue;
+
+      gl_shader_stage stage = vk_to_mesa_shader_stage(stage_info->stage);
+      assert(vk_device_supports_stage(device, stage));
+
+      stage_feedbacks[stage].flags |=
+         VK_PIPELINE_CREATION_FEEDBACK_VALID_BIT;
+
+      if (!vk_pipeline_stage_is_null(&stages[stage]))
+         continue;
+
+      struct vk_pipeline_precomp_shader *precomp;
+      result = vk_pipeline_precompile_shader(device, cache, pipeline_flags,
+                                             pCreateInfo->pNext,
+                                             stage_info,
+                                             &precomp);
+      if (result != VK_SUCCESS)
+         goto fail_stages;
+
+      stages[stage] = (struct vk_pipeline_stage) {
+         .stage = stage,
+         .precomp = precomp,
+      };
+
+      const int64_t stage_end = os_time_get_nano();
+      stage_feedbacks[stage].duration += stage_end - stage_start;
+   }
+
+   /* Compact the array of stages */
+   uint32_t stage_count = 0;
+   for (uint32_t s = 0; s < ARRAY_SIZE(stages); s++) {
+      assert(s >= stage_count);
+      if (!vk_pipeline_stage_is_null(&stages[s]))
+         stages[stage_count++] = stages[s];
+   }
+   for (uint32_t s = stage_count; s < ARRAY_SIZE(stages); s++)
+      memset(&stages[s], 0, sizeof(stages[s]));
+
+   /* Sort so we always give the driver shaders in order.
+    *
+    * This makes everything easier for everyone.  This also helps stabilize
+    * shader keys so that we get a cache hit even if the client gives us
+    * the stages in a different order.
+    */
+   qsort(stages, stage_count, sizeof(*stages), cmp_vk_pipeline_stages);
+
+   result = vk_graphics_pipeline_compile_shaders(device, cache, pipeline,
+                                                 pipeline_layout, state,
+                                                 stage_count, stages,
+                                                 stage_feedbacks);
+   if (result != VK_SUCCESS)
+      goto fail_stages;
+
+   /* Throw away precompiled shaders unless the client explicitly asks us to
+    * keep them.
+    */
+   if (!(pipeline_flags &
+         VK_PIPELINE_CREATE_2_RETAIN_LINK_TIME_OPTIMIZATION_INFO_BIT_EXT)) {
+      for (uint32_t i = 0; i < stage_count; i++) {
+         if (stages[i].precomp != NULL) {
+            vk_pipeline_precomp_shader_unref(device, stages[i].precomp);
+            stages[i].precomp = NULL;
+         }
+      }
+   }
+
+   pipeline->stage_count = stage_count;
+   for (uint32_t i = 0; i < stage_count; i++)
+      pipeline->stages[i] = stages[i];
+
+   const int64_t pipeline_end = os_time_get_nano();
+   if (feedback_info != NULL) {
+      VkPipelineCreationFeedback pipeline_feedback = {
+         .flags = VK_PIPELINE_CREATION_FEEDBACK_VALID_BIT,
+         .duration = pipeline_end - pipeline_start,
+      };
+
+      /* From the Vulkan 1.3.275 spec:
+       *
+       *    "An implementation should set the
+       *    VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT
+       *    bit if it was able to avoid the large majority of pipeline or
+       *    pipeline stage creation work by using the pipelineCache parameter"
+       *
+       * We really shouldn't set this bit unless all the shaders hit the
+       * cache.
+       */
+      uint32_t cache_hit_count = 0;
+      for (uint32_t i = 0; i < stage_count; i++) {
+         const gl_shader_stage stage = stages[i].stage;
+         if (stage_feedbacks[stage].flags &
+             VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT)
+            cache_hit_count++;
+      }
+      if (cache_hit_count > 0 && cache_hit_count == stage_count) {
+         pipeline_feedback.flags |=
+            VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT;
+      }
+
+      *feedback_info->pPipelineCreationFeedback = pipeline_feedback;
+
+      /* VUID-VkGraphicsPipelineCreateInfo-pipelineStageCreationFeedbackCount-06594 */
+      assert(feedback_info->pipelineStageCreationFeedbackCount == 0 ||
+             feedback_info->pipelineStageCreationFeedbackCount ==
+             pCreateInfo->stageCount);
+      for (uint32_t i = 0;
+           i < feedback_info->pipelineStageCreationFeedbackCount; i++) {
+         const gl_shader_stage stage =
+            vk_to_mesa_shader_stage(pCreateInfo->pStages[i].stage);
+
+         feedback_info->pPipelineStageCreationFeedbacks[i] =
+            stage_feedbacks[stage];
+      }
+   }
+
+   *pPipeline = vk_pipeline_to_handle(&pipeline->base);
+
+   return VK_SUCCESS;
+
+fail_stages:
+   for (uint32_t i = 0; i < ARRAY_SIZE(stages); i++)
+      vk_pipeline_stage_finish(device, &stages[i]);
+
+   vk_graphics_pipeline_destroy(device, &pipeline->base, pAllocator);
+
+   return result;
+}
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_CreateGraphicsPipelines(VkDevice _device,
+                                  VkPipelineCache pipelineCache,
+                                  uint32_t createInfoCount,
+                                  const VkGraphicsPipelineCreateInfo *pCreateInfos,
+                                  const VkAllocationCallbacks *pAllocator,
+                                  VkPipeline *pPipelines)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_pipeline_cache, cache, pipelineCache);
+   VkResult first_error_or_success = VK_SUCCESS;
+
+   /* From the Vulkan 1.3.274 spec:
+    *
+    *    "When attempting to create many pipelines in a single command, it is
+    *    possible that creation may fail for a subset of them. In this case,
+    *    the corresponding elements of pPipelines will be set to
+    *    VK_NULL_HANDLE.
+    */
+   memset(pPipelines, 0, createInfoCount * sizeof(*pPipelines));
+
+   unsigned i = 0;
+   for (; i < createInfoCount; i++) {
+      VkResult result = vk_create_graphics_pipeline(device, cache,
+                                                    &pCreateInfos[i],
+                                                    pAllocator,
+                                                    &pPipelines[i]);
+      if (result == VK_SUCCESS)
+         continue;
+
+      if (first_error_or_success == VK_SUCCESS)
+         first_error_or_success = result;
+
+      /* Bail out on the first error != VK_PIPELINE_COMPILE_REQUIRED as it
+       * is not obvious what error should be report upon 2 different failures.
+       */
+      if (result != VK_PIPELINE_COMPILE_REQUIRED)
+         return result;
+
+      const VkPipelineCreateFlags2KHR flags =
+         vk_graphics_pipeline_create_flags(&pCreateInfos[i]);
+      if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
+         return result;
+   }
+
+   return first_error_or_success;
+}
+
+struct vk_compute_pipeline {
+   struct vk_pipeline base;
+   struct vk_shader *shader;
+};
+
+static void
+vk_compute_pipeline_destroy(struct vk_device *device,
+                            struct vk_pipeline *pipeline,
+                            const VkAllocationCallbacks *pAllocator)
+{
+   struct vk_compute_pipeline *comp_pipeline =
+      container_of(pipeline, struct vk_compute_pipeline, base);
+
+   vk_shader_unref(device, comp_pipeline->shader);
+   vk_pipeline_free(device, pAllocator, pipeline);
+}
+
+static void
+vk_compute_pipeline_cmd_bind(struct vk_command_buffer *cmd_buffer,
+                             struct vk_pipeline *pipeline)
+{
+   struct vk_device *device = cmd_buffer->base.device;
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+
+   struct vk_shader *shader = NULL;
+   if (pipeline != NULL) {
+      assert(pipeline->bind_point == VK_PIPELINE_BIND_POINT_COMPUTE);
+      struct vk_compute_pipeline *comp_pipeline =
+         container_of(pipeline, struct vk_compute_pipeline, base);
+
+      shader = comp_pipeline->shader;
+
+      cmd_buffer->pipeline_shader_stages |= VK_SHADER_STAGE_COMPUTE_BIT;
+   } else {
+      cmd_buffer->pipeline_shader_stages &= ~VK_SHADER_STAGE_COMPUTE_BIT;
+   }
+
+   gl_shader_stage stage = MESA_SHADER_COMPUTE;
+   ops->cmd_bind_shaders(cmd_buffer, 1, &stage, &shader);
+}
+
+static VkResult
+vk_pipeline_compile_compute_stage(struct vk_device *device,
+                                  struct vk_pipeline_cache *cache,
+                                  struct vk_compute_pipeline *pipeline,
+                                  struct vk_pipeline_layout *pipeline_layout,
+                                  struct vk_pipeline_stage *stage,
+                                  bool *cache_hit)
+{
+   const struct vk_device_shader_ops *ops = device->shader_ops;
+   VkResult result;
+
+   const VkPushConstantRange *push_range = NULL;
+   if (pipeline_layout != NULL) {
+      for (uint32_t r = 0; r < pipeline_layout->push_range_count; r++) {
+         if (pipeline_layout->push_ranges[r].stageFlags &
+             VK_SHADER_STAGE_COMPUTE_BIT) {
+            assert(push_range == NULL);
+            push_range = &pipeline_layout->push_ranges[r];
+         }
+      }
+   }
+
+   VkShaderCreateFlagsEXT shader_flags =
+      vk_pipeline_to_shader_flags(pipeline->base.flags,
+                                  VK_SHADER_STAGE_COMPUTE_BIT);
+
+   struct mesa_blake3 blake3_ctx;
+   _mesa_blake3_init(&blake3_ctx);
+
+   _mesa_blake3_update(&blake3_ctx, stage->precomp->blake3,
+                     sizeof(stage->precomp->blake3));
+
+   _mesa_blake3_update(&blake3_ctx, &shader_flags, sizeof(shader_flags));
+
+   for (uint32_t i = 0; i < pipeline_layout->set_count; i++) {
+      if (pipeline_layout->set_layouts[i] != NULL) {
+         _mesa_blake3_update(&blake3_ctx,
+                             pipeline_layout->set_layouts[i]->blake3,
+                             sizeof(pipeline_layout->set_layouts[i]->blake3));
+      }
+   }
+   if (push_range != NULL)
+      _mesa_blake3_update(&blake3_ctx, push_range, sizeof(*push_range));
+
+   struct vk_shader_pipeline_cache_key shader_key = {
+      .stage = MESA_SHADER_COMPUTE,
+   };
+   _mesa_blake3_final(&blake3_ctx, shader_key.blake3);
+
+   if (cache != NULL) {
+      struct vk_pipeline_cache_object *cache_obj =
+         vk_pipeline_cache_lookup_object(cache, &shader_key,
+                                         sizeof(shader_key),
+                                         &pipeline_shader_cache_ops,
+                                         cache_hit);
+      if (cache_obj != NULL) {
+         stage->shader = vk_shader_from_cache_obj(cache_obj);
+         return VK_SUCCESS;
+      }
+   }
+
+   if (pipeline->base.flags &
+       VK_PIPELINE_CREATE_2_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT_KHR)
+      return VK_PIPELINE_COMPILE_REQUIRED;
+
+   const struct nir_shader_compiler_options *nir_options =
+      ops->get_nir_options(device->physical, stage->stage,
+                           &stage->precomp->rs);
+
+   nir_shader *nir = vk_pipeline_precomp_shader_get_nir(stage->precomp,
+                                                        nir_options);
+   if (nir == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   /* vk_device_shader_ops::compile() consumes the NIR regardless of whether
+    * or not it succeeds and only generates shaders on success. Once compile()
+    * returns, we own the shaders but not the NIR in infos.
+    */
+   struct vk_shader_compile_info compile_info = {
+      .stage = stage->stage,
+      .flags = shader_flags,
+      .next_stage_mask = 0,
+      .nir = nir,
+      .robustness = &stage->precomp->rs,
+      .set_layout_count = pipeline_layout->set_count,
+      .set_layouts = pipeline_layout->set_layouts,
+      .push_constant_range_count = push_range != NULL,
+      .push_constant_ranges = push_range != NULL ? push_range : NULL,
+   };
+
+   struct vk_shader *shader;
+   result = ops->compile(device, 1, &compile_info, NULL,
+                         &device->alloc, &shader);
+   if (result != VK_SUCCESS)
+      return result;
+
+   vk_shader_init_cache_obj(device, shader, &shader_key, sizeof(shader_key));
+
+   struct vk_pipeline_cache_object *cache_obj = &shader->pipeline.cache_obj;
+   if (cache != NULL)
+      cache_obj = vk_pipeline_cache_add_object(cache, cache_obj);
+
+   stage->shader = vk_shader_from_cache_obj(cache_obj);
+
+   return VK_SUCCESS;
+}
+
+static VkResult
+vk_compute_pipeline_get_executable_properties(
+   struct vk_device *device,
+   struct vk_pipeline *pipeline,
+   uint32_t *executable_count,
+   VkPipelineExecutablePropertiesKHR *properties)
+{
+   struct vk_compute_pipeline *comp_pipeline =
+      container_of(pipeline, struct vk_compute_pipeline, base);
+   struct vk_shader *shader = comp_pipeline->shader;
+
+   return shader->ops->get_executable_properties(device, shader,
+                                                 executable_count,
+                                                 properties);
+}
+
+static VkResult
+vk_compute_pipeline_get_executable_statistics(
+   struct vk_device *device,
+   struct vk_pipeline *pipeline,
+   uint32_t executable_index,
+   uint32_t *statistic_count,
+   VkPipelineExecutableStatisticKHR *statistics)
+{
+   struct vk_compute_pipeline *comp_pipeline =
+      container_of(pipeline, struct vk_compute_pipeline, base);
+   struct vk_shader *shader = comp_pipeline->shader;
+
+   return shader->ops->get_executable_statistics(device, shader,
+                                                 executable_index,
+                                                 statistic_count,
+                                                 statistics);
+}
+
+static VkResult
+vk_compute_pipeline_get_internal_representations(
+   struct vk_device *device,
+   struct vk_pipeline *pipeline,
+   uint32_t executable_index,
+   uint32_t *internal_representation_count,
+   VkPipelineExecutableInternalRepresentationKHR* internal_representations)
+{
+   struct vk_compute_pipeline *comp_pipeline =
+      container_of(pipeline, struct vk_compute_pipeline, base);
+   struct vk_shader *shader = comp_pipeline->shader;
+
+   return shader->ops->get_executable_internal_representations(
+      device, shader, executable_index,
+      internal_representation_count, internal_representations);
+}
+
+static const struct vk_pipeline_ops vk_compute_pipeline_ops = {
+   .destroy = vk_compute_pipeline_destroy,
+   .get_executable_statistics = vk_compute_pipeline_get_executable_statistics,
+   .get_executable_properties = vk_compute_pipeline_get_executable_properties,
+   .get_internal_representations = vk_compute_pipeline_get_internal_representations,
+   .cmd_bind = vk_compute_pipeline_cmd_bind,
+};
+
+static VkResult
+vk_create_compute_pipeline(struct vk_device *device,
+                           struct vk_pipeline_cache *cache,
+                           const VkComputePipelineCreateInfo *pCreateInfo,
+                           const VkAllocationCallbacks *pAllocator,
+                           VkPipeline *pPipeline)
+{
+   VK_FROM_HANDLE(vk_pipeline_layout, pipeline_layout, pCreateInfo->layout);
+   int64_t pipeline_start = os_time_get_nano();
+   VkResult result;
+
+   const VkPipelineCreateFlags2KHR pipeline_flags =
+      vk_compute_pipeline_create_flags(pCreateInfo);
+
+   const VkPipelineCreationFeedbackCreateInfo *feedback_info =
+      vk_find_struct_const(pCreateInfo->pNext,
+                           PIPELINE_CREATION_FEEDBACK_CREATE_INFO);
+
+   struct vk_compute_pipeline *pipeline =
+      vk_pipeline_zalloc(device, &vk_compute_pipeline_ops,
+                         VK_PIPELINE_BIND_POINT_COMPUTE,
+                         pipeline_flags, pAllocator, sizeof(*pipeline));
+   if (pipeline == NULL)
+      return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   struct vk_pipeline_stage stage = {
+      .stage = MESA_SHADER_COMPUTE,
+   };
+   result = vk_pipeline_precompile_shader(device, cache, pipeline_flags,
+                                          pCreateInfo->pNext,
+                                          &pCreateInfo->stage,
+                                          &stage.precomp);
+   if (result != VK_SUCCESS)
+      goto fail_pipeline;
+
+   bool cache_hit;
+   result = vk_pipeline_compile_compute_stage(device, cache, pipeline,
+                                              pipeline_layout, &stage,
+                                              &cache_hit);
+   if (result != VK_SUCCESS)
+      goto fail_stage;
+
+   if (stage.precomp != NULL)
+      vk_pipeline_precomp_shader_unref(device, stage.precomp);
+   pipeline->shader = stage.shader;
+
+   const int64_t pipeline_end = os_time_get_nano();
+   if (feedback_info != NULL) {
+      VkPipelineCreationFeedback pipeline_feedback = {
+         .flags = VK_PIPELINE_CREATION_FEEDBACK_VALID_BIT,
+         .duration = pipeline_end - pipeline_start,
+      };
+      if (cache_hit) {
+         pipeline_feedback.flags |=
+            VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT;
+      }
+
+      *feedback_info->pPipelineCreationFeedback = pipeline_feedback;
+      if (feedback_info->pipelineStageCreationFeedbackCount > 0) {
+         feedback_info->pPipelineStageCreationFeedbacks[0] =
+            pipeline_feedback;
+      }
+   }
+
+   *pPipeline = vk_pipeline_to_handle(&pipeline->base);
+
+   return VK_SUCCESS;
+
+fail_stage:
+   vk_pipeline_stage_finish(device, &stage);
+fail_pipeline:
+   vk_pipeline_free(device, pAllocator, &pipeline->base);
+
+   return result;
+}
+
+VKAPI_ATTR VkResult VKAPI_CALL
+vk_common_CreateComputePipelines(VkDevice _device,
+                                 VkPipelineCache pipelineCache,
+                                 uint32_t createInfoCount,
+                                 const VkComputePipelineCreateInfo *pCreateInfos,
+                                 const VkAllocationCallbacks *pAllocator,
+                                 VkPipeline *pPipelines)
+{
+   VK_FROM_HANDLE(vk_device, device, _device);
+   VK_FROM_HANDLE(vk_pipeline_cache, cache, pipelineCache);
+   VkResult first_error_or_success = VK_SUCCESS;
+
+   /* From the Vulkan 1.3.274 spec:
+    *
+    *    "When attempting to create many pipelines in a single command, it is
+    *    possible that creation may fail for a subset of them. In this case,
+    *    the corresponding elements of pPipelines will be set to
+    *    VK_NULL_HANDLE.
+    */
+   memset(pPipelines, 0, createInfoCount * sizeof(*pPipelines));
+
+   unsigned i = 0;
+   for (; i < createInfoCount; i++) {
+      VkResult result = vk_create_compute_pipeline(device, cache,
+                                                   &pCreateInfos[i],
+                                                   pAllocator,
+                                                   &pPipelines[i]);
+      if (result == VK_SUCCESS)
+         continue;
+
+      if (first_error_or_success == VK_SUCCESS)
+         first_error_or_success = result;
+
+      /* Bail out on the first error != VK_PIPELINE_COMPILE_REQUIRED as it
+       * is not obvious what error should be report upon 2 different failures.
+       */
+      if (result != VK_PIPELINE_COMPILE_REQUIRED)
+         return result;
+
+      const VkPipelineCreateFlags2KHR flags =
+         vk_compute_pipeline_create_flags(&pCreateInfos[i]);
+      if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
+         return result;
+   }
+
+   return first_error_or_success;
+}
+
+void
+vk_cmd_unbind_pipelines_for_stages(struct vk_command_buffer *cmd_buffer,
+                                   VkShaderStageFlags stages)
+{
+   stages &= cmd_buffer->pipeline_shader_stages;
+
+   if (stages & ~VK_SHADER_STAGE_COMPUTE_BIT)
+      vk_graphics_pipeline_cmd_bind(cmd_buffer, NULL);
+
+   if (stages & VK_SHADER_STAGE_COMPUTE_BIT)
+      vk_compute_pipeline_cmd_bind(cmd_buffer, NULL);
+}
diff --git a/src/vulkan/runtime/vk_pipeline.h b/src/vulkan/runtime/vk_pipeline.h
index 62ae730e1e440..ed05d567a8fc0 100644
--- a/src/vulkan/runtime/vk_pipeline.h
+++ b/src/vulkan/runtime/vk_pipeline.h
@@ -199,6 +199,10 @@ void vk_pipeline_free(struct vk_device *device,
                       const VkAllocationCallbacks *alloc,
                       struct vk_pipeline *pipeline);
 
+void
+vk_cmd_unbind_pipelines_for_stages(struct vk_command_buffer *cmd_buffer,
+                                   VkShaderStageFlags stages);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/vulkan/runtime/vk_shader.c b/src/vulkan/runtime/vk_shader.c
index 54187b425a0a5..2a52ec372b14e 100644
--- a/src/vulkan/runtime/vk_shader.c
+++ b/src/vulkan/runtime/vk_shader.c
@@ -369,7 +369,10 @@ vk_common_GetShaderBinaryDataEXT(VkDevice _device,
    return result;
 }
 
-#define VK_MAX_LINKED_SHADER_STAGES 5
+/* The only place where we have "real" linking is graphics shaders and there
+ * is a limit as to how many of them can be linked together at one time.
+ */
+#define VK_MAX_LINKED_SHADER_STAGES MESA_VK_MAX_GRAPHICS_PIPELINE_STAGES
 
 VKAPI_ATTR VkResult VKAPI_CALL
 vk_common_CreateShadersEXT(VkDevice _device,
@@ -552,10 +555,16 @@ vk_common_CmdBindShadersEXT(VkCommandBuffer commandBuffer,
    STACK_ARRAY(gl_shader_stage, stages, stageCount);
    STACK_ARRAY(struct vk_shader *, shaders, stageCount);
 
+   VkShaderStageFlags vk_stages = 0;
    for (uint32_t i = 0; i < stageCount; i++) {
+      vk_stages |= pStages[i];
       stages[i] = vk_to_mesa_shader_stage(pStages[i]);
       shaders[i] = pShaders != NULL ? vk_shader_from_handle(pShaders[i]) : NULL;
    }
 
+   vk_cmd_unbind_pipelines_for_stages(cmd_buffer, vk_stages);
+   if (vk_stages & ~VK_SHADER_STAGE_COMPUTE_BIT)
+      vk_cmd_set_rp_attachments(cmd_buffer, ~0);
+
    ops->cmd_bind_shaders(cmd_buffer, stageCount, stages, shaders);
 }
diff --git a/src/vulkan/runtime/vk_shader.h b/src/vulkan/runtime/vk_shader.h
index 0ee6e7681c343..67f69fff301ed 100644
--- a/src/vulkan/runtime/vk_shader.h
+++ b/src/vulkan/runtime/vk_shader.h
@@ -28,6 +28,8 @@
 #include "vk_limits.h"
 #include "vk_pipeline_cache.h"
 
+#include "util/mesa-blake3.h"
+
 #ifdef __cplusplus
 extern "C" {
 #endif
@@ -45,6 +47,8 @@ struct vk_pipeline_robustness_state;
 
 int vk_shader_cmp_graphics_stages(gl_shader_stage a, gl_shader_stage b);
 
+#define VK_SHADER_CREATE_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_MESA 0x1000
+
 struct vk_shader_compile_info {
    gl_shader_stage stage;
    VkShaderCreateFlagsEXT flags;
@@ -62,12 +66,26 @@ struct vk_shader_compile_info {
 
 struct vk_shader_ops;
 
+#pragma GCC diagnostic push
+#pragma GCC diagnostic error "-Wpadded"
+struct vk_shader_pipeline_cache_key {
+   gl_shader_stage stage;
+   blake3_hash blake3;
+};
+#pragma GCC diagnostic pop
+
 struct vk_shader {
    struct vk_object_base base;
 
    const struct vk_shader_ops *ops;
 
    gl_shader_stage stage;
+
+   /* Used for the generic VkPipeline implementation */
+   struct {
+      struct vk_pipeline_cache_object cache_obj;
+      struct vk_shader_pipeline_cache_key cache_key;
+   } pipeline;
 };
 
 VK_DEFINE_NONDISP_HANDLE_CASTS(vk_shader, base, VkShaderEXT,
@@ -90,6 +108,39 @@ struct vk_shader_ops {
    bool (*serialize)(struct vk_device *device,
                      const struct vk_shader *shader,
                      struct blob *blob);
+
+   /** Returns executable properties for this shader
+    *
+    * This is equivalent to vkGetPipelineExecutableProperties(), only for a
+    * single vk_shader.
+    */
+   VkResult (*get_executable_properties)(struct vk_device *device,
+                                         const struct vk_shader *shader,
+                                         uint32_t *executable_count,
+                                         VkPipelineExecutablePropertiesKHR *properties);
+
+   /** Returns executable statistics for this shader
+    *
+    * This is equivalent to vkGetPipelineExecutableStatistics(), only for a
+    * single vk_shader.
+    */
+   VkResult (*get_executable_statistics)(struct vk_device *device,
+                                         const struct vk_shader *shader,
+                                         uint32_t executable_index,
+                                         uint32_t *statistic_count,
+                                         VkPipelineExecutableStatisticKHR *statistics);
+
+   /** Returns executable internal representations for this shader
+    *
+    * This is equivalent to vkGetPipelineExecutableInternalRepresentations(),
+    * only for a single vk_shader.
+    */
+   VkResult (*get_executable_internal_representations)(
+      struct vk_device *device,
+      const struct vk_shader *shader,
+      uint32_t executable_index,
+      uint32_t *internal_representation_count,
+      VkPipelineExecutableInternalRepresentationKHR *internal_representations);
 };
 
 void *vk_shader_zalloc(struct vk_device *device,
@@ -143,6 +194,23 @@ struct vk_device_shader_ops {
     */
    void (*preprocess_nir)(struct vk_physical_device *device, nir_shader *nir);
 
+   /** True if the driver wants geometry stages linked
+    *
+    * If set to true, geometry stages will always be compiled with
+    * VK_SHADER_CREATE_LINK_STAGE_BIT_EXT when pipelines are used.
+    */
+   bool link_geom_stages;
+
+   /** Hash a vk_graphics_state object
+    *
+    * This callback hashes whatever bits of vk_graphics_pipeline_state might
+    * be used to compile a shader in one of the given stages.
+    */
+   void (*hash_graphics_state)(struct vk_physical_device *device,
+                               const struct vk_graphics_pipeline_state *state,
+                               VkShaderStageFlags stages,
+                               blake3_hash blake3_out);
+
    /** Compile (and potentially link) a set of shaders
     *
     * Unlike vkCreateShadersEXT, this callback will only ever be called with
@@ -175,6 +243,10 @@ struct vk_device_shader_ops {
                             uint32_t stage_count,
                             const gl_shader_stage *stages,
                             struct vk_shader ** const shaders);
+
+   /** Sets dynamic state */
+   void (*cmd_set_dynamic_graphics_state)(struct vk_command_buffer *cmd_buffer,
+                                          const struct vk_dynamic_graphics_state *state);
 };
 
 #ifdef __cplusplus
-- 
GitLab


From 4900cb5b8d9c116426a4c51ca5bc675af5f0d374 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 4 Jan 2024 15:51:54 -0600
Subject: [PATCH 13/22] nvk: Populate vk_descriptor_set_layout::blake3

---
 .../vulkan/nvk_descriptor_set_layout.c        | 30 +++++++++----------
 .../vulkan/nvk_descriptor_set_layout.h        |  2 --
 src/nouveau/vulkan/nvk_shader.c               |  2 +-
 3 files changed, 15 insertions(+), 19 deletions(-)

diff --git a/src/nouveau/vulkan/nvk_descriptor_set_layout.c b/src/nouveau/vulkan/nvk_descriptor_set_layout.c
index 35dec3c4b55a4..12b1be6152ad5 100644
--- a/src/nouveau/vulkan/nvk_descriptor_set_layout.c
+++ b/src/nouveau/vulkan/nvk_descriptor_set_layout.c
@@ -12,8 +12,6 @@
 
 #include "vk_pipeline_layout.h"
 
-#include "util/mesa-sha1.h"
-
 static bool
 binding_has_immutable_samplers(const VkDescriptorSetLayoutBinding *binding)
 {
@@ -257,26 +255,26 @@ nvk_CreateDescriptorSetLayout(VkDevice device,
    layout->non_variable_descriptor_buffer_size = buffer_size;
    layout->dynamic_buffer_count = dynamic_buffer_count;
 
-   struct mesa_sha1 sha1_ctx;
-   _mesa_sha1_init(&sha1_ctx);
+   struct mesa_blake3 blake3_ctx;
+   _mesa_blake3_init(&blake3_ctx);
 
-#define SHA1_UPDATE_VALUE(x) _mesa_sha1_update(&sha1_ctx, &(x), sizeof(x));
-   SHA1_UPDATE_VALUE(layout->non_variable_descriptor_buffer_size);
-   SHA1_UPDATE_VALUE(layout->dynamic_buffer_count);
-   SHA1_UPDATE_VALUE(layout->binding_count);
+#define BLAKE3_UPDATE_VALUE(x) _mesa_blake3_update(&blake3_ctx, &(x), sizeof(x));
+   BLAKE3_UPDATE_VALUE(layout->non_variable_descriptor_buffer_size);
+   BLAKE3_UPDATE_VALUE(layout->dynamic_buffer_count);
+   BLAKE3_UPDATE_VALUE(layout->binding_count);
 
    for (uint32_t b = 0; b < num_bindings; b++) {
-      SHA1_UPDATE_VALUE(layout->binding[b].type);
-      SHA1_UPDATE_VALUE(layout->binding[b].flags);
-      SHA1_UPDATE_VALUE(layout->binding[b].array_size);
-      SHA1_UPDATE_VALUE(layout->binding[b].offset);
-      SHA1_UPDATE_VALUE(layout->binding[b].stride);
-      SHA1_UPDATE_VALUE(layout->binding[b].dynamic_buffer_index);
+      BLAKE3_UPDATE_VALUE(layout->binding[b].type);
+      BLAKE3_UPDATE_VALUE(layout->binding[b].flags);
+      BLAKE3_UPDATE_VALUE(layout->binding[b].array_size);
+      BLAKE3_UPDATE_VALUE(layout->binding[b].offset);
+      BLAKE3_UPDATE_VALUE(layout->binding[b].stride);
+      BLAKE3_UPDATE_VALUE(layout->binding[b].dynamic_buffer_index);
       /* Immutable samplers are ignored for now */
    }
-#undef SHA1_UPDATE_VALUE
+#undef BLAKE3_UPDATE_VALUE
 
-   _mesa_sha1_final(&sha1_ctx, layout->sha1);
+   _mesa_blake3_final(&blake3_ctx, layout->vk.blake3);
 
    *pSetLayout = nvk_descriptor_set_layout_to_handle(layout);
 
diff --git a/src/nouveau/vulkan/nvk_descriptor_set_layout.h b/src/nouveau/vulkan/nvk_descriptor_set_layout.h
index d1d7e6b3a8015..56425c4c485de 100644
--- a/src/nouveau/vulkan/nvk_descriptor_set_layout.h
+++ b/src/nouveau/vulkan/nvk_descriptor_set_layout.h
@@ -43,8 +43,6 @@ struct nvk_descriptor_set_binding_layout {
 struct nvk_descriptor_set_layout {
    struct vk_descriptor_set_layout vk;
 
-   unsigned char sha1[20];
-
    /* Size of the descriptor buffer for this descriptor set */
    /* Does not contain the size needed for variable count descriptors */
    uint32_t non_variable_descriptor_buffer_size;
diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index dc137bef5f9d3..db2396b837e65 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -643,7 +643,7 @@ nvk_hash_shader(unsigned char *hash,
       for (int i = 0; i < layout->set_count; i++) {
          struct nvk_descriptor_set_layout *set =
             vk_to_nvk_descriptor_set_layout(layout->set_layouts[i]);
-         _mesa_sha1_update(&ctx, &set->sha1, sizeof(set->sha1));
+         _mesa_sha1_update(&ctx, &set->vk.blake3, sizeof(set->vk.blake3));
       }
    }
 
-- 
GitLab


From c56f8464a6c29605e63e63705695bbe93ee50170 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 4 Jan 2024 16:13:20 -0600
Subject: [PATCH 14/22] nvk/shader: Refactor some helpers

This puts them in the form we need for vk_shader.
---
 src/nouveau/vulkan/nvk_shader.c | 47 ++++++++++++++++++++++-----------
 src/nouveau/vulkan/nvk_shader.h |  8 ------
 2 files changed, 32 insertions(+), 23 deletions(-)

diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index db2396b837e65..86dd4c94b25db 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -98,20 +98,28 @@ nvk_physical_device_compiler_flags(const struct nvk_physical_device *pdev)
       | (nak_flags << 48);
 }
 
-const nir_shader_compiler_options *
-nvk_physical_device_nir_options(const struct nvk_physical_device *pdev,
-                                gl_shader_stage stage)
+static const nir_shader_compiler_options *
+nvk_get_nir_options(struct vk_physical_device *vk_pdev,
+                    gl_shader_stage stage,
+                    UNUSED const struct vk_pipeline_robustness_state *rs)
 {
+   const struct nvk_physical_device *pdev =
+      container_of(vk_pdev, struct nvk_physical_device, vk);
+
    if (use_nak(pdev, stage))
       return nak_nir_options(pdev->nak);
    else
       return nvk_cg_nir_options(pdev, stage);
 }
 
-struct spirv_to_nir_options
-nvk_physical_device_spirv_options(const struct nvk_physical_device *pdev,
-                                  const struct vk_pipeline_robustness_state *rs)
+static struct spirv_to_nir_options
+nvk_get_spirv_options(struct vk_physical_device *vk_pdev,
+                      UNUSED gl_shader_stage stage,
+                      const struct vk_pipeline_robustness_state *rs)
 {
+   const struct nvk_physical_device *pdev =
+      container_of(vk_pdev, struct nvk_physical_device, vk);
+
    return (struct spirv_to_nir_options) {
       .caps = {
          .demote_to_helper_invocation = true,
@@ -162,6 +170,21 @@ nvk_physical_device_spirv_options(const struct nvk_physical_device *pdev,
    };
 }
 
+static void
+nvk_preprocess_nir(struct vk_physical_device *vk_pdev, nir_shader *nir)
+{
+   const struct nvk_physical_device *pdev =
+      container_of(vk_pdev, struct nvk_physical_device, vk);
+
+   NIR_PASS_V(nir, nir_lower_io_to_temporaries,
+              nir_shader_get_entrypoint(nir), true, false);
+
+   if (use_nak(pdev, nir->info.stage))
+      nak_preprocess_nir(nir, pdev->nak);
+   else
+      nvk_cg_preprocess_nir(nir);
+}
+
 static bool
 lower_load_global_constant_offset_instr(nir_builder *b,
                                         nir_intrinsic_instr *intrin,
@@ -248,7 +271,7 @@ nvk_shader_stage_to_nir(struct nvk_device *dev,
    struct nvk_physical_device *pdev = nvk_device_physical(dev);
    const gl_shader_stage stage = vk_to_mesa_shader_stage(sinfo->stage);
    const nir_shader_compiler_options *nir_options =
-      nvk_physical_device_nir_options(pdev, stage);
+      nvk_get_nir_options(&pdev->vk, stage, rstate);
 
    unsigned char stage_sha1[SHA1_DIGEST_LENGTH];
    vk_pipeline_hash_shader_stage(sinfo, rstate, stage_sha1);
@@ -266,7 +289,7 @@ nvk_shader_stage_to_nir(struct nvk_device *dev,
    }
 
    const struct spirv_to_nir_options spirv_options =
-      nvk_physical_device_spirv_options(pdev, rstate);
+      nvk_get_spirv_options(&pdev->vk, stage, rstate);
 
    VkResult result = vk_pipeline_shader_stage_to_nir(&dev->vk, sinfo,
                                                      &spirv_options,
@@ -275,13 +298,7 @@ nvk_shader_stage_to_nir(struct nvk_device *dev,
    if (result != VK_SUCCESS)
       return result;
 
-   NIR_PASS_V(nir, nir_lower_io_to_temporaries,
-              nir_shader_get_entrypoint(nir), true, false);
-
-   if (use_nak(dev->pdev, nir->info.stage))
-      nak_preprocess_nir(nir, NULL);
-   else
-      nvk_cg_preprocess_nir(nir);
+   nvk_preprocess_nir(&dev->pdev->vk, nir);
 
    vk_pipeline_cache_add_nir(cache, stage_sha1, sizeof(stage_sha1), nir);
 
diff --git a/src/nouveau/vulkan/nvk_shader.h b/src/nouveau/vulkan/nvk_shader.h
index 47ac49dc68131..ee392110dac79 100644
--- a/src/nouveau/vulkan/nvk_shader.h
+++ b/src/nouveau/vulkan/nvk_shader.h
@@ -94,10 +94,6 @@ VkShaderStageFlags nvk_nak_stages(const struct nv_device_info *info);
 uint64_t
 nvk_physical_device_compiler_flags(const struct nvk_physical_device *pdev);
 
-const nir_shader_compiler_options *
-nvk_physical_device_nir_options(const struct nvk_physical_device *pdev,
-                                gl_shader_stage stage);
-
 static inline nir_address_format
 nvk_buffer_addr_format(VkPipelineRobustnessBufferBehaviorEXT robustness)
 {
@@ -112,10 +108,6 @@ nvk_buffer_addr_format(VkPipelineRobustnessBufferBehaviorEXT robustness)
    }
 }
 
-struct spirv_to_nir_options
-nvk_physical_device_spirv_options(const struct nvk_physical_device *pdev,
-                                  const struct vk_pipeline_robustness_state *rs);
-
 bool
 nvk_nir_lower_descriptors(nir_shader *nir,
                           const struct vk_pipeline_robustness_state *rs,
-- 
GitLab


From ed875668aabbb1474d93dcbd0e7c73970c4a4716 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 4 Jan 2024 16:19:26 -0600
Subject: [PATCH 15/22] nvk: Move populate_fs_key to nvk_shader.c

---
 src/nouveau/vulkan/nvk_graphics_pipeline.c | 25 +---------------------
 src/nouveau/vulkan/nvk_shader.c            | 23 ++++++++++++++++++++
 src/nouveau/vulkan/nvk_shader.h            |  5 +++++
 3 files changed, 29 insertions(+), 24 deletions(-)

diff --git a/src/nouveau/vulkan/nvk_graphics_pipeline.c b/src/nouveau/vulkan/nvk_graphics_pipeline.c
index e212ca61b26ac..6e8efc597d372 100644
--- a/src/nouveau/vulkan/nvk_graphics_pipeline.c
+++ b/src/nouveau/vulkan/nvk_graphics_pipeline.c
@@ -4,7 +4,6 @@
  */
 #include "nvk_pipeline.h"
 
-#include "nvk_cmd_buffer.h"
 #include "nvk_device.h"
 #include "nvk_mme.h"
 #include "nvk_physical_device.h"
@@ -24,28 +23,6 @@
 #include "nvk_clb197.h"
 #include "nvk_clc397.h"
 
-static void
-nvk_populate_fs_key(struct nak_fs_key *key,
-                    const struct vk_multisample_state *ms,
-                    const struct vk_graphics_pipeline_state *state)
-{
-   memset(key, 0, sizeof(*key));
-
-   key->sample_locations_cb = 0;
-   key->sample_locations_offset = nvk_root_descriptor_offset(draw.sample_locations);
-
-   if (state->pipeline_flags &
-       VK_PIPELINE_CREATE_2_DEPTH_STENCIL_ATTACHMENT_FEEDBACK_LOOP_BIT_EXT)
-      key->zs_self_dep = true;
-
-   if (ms == NULL || ms->rasterization_samples <= 1)
-      return;
-
-   if (ms->sample_shading_enable &&
-       (ms->rasterization_samples * ms->min_sample_shading) > 1.0)
-      key->force_sample_shading = true;
-}
-
 static void
 emit_pipeline_ct_write_state(struct nv_push *p,
                              const struct vk_color_blend_state *cb,
@@ -188,7 +165,7 @@ nvk_graphics_pipeline_create(struct nvk_device *dev,
    struct vk_pipeline_cache_object *cache_objs[MESA_SHADER_STAGES] = {};
 
    struct nak_fs_key fs_key_tmp, *fs_key = NULL;
-   nvk_populate_fs_key(&fs_key_tmp, state.ms, &state);
+   nvk_populate_fs_key(&fs_key_tmp, &state);
    fs_key = &fs_key_tmp;
 
    for (uint32_t i = 0; i < pCreateInfo->stageCount; i++) {
diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index 86dd4c94b25db..0d4ffb338e0ab 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -4,6 +4,7 @@
  */
 #include "nvk_shader.h"
 
+#include "nvk_cmd_buffer.h"
 #include "nvk_descriptor_set_layout.h"
 #include "nvk_device.h"
 #include "nvk_physical_device.h"
@@ -185,6 +186,28 @@ nvk_preprocess_nir(struct vk_physical_device *vk_pdev, nir_shader *nir)
       nvk_cg_preprocess_nir(nir);
 }
 
+void
+nvk_populate_fs_key(struct nak_fs_key *key,
+                    const struct vk_graphics_pipeline_state *state)
+{
+   memset(key, 0, sizeof(*key));
+
+   key->sample_locations_cb = 0;
+   key->sample_locations_offset = nvk_root_descriptor_offset(draw.sample_locations);
+
+   if (state->pipeline_flags &
+       VK_PIPELINE_CREATE_2_DEPTH_STENCIL_ATTACHMENT_FEEDBACK_LOOP_BIT_EXT)
+      key->zs_self_dep = true;
+
+   const struct vk_multisample_state *ms = state->ms;
+   if (ms == NULL || ms->rasterization_samples <= 1)
+      return;
+
+   if (ms->sample_shading_enable &&
+       (ms->rasterization_samples * ms->min_sample_shading) > 1.0)
+      key->force_sample_shading = true;
+}
+
 static bool
 lower_load_global_constant_offset_instr(nir_builder *b,
                                         nir_intrinsic_instr *intrin,
diff --git a/src/nouveau/vulkan/nvk_shader.h b/src/nouveau/vulkan/nvk_shader.h
index ee392110dac79..9e77b2f78baaf 100644
--- a/src/nouveau/vulkan/nvk_shader.h
+++ b/src/nouveau/vulkan/nvk_shader.h
@@ -19,6 +19,7 @@ struct nvk_device;
 struct nvk_physical_device;
 struct nvk_pipeline_compilation_ctx;
 struct vk_descriptor_set_layout;
+struct vk_graphics_pipeline_state;
 struct vk_pipeline_cache;
 struct vk_pipeline_layout;
 struct vk_pipeline_robustness_state;
@@ -122,6 +123,10 @@ nvk_shader_stage_to_nir(struct nvk_device *dev,
                         struct vk_pipeline_cache *cache,
                         void *mem_ctx, struct nir_shader **nir_out);
 
+void
+nvk_populate_fs_key(struct nak_fs_key *key,
+                    const struct vk_graphics_pipeline_state *state);
+
 void
 nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
               const struct vk_pipeline_robustness_state *rs,
-- 
GitLab


From 331c500202bd235596da3156724a8fdba32a5ff6 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 4 Jan 2024 17:46:30 -0600
Subject: [PATCH 16/22] nvk: Pass an array of descriptor sets to nvk_lower_nir

---
 src/nouveau/vulkan/nvk_compute_pipeline.c  |  4 +++-
 src/nouveau/vulkan/nvk_graphics_pipeline.c |  4 +++-
 src/nouveau/vulkan/nvk_shader.c            | 26 ++++++++++++++++------
 src/nouveau/vulkan/nvk_shader.h            |  3 ++-
 4 files changed, 27 insertions(+), 10 deletions(-)

diff --git a/src/nouveau/vulkan/nvk_compute_pipeline.c b/src/nouveau/vulkan/nvk_compute_pipeline.c
index e51b45794c31e..5788666c1327b 100644
--- a/src/nouveau/vulkan/nvk_compute_pipeline.c
+++ b/src/nouveau/vulkan/nvk_compute_pipeline.c
@@ -214,7 +214,9 @@ nvk_compute_pipeline_create(struct nvk_device *dev,
       if(shader == NULL)
          return vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
 
-      nvk_lower_nir(dev, nir, &robustness, false, pipeline_layout,
+      nvk_lower_nir(dev, nir, &robustness, false,
+                    pipeline_layout->set_count,
+                    pipeline_layout->set_layouts,
                     &shader->cbuf_map);
 
       result = nvk_compile_nir(dev, nir, pipeline_flags, &robustness, NULL, cache, shader);
diff --git a/src/nouveau/vulkan/nvk_graphics_pipeline.c b/src/nouveau/vulkan/nvk_graphics_pipeline.c
index 6e8efc597d372..a57891e4d18e9 100644
--- a/src/nouveau/vulkan/nvk_graphics_pipeline.c
+++ b/src/nouveau/vulkan/nvk_graphics_pipeline.c
@@ -248,7 +248,9 @@ nvk_graphics_pipeline_create(struct nvk_device *dev,
          }
 
          nvk_lower_nir(dev, nir[stage], &robustness[stage],
-                       state.rp->view_mask != 0, pipeline_layout,
+                       state.rp->view_mask != 0,
+                       pipeline_layout->set_count,
+                       pipeline_layout->set_layouts,
                        &shader->cbuf_map);
 
          result = nvk_compile_nir(dev, nir[stage],
diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index 0d4ffb338e0ab..77d4c66aea876 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -259,14 +259,20 @@ lower_load_global_constant_offset_instr(nir_builder *b,
    return true;
 }
 
+struct lower_ycbcr_state {
+   uint32_t set_layout_count;
+   struct vk_descriptor_set_layout * const *set_layouts;
+};
+
 static const struct vk_ycbcr_conversion_state *
-lookup_ycbcr_conversion(const void *_layout, uint32_t set,
+lookup_ycbcr_conversion(const void *_state, uint32_t set,
                         uint32_t binding, uint32_t array_index)
 {
-   const struct vk_pipeline_layout *pipeline_layout = _layout;
-   assert(set < pipeline_layout->set_count);
+   const struct lower_ycbcr_state *state = _state;
+   assert(set < state->set_layout_count);
+   assert(state->set_layouts[set] != NULL);
    const struct nvk_descriptor_set_layout *set_layout =
-      vk_to_nvk_descriptor_set_layout(pipeline_layout->set_layouts[set]);
+      vk_to_nvk_descriptor_set_layout(state->set_layouts[set]);
    assert(binding < set_layout->binding_count);
 
    const struct nvk_descriptor_set_binding_layout *bind_layout =
@@ -343,7 +349,8 @@ void
 nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
               const struct vk_pipeline_robustness_state *rs,
               bool is_multiview,
-              const struct vk_pipeline_layout *layout,
+              uint32_t set_layout_count,
+              struct vk_descriptor_set_layout * const *set_layouts,
               struct nvk_cbuf_map *cbuf_map_out)
 {
    struct nvk_physical_device *pdev = nvk_device_physical(dev);
@@ -358,7 +365,12 @@ nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
                });
    }
 
-   NIR_PASS(_, nir, nir_vk_lower_ycbcr_tex, lookup_ycbcr_conversion, layout);
+   const struct lower_ycbcr_state ycbcr_state = {
+      .set_layout_count = set_layout_count,
+      .set_layouts = set_layouts,
+   };
+   NIR_PASS(_, nir, nir_vk_lower_ycbcr_tex,
+            lookup_ycbcr_conversion, &ycbcr_state);
 
    nir_lower_compute_system_values_options csv_options = {
       .has_base_workgroup_id = true,
@@ -417,7 +429,7 @@ nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
    }
 
    NIR_PASS(_, nir, nvk_nir_lower_descriptors, rs,
-            layout->set_count, layout->set_layouts, cbuf_map);
+            set_layout_count, set_layouts, cbuf_map);
    NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_global,
             nir_address_format_64bit_global);
    NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ssbo,
diff --git a/src/nouveau/vulkan/nvk_shader.h b/src/nouveau/vulkan/nvk_shader.h
index 9e77b2f78baaf..08128bb95bceb 100644
--- a/src/nouveau/vulkan/nvk_shader.h
+++ b/src/nouveau/vulkan/nvk_shader.h
@@ -131,7 +131,8 @@ void
 nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
               const struct vk_pipeline_robustness_state *rs,
               bool is_multiview,
-              const struct vk_pipeline_layout *layout,
+              uint32_t set_layout_count,
+              struct vk_descriptor_set_layout * const *set_layouts,
               struct nvk_cbuf_map *cbuf_map_out);
 
 VkResult
-- 
GitLab


From b556f7ed0f299dbc092eccbb0ca47892e0068965 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 11 Jan 2024 16:04:09 -0600
Subject: [PATCH 17/22] nvk: Move nir_lower_patch_vertices to nvk_lower_nir()

As long as it happens after we merge tess info between the two stages
(it does) then there's no need to have it in the pipeline code.  It's
just an optimization anyway.
---
 src/nouveau/vulkan/nvk_graphics_pipeline.c | 1 -
 src/nouveau/vulkan/nvk_shader.c            | 5 +++++
 2 files changed, 5 insertions(+), 1 deletion(-)

diff --git a/src/nouveau/vulkan/nvk_graphics_pipeline.c b/src/nouveau/vulkan/nvk_graphics_pipeline.c
index a57891e4d18e9..a59b18721bc83 100644
--- a/src/nouveau/vulkan/nvk_graphics_pipeline.c
+++ b/src/nouveau/vulkan/nvk_graphics_pipeline.c
@@ -224,7 +224,6 @@ nvk_graphics_pipeline_create(struct nvk_device *dev,
    }
 
    if (nir[MESA_SHADER_TESS_CTRL] && nir[MESA_SHADER_TESS_EVAL]) {
-      nir_lower_patch_vertices(nir[MESA_SHADER_TESS_EVAL], nir[MESA_SHADER_TESS_CTRL]->info.tess.tcs_vertices_out, NULL);
       merge_tess_info(&nir[MESA_SHADER_TESS_EVAL]->info, &nir[MESA_SHADER_TESS_CTRL]->info);
    }
 
diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index 77d4c66aea876..b2b70011a7843 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -365,6 +365,11 @@ nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
                });
    }
 
+   if (nir->info.stage == MESA_SHADER_TESS_EVAL) {
+      NIR_PASS(_, nir, nir_lower_patch_vertices,
+               nir->info.tess.tcs_vertices_out, NULL);
+   }
+
    const struct lower_ycbcr_state ycbcr_state = {
       .set_layout_count = set_layout_count,
       .set_layouts = set_layouts,
-- 
GitLab


From ef6aca83ee38998258ec568260645f9c9e71f7a0 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 9 Jan 2024 22:33:00 -0600
Subject: [PATCH 18/22] nvk: Use vk_render_pass_state::attachments for write
 masks

This pulls everything into nvk_cmd_draw.c where it's a bit easier to
manage.  When the time comes for switching to EXT_shader_object, this
will let us handle VK_EXT_dynamic_rendering_unused_attachments via the
common vk_pipeline code.
---
 src/nouveau/vulkan/nvk_cmd_draw.c          | 32 ++++++++++++----------
 src/nouveau/vulkan/nvk_graphics_pipeline.c | 26 ------------------
 src/nouveau/vulkan/nvk_mme.h               |  2 --
 3 files changed, 17 insertions(+), 43 deletions(-)

diff --git a/src/nouveau/vulkan/nvk_cmd_draw.c b/src/nouveau/vulkan/nvk_cmd_draw.c
index 6d4f846ce2db3..1e0b7240a585d 100644
--- a/src/nouveau/vulkan/nvk_cmd_draw.c
+++ b/src/nouveau/vulkan/nvk_cmd_draw.c
@@ -1706,20 +1706,15 @@ void
 nvk_mme_set_write_mask(struct mme_builder *b)
 {
    struct mme_value count = mme_load(b);
-   struct mme_value pipeline = nvk_mme_load_scratch(b, WRITE_MASK_PIPELINE);
-   struct mme_value dynamic = nvk_mme_load_scratch(b, WRITE_MASK_DYN);
+   struct mme_value mask = mme_load(b);
 
    /*
-      dynamic and pipeline are both bit fields
-
-      attachment index 88887777666655554444333322221111
-      component        abgrabgrabgrabgrabgrabgrabgrabgr
+    * mask is a bit field
+    *
+    * attachment index 88887777666655554444333322221111
+    * component        abgrabgrabgrabgrabgrabgrabgrabgr
    */
 
-   struct mme_value mask = mme_and(b, pipeline, dynamic);
-   mme_free_reg(b, pipeline);
-   mme_free_reg(b, dynamic);
-
    struct mme_value common_mask = mme_mov(b, mme_imm(1));
    struct mme_value first = mme_and(b, mask, mme_imm(BITFIELD_RANGE(0, 4)));
    struct mme_value i = mme_mov(b, mme_zero());
@@ -1806,22 +1801,29 @@ nvk_flush_cb_state(struct nvk_cmd_buffer *cmd)
    }
 
    if (BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_CB_WRITE_MASKS) ||
-       BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_CB_COLOR_WRITE_ENABLES)) {
+       BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_CB_COLOR_WRITE_ENABLES) ||
+       BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_RP_ATTACHMENTS)) {
       uint32_t color_write_enables = 0x0;
       for (uint8_t a = 0; a < render->color_att_count; a++) {
          if (dyn->cb.color_write_enables & BITFIELD_BIT(a))
             color_write_enables |= 0xf << (4 * a);
       }
 
-      uint32_t att_write_mask = 0x0;
+      uint32_t cb_att_write_mask = 0x0;
       for (uint8_t a = 0; a < render->color_att_count; a++)
-         att_write_mask |= dyn->cb.attachments[a].write_mask << (a * 4);
+         cb_att_write_mask |= dyn->cb.attachments[a].write_mask << (a * 4);
 
-      P_IMMD(p, NV9097, SET_MME_SHADOW_SCRATCH(NVK_MME_SCRATCH_WRITE_MASK_DYN),
-             color_write_enables & att_write_mask);
+      uint32_t rp_att_write_mask = 0x0;
+      for (uint8_t a = 0; a < MESA_VK_MAX_COLOR_ATTACHMENTS; a++) {
+         if (dyn->rp.attachments & (MESA_VK_RP_ATTACHMENT_COLOR_0_BIT << a))
+            rp_att_write_mask |= 0xf << (4 * a);
+      }
 
       P_1INC(p, NV9097, CALL_MME_MACRO(NVK_MME_SET_WRITE_MASK));
       P_INLINE_DATA(p, render->color_att_count);
+      P_INLINE_DATA(p, color_write_enables &
+                       cb_att_write_mask &
+                       rp_att_write_mask);
    }
 
    if (BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_CB_BLEND_CONSTANTS)) {
diff --git a/src/nouveau/vulkan/nvk_graphics_pipeline.c b/src/nouveau/vulkan/nvk_graphics_pipeline.c
index a59b18721bc83..a705a89f05c2c 100644
--- a/src/nouveau/vulkan/nvk_graphics_pipeline.c
+++ b/src/nouveau/vulkan/nvk_graphics_pipeline.c
@@ -23,30 +23,6 @@
 #include "nvk_clb197.h"
 #include "nvk_clc397.h"
 
-static void
-emit_pipeline_ct_write_state(struct nv_push *p,
-                             const struct vk_color_blend_state *cb,
-                             const struct vk_render_pass_state *rp)
-{
-   uint32_t write_mask = 0;
-   uint32_t att_count = 0;
-
-   if (rp != NULL) {
-      att_count = rp->color_attachment_count;
-      for (uint32_t a = 0; a < rp->color_attachment_count; a++) {
-         VkFormat att_format = rp->color_attachment_formats[a];
-         if (att_format != VK_FORMAT_UNDEFINED)
-            write_mask |= 0xf << (4 * a);
-      }
-   }
-
-   P_IMMD(p, NV9097, SET_MME_SHADOW_SCRATCH(NVK_MME_SCRATCH_WRITE_MASK_PIPELINE),
-          write_mask);
-
-   P_1INC(p, NV9097, CALL_MME_MACRO(NVK_MME_SET_WRITE_MASK));
-   P_INLINE_DATA(p, att_count);
-}
-
 static void
 emit_pipeline_xfb_state(struct nv_push *p, const struct nak_xfb_info *xfb)
 {
@@ -402,8 +378,6 @@ nvk_graphics_pipeline_create(struct nvk_device *dev,
 
    emit_pipeline_xfb_state(&push, &last_geom->info.vtg.xfb);
 
-   emit_pipeline_ct_write_state(&push, state.cb, state.rp);
-
    pipeline->push_dw_count = nv_push_dw_count(&push);
 
    if (force_max_samples)
diff --git a/src/nouveau/vulkan/nvk_mme.h b/src/nouveau/vulkan/nvk_mme.h
index 0a635f8c37b2c..d9ce76d66fcc9 100644
--- a/src/nouveau/vulkan/nvk_mme.h
+++ b/src/nouveau/vulkan/nvk_mme.h
@@ -37,8 +37,6 @@ enum nvk_mme_scratch {
    NVK_MME_SCRATCH_DRAW_PAD_DW,
    NVK_MME_SCRATCH_DRAW_IDX,
    NVK_MME_SCRATCH_VIEW_MASK,
-   NVK_MME_SCRATCH_WRITE_MASK_DYN,
-   NVK_MME_SCRATCH_WRITE_MASK_PIPELINE,
 
    /* Must be at the end */
    NVK_MME_NUM_SCRATCH,
-- 
GitLab


From 004c08429ccc602c568a2bbb443e3905864a629f Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 11 Jan 2024 18:18:54 -0600
Subject: [PATCH 19/22] nvk: Switch to shader objects

---
 src/nouveau/vulkan/meson.build             |   4 -
 src/nouveau/vulkan/nvk_cmd_buffer.c        |  42 +-
 src/nouveau/vulkan/nvk_cmd_buffer.h        |  22 +-
 src/nouveau/vulkan/nvk_cmd_dispatch.c      | 146 ++++++-
 src/nouveau/vulkan/nvk_cmd_draw.c          | 247 +++++++++--
 src/nouveau/vulkan/nvk_cmd_meta.c          |  14 +-
 src/nouveau/vulkan/nvk_device.c            |   3 +
 src/nouveau/vulkan/nvk_graphics_pipeline.c | 417 ------------------
 src/nouveau/vulkan/nvk_pipeline.h          |  74 ----
 src/nouveau/vulkan/nvk_query_pool.c        |  15 +-
 src/nouveau/vulkan/nvk_shader.c            | 484 ++++++++++++++-------
 src/nouveau/vulkan/nvk_shader.h            |  53 +--
 12 files changed, 740 insertions(+), 781 deletions(-)
 delete mode 100644 src/nouveau/vulkan/nvk_graphics_pipeline.c
 delete mode 100644 src/nouveau/vulkan/nvk_pipeline.h

diff --git a/src/nouveau/vulkan/meson.build b/src/nouveau/vulkan/meson.build
index 8b2356344e88e..ad831c741316e 100644
--- a/src/nouveau/vulkan/meson.build
+++ b/src/nouveau/vulkan/meson.build
@@ -16,7 +16,6 @@ nvk_files = files(
   'nvk_cmd_pool.c',
   'nvk_cmd_pool.h',
   'nvk_codegen.c',
-  'nvk_compute_pipeline.c',
   'nvk_descriptor_set.h',
   'nvk_descriptor_set.c',
   'nvk_descriptor_set_layout.c',
@@ -31,7 +30,6 @@ nvk_files = files(
   'nvk_event.h',
   'nvk_format.c',
   'nvk_format.h',
-  'nvk_graphics_pipeline.c',
   'nvk_heap.c',
   'nvk_heap.h',
   'nvk_image.c',
@@ -45,8 +43,6 @@ nvk_files = files(
   'nvk_nir_lower_descriptors.c',
   'nvk_physical_device.c',
   'nvk_physical_device.h',
-  'nvk_pipeline.c',
-  'nvk_pipeline.h',
   'nvk_private.h',
   'nvk_query_pool.c',
   'nvk_query_pool.h',
diff --git a/src/nouveau/vulkan/nvk_cmd_buffer.c b/src/nouveau/vulkan/nvk_cmd_buffer.c
index 435aa9ed90e71..8d4f6001892c1 100644
--- a/src/nouveau/vulkan/nvk_cmd_buffer.c
+++ b/src/nouveau/vulkan/nvk_cmd_buffer.c
@@ -13,7 +13,7 @@
 #include "nvk_entrypoints.h"
 #include "nvk_mme.h"
 #include "nvk_physical_device.h"
-#include "nvk_pipeline.h"
+#include "nvk_shader.h"
 
 #include "vk_pipeline_layout.h"
 #include "vk_synchronization.h"
@@ -551,33 +551,27 @@ nvk_CmdPipelineBarrier2(VkCommandBuffer commandBuffer,
    nvk_cmd_invalidate_deps(cmd, 1, pDependencyInfo);
 }
 
-VKAPI_ATTR void VKAPI_CALL
-nvk_CmdBindPipeline(VkCommandBuffer commandBuffer,
-                    VkPipelineBindPoint pipelineBindPoint,
-                    VkPipeline _pipeline)
+void
+nvk_cmd_bind_shaders(struct vk_command_buffer *vk_cmd,
+                     uint32_t stage_count,
+                     const gl_shader_stage *stages,
+                     struct vk_shader ** const shaders)
 {
-   VK_FROM_HANDLE(nvk_cmd_buffer, cmd, commandBuffer);
-   VK_FROM_HANDLE(nvk_pipeline, pipeline, _pipeline);
+   struct nvk_cmd_buffer *cmd = container_of(vk_cmd, struct nvk_cmd_buffer, vk);
    struct nvk_device *dev = nvk_cmd_buffer_device(cmd);
 
-   for (unsigned s = 0; s < ARRAY_SIZE(pipeline->shaders); s++) {
-      if(!pipeline->shaders[s])
-         continue;
-      if (pipeline->shaders[s]->info.slm_size)
-         nvk_device_ensure_slm(dev, pipeline->shaders[s]->info.slm_size);
-   }
+   for (uint32_t i = 0; i < stage_count; i++) {
+      struct nvk_shader *shader =
+         container_of(shaders[i], struct nvk_shader, vk);
 
-   switch (pipelineBindPoint) {
-   case VK_PIPELINE_BIND_POINT_GRAPHICS:
-      assert(pipeline->type == NVK_PIPELINE_GRAPHICS);
-      nvk_cmd_bind_graphics_pipeline(cmd, (void *)pipeline);
-      break;
-   case VK_PIPELINE_BIND_POINT_COMPUTE:
-      assert(pipeline->type == NVK_PIPELINE_COMPUTE);
-      nvk_cmd_bind_compute_pipeline(cmd, (void *)pipeline);
-      break;
-   default:
-      unreachable("Unhandled bind point");
+      if (shader != NULL && shader->info.slm_size > 0)
+         nvk_device_ensure_slm(dev, shader->info.slm_size);
+
+      if (stages[i] == MESA_SHADER_COMPUTE ||
+          stages[i] == MESA_SHADER_KERNEL)
+         nvk_cmd_bind_compute_shader(cmd, shader);
+      else
+         nvk_cmd_bind_graphics_shader(cmd, stages[i], shader);
    }
 }
 
diff --git a/src/nouveau/vulkan/nvk_cmd_buffer.h b/src/nouveau/vulkan/nvk_cmd_buffer.h
index 7c9c4b7511971..a087272cd9fe2 100644
--- a/src/nouveau/vulkan/nvk_cmd_buffer.h
+++ b/src/nouveau/vulkan/nvk_cmd_buffer.h
@@ -24,6 +24,7 @@ struct nvk_cmd_pool;
 struct nvk_image_view;
 struct nvk_push_descriptor_set;
 struct nvk_shader;
+struct vk_shader;
 
 struct nvk_sample_location {
    uint8_t x_u4:4;
@@ -102,9 +103,11 @@ struct nvk_rendering_state {
 
 struct nvk_graphics_state {
    struct nvk_rendering_state render;
-   struct nvk_graphics_pipeline *pipeline;
    struct nvk_descriptor_state descriptors;
 
+   uint32_t shaders_dirty;
+   struct nvk_shader *shaders[MESA_SHADER_MESH + 1];
+
    /* Used for meta save/restore */
    struct nvk_addr_range vb0;
 
@@ -114,8 +117,8 @@ struct nvk_graphics_state {
 };
 
 struct nvk_compute_state {
-   struct nvk_compute_pipeline *pipeline;
    struct nvk_descriptor_state descriptors;
+   struct nvk_shader *shader;
 };
 
 struct nvk_cmd_push {
@@ -209,10 +212,17 @@ void nvk_cmd_buffer_begin_compute(struct nvk_cmd_buffer *cmd,
 void nvk_cmd_invalidate_graphics_state(struct nvk_cmd_buffer *cmd);
 void nvk_cmd_invalidate_compute_state(struct nvk_cmd_buffer *cmd);
 
-void nvk_cmd_bind_graphics_pipeline(struct nvk_cmd_buffer *cmd,
-                                    struct nvk_graphics_pipeline *pipeline);
-void nvk_cmd_bind_compute_pipeline(struct nvk_cmd_buffer *cmd,
-                                   struct nvk_compute_pipeline *pipeline);
+void nvk_cmd_bind_shaders(struct vk_command_buffer *vk_cmd,
+                          uint32_t stage_count,
+                          const gl_shader_stage *stages,
+                          struct vk_shader ** const shaders);
+
+void nvk_cmd_bind_graphics_shader(struct nvk_cmd_buffer *cmd,
+                                  const gl_shader_stage stage,
+                                  struct nvk_shader *shader);
+
+void nvk_cmd_bind_compute_shader(struct nvk_cmd_buffer *cmd,
+                                 struct nvk_shader *shader);
 
 void nvk_cmd_bind_vertex_buffer(struct nvk_cmd_buffer *cmd, uint32_t vb_idx,
                                 struct nvk_addr_range addr_range);
diff --git a/src/nouveau/vulkan/nvk_cmd_dispatch.c b/src/nouveau/vulkan/nvk_cmd_dispatch.c
index 077349db39635..981adb4423e5d 100644
--- a/src/nouveau/vulkan/nvk_cmd_dispatch.c
+++ b/src/nouveau/vulkan/nvk_cmd_dispatch.c
@@ -9,7 +9,7 @@
 #include "nvk_entrypoints.h"
 #include "nvk_mme.h"
 #include "nvk_physical_device.h"
-#include "nvk_pipeline.h"
+#include "nvk_shader.h"
 
 #include "nouveau_context.h"
 
@@ -41,6 +41,11 @@
 #define NVC6C0_QMDV03_00_VAL_SET(p,a...) NVVAL_MW_SET((p), NVC6C0, QMDV03_00, ##a)
 #define NVC6C0_QMDV03_00_DEF_SET(p,a...) NVDEF_MW_SET((p), NVC6C0, QMDV03_00, ##a)
 
+#define QMD_DEF_SET(qmd, class_id, version_major, version_minor, a...) \
+   NVDEF_MW_SET((qmd), NV##class_id, QMDV##version_major##_##version_minor, ##a)
+#define QMD_VAL_SET(qmd, class_id, version_major, version_minor, a...) \
+   NVVAL_MW_SET((qmd), NV##class_id, QMDV##version_major##_##version_minor, ##a)
+
 VkResult
 nvk_push_dispatch_state_init(struct nvk_device *dev, struct nv_push *p)
 {
@@ -97,6 +102,129 @@ nvk_cmd_invalidate_compute_state(struct nvk_cmd_buffer *cmd)
    memset(&cmd->state.cs, 0, sizeof(cmd->state.cs));
 }
 
+static int
+gv100_sm_config_smem_size(uint32_t size)
+{
+   if      (size > 64 * 1024) size = 96 * 1024;
+   else if (size > 32 * 1024) size = 64 * 1024;
+   else if (size > 16 * 1024) size = 32 * 1024;
+   else if (size >  8 * 1024) size = 16 * 1024;
+   else                       size =  8 * 1024;
+   return (size / 4096) + 1;
+}
+
+#define nvk_qmd_init_base(qmd, shader, class_id, version_major, version_minor)   \
+do {                                                                                                   \
+   QMD_DEF_SET(qmd, class_id, version_major, version_minor, API_VISIBLE_CALL_LIMIT, NO_CHECK);         \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, BARRIER_COUNT, shader->info.num_barriers);      \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, CTA_THREAD_DIMENSION0,                     \
+                                                            shader->info.cs.local_size[0]);                 \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, CTA_THREAD_DIMENSION1,                     \
+                                                            shader->info.cs.local_size[1]);                 \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, CTA_THREAD_DIMENSION2,                     \
+                                                            shader->info.cs.local_size[2]);                 \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, QMD_MAJOR_VERSION, version_major);         \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, QMD_VERSION, version_minor);               \
+   QMD_DEF_SET(qmd, class_id, version_major, version_minor, SAMPLER_INDEX, INDEPENDENTLY);             \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, SHADER_LOCAL_MEMORY_HIGH_SIZE, 0);         \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, SHADER_LOCAL_MEMORY_LOW_SIZE,              \
+                                                            align(shader->info.slm_size, 0x10));            \
+   QMD_VAL_SET(qmd, class_id, version_major, version_minor, SHARED_MEMORY_SIZE,                        \
+                                                            align(shader->info.cs.smem_size, 0x100));       \
+} while (0)
+
+static void
+nva0c0_qmd_init(uint32_t *qmd, const struct nvk_shader *shader)
+{
+   nvk_qmd_init_base(qmd, shader, A0C0, 00, 06);
+
+   if (shader->info.cs.smem_size <= (16 << 10))
+      NVA0C0_QMDV00_06_DEF_SET(qmd, L1_CONFIGURATION, DIRECTLY_ADDRESSABLE_MEMORY_SIZE_16KB);
+   else if (shader->info.cs.smem_size <= (32 << 10))
+      NVA0C0_QMDV00_06_DEF_SET(qmd, L1_CONFIGURATION, DIRECTLY_ADDRESSABLE_MEMORY_SIZE_32KB);
+   else if (shader->info.cs.smem_size <= (48 << 10))
+      NVA0C0_QMDV00_06_DEF_SET(qmd, L1_CONFIGURATION, DIRECTLY_ADDRESSABLE_MEMORY_SIZE_48KB);
+   else
+      unreachable("Invalid shared memory size");
+
+   uint64_t addr = shader->hdr_addr;
+   assert(addr < 0xffffffff);
+   NVA0C0_QMDV00_06_VAL_SET(qmd, PROGRAM_OFFSET, addr);
+   NVA0C0_QMDV00_06_VAL_SET(qmd, REGISTER_COUNT, shader->info.num_gprs);
+   NVA0C0_QMDV00_06_VAL_SET(qmd, SASS_VERSION, 0x30);
+}
+
+static void
+nvc0c0_qmd_init(uint32_t *qmd, const struct nvk_shader *shader)
+{
+   nvk_qmd_init_base(qmd, shader, C0C0, 02, 01);
+
+   uint64_t addr = shader->hdr_addr;
+   assert(addr < 0xffffffff);
+
+   NVC0C0_QMDV02_01_VAL_SET(qmd, SM_GLOBAL_CACHING_ENABLE, 1);
+   NVC0C0_QMDV02_01_VAL_SET(qmd, PROGRAM_OFFSET, addr);
+   NVC0C0_QMDV02_01_VAL_SET(qmd, REGISTER_COUNT, shader->info.num_gprs);
+}
+
+static void
+nvc3c0_qmd_init(uint32_t *qmd, const struct nvk_shader *shader)
+{
+   nvk_qmd_init_base(qmd, shader, C3C0, 02, 02);
+
+   NVC3C0_QMDV02_02_VAL_SET(qmd, SM_GLOBAL_CACHING_ENABLE, 1);
+   /* those are all QMD 2.2+ */
+   NVC3C0_QMDV02_02_VAL_SET(qmd, MIN_SM_CONFIG_SHARED_MEM_SIZE,
+                            gv100_sm_config_smem_size(shader->info.cs.smem_size));
+   NVC3C0_QMDV02_02_VAL_SET(qmd, MAX_SM_CONFIG_SHARED_MEM_SIZE,
+                            gv100_sm_config_smem_size(NVK_MAX_SHARED_SIZE));
+   NVC3C0_QMDV02_02_VAL_SET(qmd, TARGET_SM_CONFIG_SHARED_MEM_SIZE,
+                            gv100_sm_config_smem_size(shader->info.cs.smem_size));
+
+   NVC3C0_QMDV02_02_VAL_SET(qmd, REGISTER_COUNT_V, shader->info.num_gprs);
+
+   uint64_t addr = shader->hdr_addr;
+   NVC3C0_QMDV02_02_VAL_SET(qmd, PROGRAM_ADDRESS_LOWER, addr & 0xffffffff);
+   NVC3C0_QMDV02_02_VAL_SET(qmd, PROGRAM_ADDRESS_UPPER, addr >> 32);
+}
+
+static void
+nvc6c0_qmd_init(uint32_t *qmd, const struct nvk_shader *shader)
+{
+   nvk_qmd_init_base(qmd, shader, C6C0, 03, 00);
+
+   NVC6C0_QMDV03_00_VAL_SET(qmd, SM_GLOBAL_CACHING_ENABLE, 1);
+   /* those are all QMD 2.2+ */
+   NVC6C0_QMDV03_00_VAL_SET(qmd, MIN_SM_CONFIG_SHARED_MEM_SIZE,
+                            gv100_sm_config_smem_size(shader->info.cs.smem_size));
+   NVC6C0_QMDV03_00_VAL_SET(qmd, MAX_SM_CONFIG_SHARED_MEM_SIZE,
+                            gv100_sm_config_smem_size(NVK_MAX_SHARED_SIZE));
+   NVC6C0_QMDV03_00_VAL_SET(qmd, TARGET_SM_CONFIG_SHARED_MEM_SIZE,
+                            gv100_sm_config_smem_size(shader->info.cs.smem_size));
+
+   NVC6C0_QMDV03_00_VAL_SET(qmd, REGISTER_COUNT_V, shader->info.num_gprs);
+
+   uint64_t addr = shader->hdr_addr;
+   NVC6C0_QMDV03_00_VAL_SET(qmd, PROGRAM_ADDRESS_LOWER, addr & 0xffffffff);
+   NVC6C0_QMDV03_00_VAL_SET(qmd, PROGRAM_ADDRESS_UPPER, addr >> 32);
+}
+
+static void
+nvk_qmd_init(struct nvk_physical_device *pdev,
+             uint32_t *qmd, const struct nvk_shader *shader)
+{
+   if (pdev->info.cls_compute >= AMPERE_COMPUTE_A)
+      nvc6c0_qmd_init(qmd, shader);
+   else if (pdev->info.cls_compute >= VOLTA_COMPUTE_A)
+      nvc3c0_qmd_init(qmd, shader);
+   else if (pdev->info.cls_compute >= PASCAL_COMPUTE_A)
+      nvc0c0_qmd_init(qmd, shader);
+   else if (pdev->info.cls_compute >= KEPLER_COMPUTE_A)
+      nva0c0_qmd_init(qmd, shader);
+   else
+      unreachable("Unknown GPU generation");
+}
+
 static void
 nva0c0_qmd_set_dispatch_size(UNUSED struct nvk_device *dev, uint32_t *qmd,
                              uint32_t x, uint32_t y, uint32_t z)
@@ -171,18 +299,16 @@ nvc6c0_cp_launch_desc_set_cb(uint32_t *qmd, unsigned index,
 
 
 void
-nvk_cmd_bind_compute_pipeline(struct nvk_cmd_buffer *cmd,
-                              struct nvk_compute_pipeline *pipeline)
+nvk_cmd_bind_compute_shader(struct nvk_cmd_buffer *cmd,
+                            struct nvk_shader *shader)
 {
-   cmd->state.cs.pipeline = pipeline;
+   cmd->state.cs.shader = shader;
 }
 
 static uint32_t
 nvk_compute_local_size(struct nvk_cmd_buffer *cmd)
 {
-   const struct nvk_compute_pipeline *pipeline = cmd->state.cs.pipeline;
-   const struct nvk_shader *shader =
-      pipeline->base.shaders[MESA_SHADER_COMPUTE];
+   const struct nvk_shader *shader = cmd->state.cs.shader;
 
    return shader->info.cs.local_size[0] *
           shader->info.cs.local_size[1] *
@@ -196,7 +322,7 @@ nvk_flush_compute_state(struct nvk_cmd_buffer *cmd,
    struct nvk_device *dev = nvk_cmd_buffer_device(cmd);
    struct nvk_physical_device *pdev = nvk_device_physical(dev);
    const uint32_t min_cbuf_alignment = nvk_min_cbuf_alignment(&pdev->info);
-   const struct nvk_compute_pipeline *pipeline = cmd->state.cs.pipeline;
+   const struct nvk_shader *shader = cmd->state.cs.shader;
    struct nvk_descriptor_state *desc = &cmd->state.cs.descriptors;
    VkResult result;
 
@@ -224,7 +350,7 @@ nvk_flush_compute_state(struct nvk_cmd_buffer *cmd,
 
    uint32_t qmd[128];
    memset(qmd, 0, sizeof(qmd));
-   memcpy(qmd, pipeline->qmd_template, sizeof(pipeline->qmd_template));
+   nvk_qmd_init(pdev, qmd, shader);
 
    if (nvk_cmd_buffer_compute_cls(cmd) >= AMPERE_COMPUTE_A) {
       nvc6c0_qmd_set_dispatch_size(nvk_cmd_buffer_device(cmd), qmd,
@@ -244,8 +370,6 @@ nvk_flush_compute_state(struct nvk_cmd_buffer *cmd,
                                    desc->root.cs.group_count[2]);
    }
 
-   const struct nvk_shader *shader =
-      pipeline->base.shaders[MESA_SHADER_COMPUTE];
    for (uint32_t c = 0; c < shader->cbuf_map.cbuf_count; c++) {
       const struct nvk_cbuf *cbuf = &shader->cbuf_map.cbufs[c];
 
diff --git a/src/nouveau/vulkan/nvk_cmd_draw.c b/src/nouveau/vulkan/nvk_cmd_draw.c
index 1e0b7240a585d..7a17022100a78 100644
--- a/src/nouveau/vulkan/nvk_cmd_draw.c
+++ b/src/nouveau/vulkan/nvk_cmd_draw.c
@@ -11,7 +11,7 @@
 #include "nvk_image_view.h"
 #include "nvk_mme.h"
 #include "nvk_physical_device.h"
-#include "nvk_pipeline.h"
+#include "nvk_shader.h"
 
 #include "nil_format.h"
 #include "util/bitpack_helpers.h"
@@ -370,13 +370,6 @@ nvk_push_draw_state_init(struct nvk_device *dev, struct nv_push *p)
       P_NV9097_SET_PROGRAM_REGION_B(p, shader_base_addr);
    }
 
-   for (uint32_t i = 0; i < 6; i++) {
-      P_IMMD(p, NV9097, SET_PIPELINE_SHADER(i), {
-         .enable  = ENABLE_FALSE,
-         .type    = i,
-      });
-   }
-
    for (uint32_t group = 0; group < 5; group++) {
       for (uint32_t slot = 0; slot < 16; slot++) {
          P_IMMD(p, NV9097, BIND_GROUP_CONSTANT_BUFFER(group), {
@@ -495,6 +488,8 @@ nvk_cmd_buffer_begin_graphics(struct nvk_cmd_buffer *cmd,
          nvk_cmd_buffer_dirty_render_pass(cmd);
       }
    }
+
+   cmd->state.gfx.shaders_dirty = ~0;
 }
 
 void
@@ -514,6 +509,8 @@ nvk_cmd_invalidate_graphics_state(struct nvk_cmd_buffer *cmd)
    struct nvk_rendering_state render_save = cmd->state.gfx.render;
    memset(&cmd->state.gfx, 0, sizeof(cmd->state.gfx));
    cmd->state.gfx.render = render_save;
+
+   cmd->state.gfx.shaders_dirty = ~0;
 }
 
 static void
@@ -951,23 +948,223 @@ nvk_CmdEndRendering(VkCommandBuffer commandBuffer)
 }
 
 void
-nvk_cmd_bind_graphics_pipeline(struct nvk_cmd_buffer *cmd,
-                               struct nvk_graphics_pipeline *pipeline)
+nvk_cmd_bind_graphics_shader(struct nvk_cmd_buffer *cmd,
+                             const gl_shader_stage stage,
+                             struct nvk_shader *shader)
 {
-   cmd->state.gfx.pipeline = pipeline;
-   vk_cmd_set_dynamic_graphics_state(&cmd->vk, &pipeline->dynamic);
+   struct vk_dynamic_graphics_state *dyn = &cmd->vk.dynamic_graphics_state;
+
+   assert(stage < ARRAY_SIZE(cmd->state.gfx.shaders));
+   if (cmd->state.gfx.shaders[stage] == shader)
+      return;
+
+   cmd->state.gfx.shaders[stage] = shader;
+   cmd->state.gfx.shaders_dirty |= BITFIELD_BIT(stage);
 
    /* When a pipeline with tess shaders is bound we need to re-upload the
     * tessellation parameters at flush_ts_state, as the domain origin can be
     * dynamic.
     */
-   if (nvk_shader_is_enabled(pipeline->base.shaders[MESA_SHADER_TESS_EVAL])) {
-      BITSET_SET(cmd->vk.dynamic_graphics_state.dirty,
-                 MESA_VK_DYNAMIC_TS_DOMAIN_ORIGIN);
+   if (stage == MESA_SHADER_TESS_EVAL)
+      BITSET_SET(dyn->dirty, MESA_VK_DYNAMIC_TS_DOMAIN_ORIGIN);
+
+   /* Emitting SET_HYBRID_ANTI_ALIAS_CONTROL requires the fragment shader */
+   if (stage == MESA_SHADER_FRAGMENT)
+      BITSET_SET(dyn->dirty, MESA_VK_DYNAMIC_MS_RASTERIZATION_SAMPLES);
+}
+
+static uint32_t
+mesa_to_nv9097_shader_type(gl_shader_stage stage)
+{
+   static const uint32_t mesa_to_nv9097[] = {
+      [MESA_SHADER_VERTEX]    = NV9097_SET_PIPELINE_SHADER_TYPE_VERTEX,
+      [MESA_SHADER_TESS_CTRL] = NV9097_SET_PIPELINE_SHADER_TYPE_TESSELLATION_INIT,
+      [MESA_SHADER_TESS_EVAL] = NV9097_SET_PIPELINE_SHADER_TYPE_TESSELLATION,
+      [MESA_SHADER_GEOMETRY]  = NV9097_SET_PIPELINE_SHADER_TYPE_GEOMETRY,
+      [MESA_SHADER_FRAGMENT]  = NV9097_SET_PIPELINE_SHADER_TYPE_PIXEL,
+   };
+   assert(stage < ARRAY_SIZE(mesa_to_nv9097));
+   return mesa_to_nv9097[stage];
+}
+
+static uint32_t
+nvk_pipeline_bind_group(gl_shader_stage stage)
+{
+   return stage;
+}
+
+static void
+nvk_flush_shaders(struct nvk_cmd_buffer *cmd)
+{
+   if (cmd->state.gfx.shaders_dirty == 0)
+      return;
+
+   /* Map shader types to shaders */
+   struct nvk_shader *type_shader[6] = { NULL, };
+   uint32_t types_dirty = 0;
+
+   const uint32_t gfx_stages = BITFIELD_BIT(MESA_SHADER_VERTEX) |
+                               BITFIELD_BIT(MESA_SHADER_TESS_CTRL) |
+                               BITFIELD_BIT(MESA_SHADER_TESS_EVAL) |
+                               BITFIELD_BIT(MESA_SHADER_GEOMETRY) |
+                               BITFIELD_BIT(MESA_SHADER_FRAGMENT);
+
+   u_foreach_bit(stage, cmd->state.gfx.shaders_dirty & gfx_stages) {
+      uint32_t type = mesa_to_nv9097_shader_type(stage);
+      types_dirty |= BITFIELD_BIT(type);
+
+      /* Only copy non-NULL shaders because mesh/task alias with vertex and
+       * tessellation stages.
+       */
+      if (cmd->state.gfx.shaders[stage] != NULL) {
+         assert(type < ARRAY_SIZE(type_shader));
+         assert(type_shader[type] == NULL);
+         type_shader[type] = cmd->state.gfx.shaders[stage];
+      }
+   }
+
+   u_foreach_bit(type, types_dirty) {
+      struct nvk_shader *shader = type_shader[type];
+
+      /* We always map index == type */
+      const uint32_t idx = type;
+
+      struct nv_push *p = nvk_cmd_buffer_push(cmd, 8);
+      P_IMMD(p, NV9097, SET_PIPELINE_SHADER(idx), {
+         .enable  = shader != NULL,
+         .type    = type,
+      });
+
+      if (shader == NULL)
+         continue;
+
+      uint64_t addr = shader->hdr_addr;
+      if (nvk_cmd_buffer_3d_cls(cmd) >= VOLTA_A) {
+         P_MTHD(p, NVC397, SET_PIPELINE_PROGRAM_ADDRESS_A(idx));
+         P_NVC397_SET_PIPELINE_PROGRAM_ADDRESS_A(p, idx, addr >> 32);
+         P_NVC397_SET_PIPELINE_PROGRAM_ADDRESS_B(p, idx, addr);
+      } else {
+         assert(addr < 0xffffffff);
+         P_IMMD(p, NV9097, SET_PIPELINE_PROGRAM(idx), addr);
+      }
+
+      P_MTHD(p, NVC397, SET_PIPELINE_REGISTER_COUNT(idx));
+      P_NVC397_SET_PIPELINE_REGISTER_COUNT(p, idx, shader->info.num_gprs);
+      P_NVC397_SET_PIPELINE_BINDING(p, idx,
+         nvk_pipeline_bind_group(shader->info.stage));
+
+      if (shader->info.stage == MESA_SHADER_FRAGMENT) {
+         p = nvk_cmd_buffer_push(cmd, 9);
+
+         P_MTHD(p, NVC397, SET_SUBTILING_PERF_KNOB_A);
+         P_NV9097_SET_SUBTILING_PERF_KNOB_A(p, {
+            .fraction_of_spm_register_file_per_subtile         = 0x10,
+            .fraction_of_spm_pixel_output_buffer_per_subtile   = 0x40,
+            .fraction_of_spm_triangle_ram_per_subtile          = 0x16,
+            .fraction_of_max_quads_per_subtile                 = 0x20,
+         });
+         P_NV9097_SET_SUBTILING_PERF_KNOB_B(p, 0x20);
+
+         P_IMMD(p, NV9097, SET_API_MANDATED_EARLY_Z,
+                shader->info.fs.early_fragment_tests);
+
+         if (nvk_cmd_buffer_3d_cls(cmd) >= MAXWELL_B) {
+            P_IMMD(p, NVB197, SET_POST_Z_PS_IMASK,
+                   shader->info.fs.post_depth_coverage);
+         } else {
+            assert(!shader->info.fs.post_depth_coverage);
+         }
+
+         P_IMMD(p, NV9097, SET_ZCULL_BOUNDS, {
+            .z_min_unbounded_enable = shader->info.fs.writes_depth,
+            .z_max_unbounded_enable = shader->info.fs.writes_depth,
+         });
+      }
+   }
+
+   const uint32_t vtg_stages = BITFIELD_BIT(MESA_SHADER_VERTEX) |
+                               BITFIELD_BIT(MESA_SHADER_TESS_EVAL) |
+                               BITFIELD_BIT(MESA_SHADER_GEOMETRY);
+   const uint32_t vtgm_stages = vtg_stages | BITFIELD_BIT(MESA_SHADER_MESH);
+
+   if (cmd->state.gfx.shaders_dirty & vtg_stages) {
+      struct nak_xfb_info *xfb = NULL;
+      u_foreach_bit(stage, vtg_stages) {
+         if (cmd->state.gfx.shaders[stage] != NULL)
+            xfb = &cmd->state.gfx.shaders[stage]->info.vtg.xfb;
+      }
+
+      if (xfb == NULL) {
+         struct nv_push *p = nvk_cmd_buffer_push(cmd, 8);
+         for (uint8_t b = 0; b < 4; b++)
+            P_IMMD(p, NV9097, SET_STREAM_OUT_CONTROL_COMPONENT_COUNT(b), 0);
+      } else {
+         for (uint8_t b = 0; b < ARRAY_SIZE(xfb->attr_count); b++) {
+            const uint8_t attr_count = xfb->attr_count[b];
+            /* upload packed varying indices in multiples of 4 bytes */
+            const uint32_t n = DIV_ROUND_UP(attr_count, 4);
+
+            struct nv_push *p = nvk_cmd_buffer_push(cmd, 5 + n);
+
+            P_MTHD(p, NV9097, SET_STREAM_OUT_CONTROL_STREAM(b));
+            P_NV9097_SET_STREAM_OUT_CONTROL_STREAM(p, b, xfb->stream[b]);
+            P_NV9097_SET_STREAM_OUT_CONTROL_COMPONENT_COUNT(p, b, attr_count);
+            P_NV9097_SET_STREAM_OUT_CONTROL_STRIDE(p, b, xfb->stride[b]);
+
+            if (n > 0) {
+               P_MTHD(p, NV9097, SET_STREAM_OUT_LAYOUT_SELECT(b, 0));
+               P_INLINE_ARRAY(p, (const uint32_t*)xfb->attr_index[b], n);
+            }
+         }
+      }
+   }
+
+   if (cmd->state.gfx.shaders_dirty & vtgm_stages) {
+      struct nvk_shader *last_vtgm = NULL;
+      u_foreach_bit(stage, vtgm_stages) {
+         if (cmd->state.gfx.shaders[stage] != NULL)
+            last_vtgm = cmd->state.gfx.shaders[stage];
+      }
+
+      struct nv_push *p = nvk_cmd_buffer_push(cmd, 6);
+
+      P_IMMD(p, NV9097, SET_RT_LAYER, {
+         .v       = 0,
+         .control = last_vtgm->info.vtg.writes_layer ?
+                    CONTROL_GEOMETRY_SHADER_SELECTS_LAYER :
+                    CONTROL_V_SELECTS_LAYER,
+      });
+
+      P_IMMD(p, NV9097, SET_ATTRIBUTE_POINT_SIZE, {
+         .enable  = last_vtgm->info.vtg.writes_point_size,
+         .slot    = 0,
+      });
+
+      const uint8_t clip_enable = last_vtgm->info.vtg.clip_enable;
+      const uint8_t cull_enable = last_vtgm->info.vtg.cull_enable;
+      P_IMMD(p, NV9097, SET_USER_CLIP_ENABLE, {
+         .plane0 = ((clip_enable | cull_enable) >> 0) & 1,
+         .plane1 = ((clip_enable | cull_enable) >> 1) & 1,
+         .plane2 = ((clip_enable | cull_enable) >> 2) & 1,
+         .plane3 = ((clip_enable | cull_enable) >> 3) & 1,
+         .plane4 = ((clip_enable | cull_enable) >> 4) & 1,
+         .plane5 = ((clip_enable | cull_enable) >> 5) & 1,
+         .plane6 = ((clip_enable | cull_enable) >> 6) & 1,
+         .plane7 = ((clip_enable | cull_enable) >> 7) & 1,
+      });
+      P_IMMD(p, NV9097, SET_USER_CLIP_OP, {
+         .plane0 = (cull_enable >> 0) & 1,
+         .plane1 = (cull_enable >> 1) & 1,
+         .plane2 = (cull_enable >> 2) & 1,
+         .plane3 = (cull_enable >> 3) & 1,
+         .plane4 = (cull_enable >> 4) & 1,
+         .plane5 = (cull_enable >> 5) & 1,
+         .plane6 = (cull_enable >> 6) & 1,
+         .plane7 = (cull_enable >> 7) & 1,
+      });
    }
 
-   struct nv_push *p = nvk_cmd_buffer_push(cmd, pipeline->push_dw_count);
-   nv_push_raw(p, pipeline->push_data, pipeline->push_dw_count);
+   cmd->state.gfx.shaders_dirty = 0;
 }
 
 static void
@@ -1045,11 +1242,10 @@ nvk_flush_ts_state(struct nvk_cmd_buffer *cmd)
    }
 
    if (BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_TS_DOMAIN_ORIGIN)) {
-      const struct nvk_graphics_pipeline *pipeline= cmd->state.gfx.pipeline;
       const struct nvk_shader *shader =
-         pipeline->base.shaders[MESA_SHADER_TESS_EVAL];
+         cmd->state.gfx.shaders[MESA_SHADER_TESS_EVAL];
 
-      if (nvk_shader_is_enabled(shader)) {
+      if (shader != NULL) {
          enum nak_ts_prims prims = shader->info.ts.prims;
          /* When the origin is lower-left, we have to flip the winding order */
          if (dyn->ts.domain_origin == VK_TESSELLATION_DOMAIN_ORIGIN_LOWER_LEFT) {
@@ -1433,9 +1629,10 @@ nvk_flush_ms_state(struct nvk_cmd_buffer *cmd)
                 dyn->ms.rasterization_samples == render->samples);
       }
 
-      const struct nvk_graphics_pipeline *pipeline = cmd->state.gfx.pipeline;
+      struct nvk_shader *fs = cmd->state.gfx.shaders[MESA_SHADER_FRAGMENT];
+      const float min_sample_shading = fs != NULL ? fs->min_sample_shading : 0;
       uint32_t min_samples = ceilf(dyn->ms.rasterization_samples *
-                                   pipeline->min_sample_shading);
+                                   min_sample_shading);
       min_samples = util_next_power_of_two(MAX2(1, min_samples));
 
       P_IMMD(p, NV9097, SET_HYBRID_ANTI_ALIAS_CONTROL, {
@@ -1923,7 +2120,6 @@ nvk_flush_descriptors(struct nvk_cmd_buffer *cmd)
    struct nvk_device *dev = nvk_cmd_buffer_device(cmd);
    struct nvk_physical_device *pdev = nvk_device_physical(dev);
    const uint32_t min_cbuf_alignment = nvk_min_cbuf_alignment(&pdev->info);
-   const struct nvk_graphics_pipeline *pipeline = cmd->state.gfx.pipeline;
    struct nvk_descriptor_state *desc = &cmd->state.gfx.descriptors;
    VkResult result;
 
@@ -1952,8 +2148,8 @@ nvk_flush_descriptors(struct nvk_cmd_buffer *cmd)
    /* Find cbuf maps for the 5 cbuf groups */
    const struct nvk_shader *cbuf_shaders[5] = { NULL, };
    for (gl_shader_stage stage = 0; stage < MESA_SHADER_STAGES; stage++) {
-      const struct nvk_shader *shader = pipeline->base.shaders[stage];
-      if (!shader || shader->code_size == 0)
+      const struct nvk_shader *shader = cmd->state.gfx.shaders[stage];
+      if (shader == NULL)
          continue;
 
       uint32_t group = nvk_cbuf_binding_for_stage(stage);
@@ -2053,6 +2249,7 @@ nvk_flush_descriptors(struct nvk_cmd_buffer *cmd)
 static void
 nvk_flush_gfx_state(struct nvk_cmd_buffer *cmd)
 {
+   nvk_flush_shaders(cmd);
    nvk_flush_dynamic_state(cmd);
    nvk_flush_descriptors(cmd);
 }
diff --git a/src/nouveau/vulkan/nvk_cmd_meta.c b/src/nouveau/vulkan/nvk_cmd_meta.c
index 39135ae596770..e038743125c1d 100644
--- a/src/nouveau/vulkan/nvk_cmd_meta.c
+++ b/src/nouveau/vulkan/nvk_cmd_meta.c
@@ -60,7 +60,7 @@ struct nvk_meta_save {
    struct vk_vertex_input_state _dynamic_vi;
    struct vk_sample_locations_state _dynamic_sl;
    struct vk_dynamic_graphics_state dynamic;
-   struct nvk_graphics_pipeline *pipeline;
+   struct nvk_shader *shaders[MESA_SHADER_MESH + 1];
    struct nvk_addr_range vb0;
    struct nvk_descriptor_set *desc0;
    bool has_push_desc0;
@@ -76,7 +76,9 @@ nvk_meta_begin(struct nvk_cmd_buffer *cmd,
    save->_dynamic_vi = cmd->state.gfx._dynamic_vi;
    save->_dynamic_sl = cmd->state.gfx._dynamic_sl;
 
-   save->pipeline = cmd->state.gfx.pipeline;
+   STATIC_ASSERT(sizeof(cmd->state.gfx.shaders) == sizeof(save->shaders));
+   memcpy(save->shaders, cmd->state.gfx.shaders, sizeof(save->shaders));
+
    save->vb0 = cmd->state.gfx.vb0;
 
    save->desc0 = cmd->state.gfx.descriptors.sets[0];
@@ -148,8 +150,12 @@ nvk_meta_end(struct nvk_cmd_buffer *cmd,
           cmd->vk.dynamic_graphics_state.set,
           sizeof(cmd->vk.dynamic_graphics_state.set));
 
-   if (save->pipeline)
-      nvk_cmd_bind_graphics_pipeline(cmd, save->pipeline);
+   for (uint32_t stage = 0; stage < ARRAY_SIZE(save->shaders); stage++) {
+      if (stage == MESA_SHADER_COMPUTE)
+         continue;
+
+      nvk_cmd_bind_graphics_shader(cmd, stage, save->shaders[stage]);
+   }
 
    nvk_cmd_bind_vertex_buffer(cmd, 0, save->vb0);
 
diff --git a/src/nouveau/vulkan/nvk_device.c b/src/nouveau/vulkan/nvk_device.c
index f2c266e89f311..1639c5ce217ab 100644
--- a/src/nouveau/vulkan/nvk_device.c
+++ b/src/nouveau/vulkan/nvk_device.c
@@ -8,6 +8,7 @@
 #include "nvk_entrypoints.h"
 #include "nvk_instance.h"
 #include "nvk_physical_device.h"
+#include "nvk_shader.h"
 
 #include "vk_pipeline_cache.h"
 #include "vulkan/wsi/wsi_common.h"
@@ -146,6 +147,8 @@ nvk_CreateDevice(VkPhysicalDevice physicalDevice,
    if (result != VK_SUCCESS)
       goto fail_alloc;
 
+   dev->vk.shader_ops = &nvk_device_shader_ops;
+
    drmDevicePtr drm_device = NULL;
    int ret = drmGetDeviceFromDevId(pdev->render_dev, 0, &drm_device);
    if (ret != 0) {
diff --git a/src/nouveau/vulkan/nvk_graphics_pipeline.c b/src/nouveau/vulkan/nvk_graphics_pipeline.c
deleted file mode 100644
index a705a89f05c2c..0000000000000
--- a/src/nouveau/vulkan/nvk_graphics_pipeline.c
+++ /dev/null
@@ -1,417 +0,0 @@
-/*
- * Copyright © 2022 Collabora Ltd. and Red Hat Inc.
- * SPDX-License-Identifier: MIT
- */
-#include "nvk_pipeline.h"
-
-#include "nvk_device.h"
-#include "nvk_mme.h"
-#include "nvk_physical_device.h"
-#include "nvk_shader.h"
-
-#include "vk_nir.h"
-#include "vk_pipeline.h"
-#include "vk_pipeline_layout.h"
-
-#include "nv_push.h"
-
-#include "nouveau_context.h"
-
-#include "compiler/spirv/nir_spirv.h"
-
-#include "nvk_cl9097.h"
-#include "nvk_clb197.h"
-#include "nvk_clc397.h"
-
-static void
-emit_pipeline_xfb_state(struct nv_push *p, const struct nak_xfb_info *xfb)
-{
-   for (uint8_t b = 0; b < ARRAY_SIZE(xfb->attr_count); b++) {
-      const uint8_t attr_count = xfb->attr_count[b];
-      P_MTHD(p, NV9097, SET_STREAM_OUT_CONTROL_STREAM(b));
-      P_NV9097_SET_STREAM_OUT_CONTROL_STREAM(p, b, xfb->stream[b]);
-      P_NV9097_SET_STREAM_OUT_CONTROL_COMPONENT_COUNT(p, b, attr_count);
-      P_NV9097_SET_STREAM_OUT_CONTROL_STRIDE(p, b, xfb->stride[b]);
-
-      /* upload packed varying indices in multiples of 4 bytes */
-      const uint32_t n = DIV_ROUND_UP(attr_count, 4);
-      if (n > 0) {
-         P_MTHD(p, NV9097, SET_STREAM_OUT_LAYOUT_SELECT(b, 0));
-         P_INLINE_ARRAY(p, (const uint32_t*)xfb->attr_index[b], n);
-      }
-   }
-}
-
-static const uint32_t mesa_to_nv9097_shader_type[] = {
-   [MESA_SHADER_VERTEX]    = NV9097_SET_PIPELINE_SHADER_TYPE_VERTEX,
-   [MESA_SHADER_TESS_CTRL] = NV9097_SET_PIPELINE_SHADER_TYPE_TESSELLATION_INIT,
-   [MESA_SHADER_TESS_EVAL] = NV9097_SET_PIPELINE_SHADER_TYPE_TESSELLATION,
-   [MESA_SHADER_GEOMETRY]  = NV9097_SET_PIPELINE_SHADER_TYPE_GEOMETRY,
-   [MESA_SHADER_FRAGMENT]  = NV9097_SET_PIPELINE_SHADER_TYPE_PIXEL,
-};
-
-static void
-merge_tess_info(struct shader_info *tes_info, struct shader_info *tcs_info)
-{
-   /* The Vulkan 1.0.38 spec, section 21.1 Tessellator says:
-    *
-    *    "PointMode. Controls generation of points rather than triangles
-    *     or lines. This functionality defaults to disabled, and is
-    *     enabled if either shader stage includes the execution mode.
-    *
-    * and about Triangles, Quads, IsoLines, VertexOrderCw, VertexOrderCcw,
-    * PointMode, SpacingEqual, SpacingFractionalEven, SpacingFractionalOdd,
-    * and OutputVertices, it says:
-    *
-    *    "One mode must be set in at least one of the tessellation
-    *     shader stages."
-    *
-    * So, the fields can be set in either the TCS or TES, but they must
-    * agree if set in both.  Our backend looks at TES, so bitwise-or in
-    * the values from the TCS.
-    */
-   assert(tcs_info->tess.tcs_vertices_out == 0 || tes_info->tess.tcs_vertices_out == 0 ||
-          tcs_info->tess.tcs_vertices_out == tes_info->tess.tcs_vertices_out);
-   tes_info->tess.tcs_vertices_out |= tcs_info->tess.tcs_vertices_out;
-
-   assert(tcs_info->tess.spacing == TESS_SPACING_UNSPECIFIED ||
-          tes_info->tess.spacing == TESS_SPACING_UNSPECIFIED ||
-          tcs_info->tess.spacing == tes_info->tess.spacing);
-   tes_info->tess.spacing |= tcs_info->tess.spacing;
-
-   assert(tcs_info->tess._primitive_mode == TESS_PRIMITIVE_UNSPECIFIED ||
-          tes_info->tess._primitive_mode == TESS_PRIMITIVE_UNSPECIFIED ||
-          tcs_info->tess._primitive_mode == tes_info->tess._primitive_mode);
-   tes_info->tess._primitive_mode |= tcs_info->tess._primitive_mode;
-   tes_info->tess.ccw |= tcs_info->tess.ccw;
-   tes_info->tess.point_mode |= tcs_info->tess.point_mode;
-
-   /* Copy the merged info back to the TCS */
-   tcs_info->tess.tcs_vertices_out = tes_info->tess.tcs_vertices_out;
-   tcs_info->tess.spacing = tes_info->tess.spacing;
-   tcs_info->tess._primitive_mode = tes_info->tess._primitive_mode;
-   tcs_info->tess.ccw = tes_info->tess.ccw;
-   tcs_info->tess.point_mode = tes_info->tess.point_mode;
-}
-
-VkResult
-nvk_graphics_pipeline_create(struct nvk_device *dev,
-                             struct vk_pipeline_cache *cache,
-                             const VkGraphicsPipelineCreateInfo *pCreateInfo,
-                             const VkAllocationCallbacks *pAllocator,
-                             VkPipeline *pPipeline)
-{
-   VK_FROM_HANDLE(vk_pipeline_layout, pipeline_layout, pCreateInfo->layout);
-   struct nvk_graphics_pipeline *pipeline;
-   VkResult result = VK_SUCCESS;
-
-   pipeline = (void *)nvk_pipeline_zalloc(dev, NVK_PIPELINE_GRAPHICS,
-                                          sizeof(*pipeline), pAllocator);
-   if (pipeline == NULL)
-      return vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
-
-   VkPipelineCreateFlags2KHR pipeline_flags =
-      vk_graphics_pipeline_create_flags(pCreateInfo);
-
-   if (pipeline_flags &
-       VK_PIPELINE_CREATE_2_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_KHR)
-      cache = NULL;
-
-   struct vk_graphics_pipeline_all_state all;
-   struct vk_graphics_pipeline_state state = {};
-   result = vk_graphics_pipeline_state_fill(&dev->vk, &state, pCreateInfo,
-                                            NULL, 0, &all, NULL, 0, NULL);
-   assert(result == VK_SUCCESS);
-
-   VkPipelineCreationFeedbackEXT pipeline_feedback = {
-      .flags = VK_PIPELINE_CREATION_FEEDBACK_VALID_BIT,
-   };
-   VkPipelineCreationFeedbackEXT stage_feedbacks[MESA_SHADER_STAGES] = { 0 };
-
-   int64_t pipeline_start = os_time_get_nano();
-
-   const VkPipelineCreationFeedbackCreateInfo *creation_feedback =
-         vk_find_struct_const(pCreateInfo->pNext,
-                              PIPELINE_CREATION_FEEDBACK_CREATE_INFO);
-
-   const VkPipelineShaderStageCreateInfo *infos[MESA_SHADER_STAGES] = {};
-   nir_shader *nir[MESA_SHADER_STAGES] = {};
-   struct vk_pipeline_robustness_state robustness[MESA_SHADER_STAGES];
-
-   struct vk_pipeline_cache_object *cache_objs[MESA_SHADER_STAGES] = {};
-
-   struct nak_fs_key fs_key_tmp, *fs_key = NULL;
-   nvk_populate_fs_key(&fs_key_tmp, &state);
-   fs_key = &fs_key_tmp;
-
-   for (uint32_t i = 0; i < pCreateInfo->stageCount; i++) {
-      const VkPipelineShaderStageCreateInfo *sinfo = &pCreateInfo->pStages[i];
-      gl_shader_stage stage = vk_to_mesa_shader_stage(sinfo->stage);
-      infos[stage] = sinfo;
-   }
-
-   for (gl_shader_stage stage = 0; stage < MESA_SHADER_STAGES; stage++) {
-      const VkPipelineShaderStageCreateInfo *sinfo = infos[stage];
-      if (sinfo == NULL)
-         continue;
-
-      vk_pipeline_robustness_state_fill(&dev->vk, &robustness[stage],
-                                        pCreateInfo->pNext, sinfo->pNext);
-   }
-
-   for (gl_shader_stage stage = 0; stage < MESA_SHADER_STAGES; stage++) {
-      const VkPipelineShaderStageCreateInfo *sinfo = infos[stage];
-      if (sinfo == NULL)
-         continue;
-
-      unsigned char sha1[SHA1_DIGEST_LENGTH];
-      nvk_hash_shader(sha1, sinfo, &robustness[stage],
-                      state.rp->view_mask != 0, pipeline_layout,
-                      stage == MESA_SHADER_FRAGMENT ? fs_key : NULL);
-
-      if (cache) {
-         bool cache_hit = false;
-         cache_objs[stage] = vk_pipeline_cache_lookup_object(cache, &sha1, sizeof(sha1),
-                                                             &nvk_shader_ops, &cache_hit);
-         pipeline->base.shaders[stage] =
-            container_of(cache_objs[stage], struct nvk_shader, base);
-
-         if (cache_hit && cache != dev->mem_cache)
-            pipeline_feedback.flags |=
-               VK_PIPELINE_CREATION_FEEDBACK_APPLICATION_PIPELINE_CACHE_HIT_BIT;
-      }
-
-      if (!cache_objs[stage] &&
-          pCreateInfo->flags & VK_PIPELINE_CREATE_2_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT_KHR) {
-         result = VK_PIPELINE_COMPILE_REQUIRED;
-         goto fail;
-      }
-   }
-
-   for (gl_shader_stage stage = 0; stage < MESA_SHADER_STAGES; stage++) {
-      const VkPipelineShaderStageCreateInfo *sinfo = infos[stage];
-      if (sinfo == NULL || cache_objs[stage])
-         continue;
-
-      result = nvk_shader_stage_to_nir(dev, sinfo, &robustness[stage],
-                                       cache, NULL, &nir[stage]);
-      if (result != VK_SUCCESS)
-         goto fail;
-   }
-
-   if (nir[MESA_SHADER_TESS_CTRL] && nir[MESA_SHADER_TESS_EVAL]) {
-      merge_tess_info(&nir[MESA_SHADER_TESS_EVAL]->info, &nir[MESA_SHADER_TESS_CTRL]->info);
-   }
-
-   for (gl_shader_stage stage = 0; stage < MESA_SHADER_STAGES; stage++) {
-      const VkPipelineShaderStageCreateInfo *sinfo = infos[stage];
-      if (sinfo == NULL)
-         continue;
-
-      if (!cache_objs[stage]) {
-         int64_t stage_start = os_time_get_nano();
-
-         unsigned char sha1[SHA1_DIGEST_LENGTH];
-         nvk_hash_shader(sha1, sinfo, &robustness[stage],
-                         state.rp->view_mask != 0, pipeline_layout,
-                         stage == MESA_SHADER_FRAGMENT ? fs_key : NULL);
-
-         struct nvk_shader *shader = nvk_shader_init(dev, sha1, SHA1_DIGEST_LENGTH);
-         if(shader == NULL) {
-            result = vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
-            goto fail;
-         }
-
-         nvk_lower_nir(dev, nir[stage], &robustness[stage],
-                       state.rp->view_mask != 0,
-                       pipeline_layout->set_count,
-                       pipeline_layout->set_layouts,
-                       &shader->cbuf_map);
-
-         result = nvk_compile_nir(dev, nir[stage],
-                                  pipeline_flags, &robustness[stage],
-                                  stage == MESA_SHADER_FRAGMENT ? fs_key : NULL,
-                                  cache, shader);
-
-         if (result == VK_SUCCESS) {
-            cache_objs[stage] = &shader->base;
-
-            if (cache)
-               cache_objs[stage] = vk_pipeline_cache_add_object(cache,
-                                                                cache_objs[stage]);
-
-            stage_feedbacks[stage].flags = VK_PIPELINE_CREATION_FEEDBACK_VALID_BIT;
-            pipeline->base.shaders[stage] =
-               container_of(cache_objs[stage], struct nvk_shader, base);
-         }
-
-         stage_feedbacks[stage].duration += os_time_get_nano() - stage_start;
-         ralloc_free(nir[stage]);
-      }
-
-      if (result != VK_SUCCESS)
-         goto fail;
-
-      result = nvk_shader_upload(dev, pipeline->base.shaders[stage]);
-      if (result != VK_SUCCESS)
-         goto fail;
-   }
-
-   struct nv_push push;
-   nv_push_init(&push, pipeline->push_data, ARRAY_SIZE(pipeline->push_data));
-   struct nv_push *p = &push;
-
-   bool force_max_samples = false;
-
-   struct nvk_shader *last_geom = NULL;
-   for (gl_shader_stage stage = 0; stage <= MESA_SHADER_FRAGMENT; stage++) {
-      struct nvk_shader *shader = pipeline->base.shaders[stage];
-      uint32_t idx = mesa_to_nv9097_shader_type[stage];
-
-      P_IMMD(p, NV9097, SET_PIPELINE_SHADER(idx), {
-         .enable  = nvk_shader_is_enabled(shader),
-         .type    = mesa_to_nv9097_shader_type[stage],
-      });
-
-      if (!nvk_shader_is_enabled(shader))
-         continue;
-
-      if (stage != MESA_SHADER_FRAGMENT)
-         last_geom = shader;
-
-      uint64_t addr = shader->hdr_addr;
-      if (dev->pdev->info.cls_eng3d >= VOLTA_A) {
-         P_MTHD(p, NVC397, SET_PIPELINE_PROGRAM_ADDRESS_A(idx));
-         P_NVC397_SET_PIPELINE_PROGRAM_ADDRESS_A(p, idx, addr >> 32);
-         P_NVC397_SET_PIPELINE_PROGRAM_ADDRESS_B(p, idx, addr);
-      } else {
-         assert(addr < 0xffffffff);
-         P_IMMD(p, NV9097, SET_PIPELINE_PROGRAM(idx), addr);
-      }
-
-      P_MTHD(p, NVC397, SET_PIPELINE_REGISTER_COUNT(idx));
-      P_NVC397_SET_PIPELINE_REGISTER_COUNT(p, idx, shader->info.num_gprs);
-      P_NVC397_SET_PIPELINE_BINDING(p, idx, nvk_cbuf_binding_for_stage(stage));
-
-      switch (stage) {
-      case MESA_SHADER_VERTEX:
-      case MESA_SHADER_GEOMETRY:
-      case MESA_SHADER_TESS_CTRL:
-      case MESA_SHADER_TESS_EVAL:
-         break;
-
-      case MESA_SHADER_FRAGMENT:
-         P_IMMD(p, NV9097, SET_SUBTILING_PERF_KNOB_A, {
-            .fraction_of_spm_register_file_per_subtile         = 0x10,
-            .fraction_of_spm_pixel_output_buffer_per_subtile   = 0x40,
-            .fraction_of_spm_triangle_ram_per_subtile          = 0x16,
-            .fraction_of_max_quads_per_subtile                 = 0x20,
-         });
-         P_NV9097_SET_SUBTILING_PERF_KNOB_B(p, 0x20);
-
-         P_IMMD(p, NV9097, SET_API_MANDATED_EARLY_Z,
-                shader->info.fs.early_fragment_tests);
-
-         if (dev->pdev->info.cls_eng3d >= MAXWELL_B) {
-            P_IMMD(p, NVB197, SET_POST_Z_PS_IMASK,
-                   shader->info.fs.post_depth_coverage);
-         } else {
-            assert(!shader->info.fs.post_depth_coverage);
-         }
-
-         P_IMMD(p, NV9097, SET_ZCULL_BOUNDS, {
-            .z_min_unbounded_enable = shader->info.fs.writes_depth,
-            .z_max_unbounded_enable = shader->info.fs.writes_depth,
-         });
-
-         /* If we're using the incoming sample mask and doing sample shading,
-          * we have to do sample shading "to the max", otherwise there's no
-          * way to tell which sets of samples are covered by the current
-          * invocation.
-          */
-         force_max_samples = shader->info.fs.reads_sample_mask ||
-                             shader->info.fs.uses_sample_shading;
-         break;
-
-      default:
-         unreachable("Unsupported shader stage");
-      }
-   }
-
-   const uint8_t clip_cull = last_geom->info.vtg.clip_enable |
-                             last_geom->info.vtg.cull_enable;
-   if (clip_cull) {
-      P_IMMD(p, NV9097, SET_USER_CLIP_ENABLE, {
-         .plane0 = (clip_cull >> 0) & 1,
-         .plane1 = (clip_cull >> 1) & 1,
-         .plane2 = (clip_cull >> 2) & 1,
-         .plane3 = (clip_cull >> 3) & 1,
-         .plane4 = (clip_cull >> 4) & 1,
-         .plane5 = (clip_cull >> 5) & 1,
-         .plane6 = (clip_cull >> 6) & 1,
-         .plane7 = (clip_cull >> 7) & 1,
-      });
-      P_IMMD(p, NV9097, SET_USER_CLIP_OP, {
-         .plane0 = (last_geom->info.vtg.cull_enable >> 0) & 1,
-         .plane1 = (last_geom->info.vtg.cull_enable >> 1) & 1,
-         .plane2 = (last_geom->info.vtg.cull_enable >> 2) & 1,
-         .plane3 = (last_geom->info.vtg.cull_enable >> 3) & 1,
-         .plane4 = (last_geom->info.vtg.cull_enable >> 4) & 1,
-         .plane5 = (last_geom->info.vtg.cull_enable >> 5) & 1,
-         .plane6 = (last_geom->info.vtg.cull_enable >> 6) & 1,
-         .plane7 = (last_geom->info.vtg.cull_enable >> 7) & 1,
-      });
-   }
-
-   /* TODO: prog_selects_layer */
-   P_IMMD(p, NV9097, SET_RT_LAYER, {
-      .v       = 0,
-      .control = last_geom->info.vtg.writes_layer ?
-                 CONTROL_GEOMETRY_SHADER_SELECTS_LAYER :
-                 CONTROL_V_SELECTS_LAYER,
-   });
-
-   P_IMMD(p, NV9097, SET_ATTRIBUTE_POINT_SIZE, {
-      .enable  = last_geom->info.vtg.writes_point_size,
-      .slot    = 0,
-   });
-
-   emit_pipeline_xfb_state(&push, &last_geom->info.vtg.xfb);
-
-   pipeline->push_dw_count = nv_push_dw_count(&push);
-
-   if (force_max_samples)
-      pipeline->min_sample_shading = 1;
-   else if (state.ms != NULL && state.ms->sample_shading_enable)
-      pipeline->min_sample_shading = CLAMP(state.ms->min_sample_shading, 0, 1);
-   else
-      pipeline->min_sample_shading = 0;
-
-   pipeline->dynamic.vi = &pipeline->_dynamic_vi;
-   pipeline->dynamic.ms.sample_locations = &pipeline->_dynamic_sl;
-   vk_dynamic_graphics_state_fill(&pipeline->dynamic, &state);
-
-   pipeline_feedback.duration = os_time_get_nano() - pipeline_start;
-   if (creation_feedback) {
-      *creation_feedback->pPipelineCreationFeedback = pipeline_feedback;
-
-      int fb_count = creation_feedback->pipelineStageCreationFeedbackCount;
-      if (pCreateInfo->stageCount == fb_count) {
-         for (uint32_t i = 0; i < pCreateInfo->stageCount; i++) {
-            const VkPipelineShaderStageCreateInfo *sinfo =
-               &pCreateInfo->pStages[i];
-            gl_shader_stage stage = vk_to_mesa_shader_stage(sinfo->stage);
-            creation_feedback->pPipelineStageCreationFeedbacks[i] =
-               stage_feedbacks[stage];
-         }
-      }
-   }
-
-   *pPipeline = nvk_pipeline_to_handle(&pipeline->base);
-
-   return VK_SUCCESS;
-
-fail:
-   vk_object_free(&dev->vk, pAllocator, pipeline);
-   return result;
-}
diff --git a/src/nouveau/vulkan/nvk_pipeline.h b/src/nouveau/vulkan/nvk_pipeline.h
deleted file mode 100644
index a68b353a9c81f..0000000000000
--- a/src/nouveau/vulkan/nvk_pipeline.h
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Copyright © 2022 Collabora Ltd. and Red Hat Inc.
- * SPDX-License-Identifier: MIT
- */
-#ifndef NVK_PIPELINE_H
-#define NVK_PIPELINE_H 1
-
-#include "nvk_private.h"
-#include "nvk_shader.h"
-
-#include "vk_graphics_state.h"
-#include "vk_object.h"
-
-struct vk_pipeline_cache;
-
-enum nvk_pipeline_type {
-   NVK_PIPELINE_GRAPHICS,
-   NVK_PIPELINE_COMPUTE,
-};
-
-struct nvk_pipeline {
-   struct vk_object_base base;
-
-   enum nvk_pipeline_type type;
-
-   struct nvk_shader *shaders[MESA_SHADER_STAGES];
-};
-
-VK_DEFINE_NONDISP_HANDLE_CASTS(nvk_pipeline, base, VkPipeline,
-                               VK_OBJECT_TYPE_PIPELINE)
-
-void
-nvk_pipeline_free(struct nvk_device *dev,
-                  struct nvk_pipeline *pipeline,
-                  const VkAllocationCallbacks *pAllocator);
-struct nvk_pipeline *
-nvk_pipeline_zalloc(struct nvk_device *dev,
-                    enum nvk_pipeline_type type, size_t size,
-                    const VkAllocationCallbacks *pAllocator);
-
-struct nvk_compute_pipeline {
-   struct nvk_pipeline base;
-
-   uint32_t qmd_template[64];
-};
-
-VkResult
-nvk_compute_pipeline_create(struct nvk_device *dev,
-                            struct vk_pipeline_cache *cache,
-                            const VkComputePipelineCreateInfo *pCreateInfo,
-                            const VkAllocationCallbacks *pAllocator,
-                            VkPipeline *pPipeline);
-
-struct nvk_graphics_pipeline {
-   struct nvk_pipeline base;
-
-   uint32_t push_data[192];
-   uint32_t push_dw_count;
-
-   float min_sample_shading;
-
-   struct vk_vertex_input_state _dynamic_vi;
-   struct vk_sample_locations_state _dynamic_sl;
-   struct vk_dynamic_graphics_state dynamic;
-};
-
-VkResult
-nvk_graphics_pipeline_create(struct nvk_device *dev,
-                             struct vk_pipeline_cache *cache,
-                             const VkGraphicsPipelineCreateInfo *pCreateInfo,
-                             const VkAllocationCallbacks *pAllocator,
-                             VkPipeline *pPipeline);
-
-#endif
diff --git a/src/nouveau/vulkan/nvk_query_pool.c b/src/nouveau/vulkan/nvk_query_pool.c
index e9c6cf40e8bcc..eeef9761c9e51 100644
--- a/src/nouveau/vulkan/nvk_query_pool.c
+++ b/src/nouveau/vulkan/nvk_query_pool.c
@@ -11,7 +11,6 @@
 #include "nvk_event.h"
 #include "nvk_mme.h"
 #include "nvk_physical_device.h"
-#include "nvk_pipeline.h"
 
 #include "vk_meta.h"
 #include "vk_pipeline.h"
@@ -973,12 +972,13 @@ nvk_meta_copy_query_pool_results(struct nvk_cmd_buffer *cmd,
    }
 
    /* Save pipeline and push constants */
-   struct nvk_compute_pipeline *pipeline_save = cmd->state.cs.pipeline;
+   struct nvk_shader *shader_save = cmd->state.cs.shader;
    uint8_t push_save[NVK_MAX_PUSH_SIZE];
    memcpy(push_save, desc->root.push, NVK_MAX_PUSH_SIZE);
 
-   nvk_CmdBindPipeline(nvk_cmd_buffer_to_handle(cmd),
-                       VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
+   dev->vk.dispatch_table.CmdBindPipeline(nvk_cmd_buffer_to_handle(cmd),
+                                          VK_PIPELINE_BIND_POINT_COMPUTE,
+                                          pipeline);
 
    nvk_CmdPushConstants(nvk_cmd_buffer_to_handle(cmd), layout,
                         VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(push), &push);
@@ -986,11 +986,8 @@ nvk_meta_copy_query_pool_results(struct nvk_cmd_buffer *cmd,
    nvk_CmdDispatchBase(nvk_cmd_buffer_to_handle(cmd), 0, 0, 0, 1, 1, 1);
 
    /* Restore pipeline and push constants */
-   if (pipeline_save) {
-      nvk_CmdBindPipeline(nvk_cmd_buffer_to_handle(cmd),
-                          VK_PIPELINE_BIND_POINT_COMPUTE,
-                          nvk_pipeline_to_handle(&pipeline_save->base));
-   }
+   if (shader_save)
+      nvk_cmd_bind_compute_shader(cmd, shader_save);
    memcpy(desc->root.push, push_save, NVK_MAX_PUSH_SIZE);
 }
 
diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index b2b70011a7843..181205d9ba57e 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -8,12 +8,11 @@
 #include "nvk_descriptor_set_layout.h"
 #include "nvk_device.h"
 #include "nvk_physical_device.h"
-#include "nvk_pipeline.h"
 #include "nvk_sampler.h"
+#include "nvk_shader.h"
 
 #include "vk_nir_convert_ycbcr.h"
 #include "vk_pipeline.h"
-#include "vk_pipeline_cache.h"
 #include "vk_pipeline_layout.h"
 #include "vk_shader_module.h"
 #include "vk_ycbcr_conversion.h"
@@ -186,7 +185,7 @@ nvk_preprocess_nir(struct vk_physical_device *vk_pdev, nir_shader *nir)
       nvk_cg_preprocess_nir(nir);
 }
 
-void
+static void
 nvk_populate_fs_key(struct nak_fs_key *key,
                     const struct vk_graphics_pipeline_state *state)
 {
@@ -195,6 +194,9 @@ nvk_populate_fs_key(struct nak_fs_key *key,
    key->sample_locations_cb = 0;
    key->sample_locations_offset = nvk_root_descriptor_offset(draw.sample_locations);
 
+   if (state == NULL)
+      return;
+
    if (state->pipeline_flags &
        VK_PIPELINE_CREATE_2_DEPTH_STENCIL_ATTACHMENT_FEEDBACK_LOOP_BIT_EXT)
       key->zs_self_dep = true;
@@ -208,6 +210,25 @@ nvk_populate_fs_key(struct nak_fs_key *key,
       key->force_sample_shading = true;
 }
 
+static void
+nvk_hash_graphics_state(struct vk_physical_device *device,
+                        const struct vk_graphics_pipeline_state *state,
+                        VkShaderStageFlags stages,
+                        blake3_hash blake3_out)
+{
+   struct mesa_blake3 blake3_ctx;
+   _mesa_blake3_init(&blake3_ctx);
+   if (stages & VK_SHADER_STAGE_FRAGMENT_BIT) {
+      struct nak_fs_key key;
+      nvk_populate_fs_key(&key, state);
+      _mesa_blake3_update(&blake3_ctx, &key, sizeof(key));
+
+      const bool is_multiview = state->rp->view_mask != 0;
+      _mesa_blake3_update(&blake3_ctx, &is_multiview, sizeof(is_multiview));
+   }
+   _mesa_blake3_final(&blake3_ctx, blake3_out);
+}
+
 static bool
 lower_load_global_constant_offset_instr(nir_builder *b,
                                         nir_intrinsic_instr *intrin,
@@ -290,52 +311,6 @@ lookup_ycbcr_conversion(const void *_state, uint32_t set,
           &sampler->vk.ycbcr_conversion->state : NULL;
 }
 
-VkResult
-nvk_shader_stage_to_nir(struct nvk_device *dev,
-                        const VkPipelineShaderStageCreateInfo *sinfo,
-                        const struct vk_pipeline_robustness_state *rstate,
-                        struct vk_pipeline_cache *cache,
-                        void *mem_ctx, struct nir_shader **nir_out)
-{
-   struct nvk_physical_device *pdev = nvk_device_physical(dev);
-   const gl_shader_stage stage = vk_to_mesa_shader_stage(sinfo->stage);
-   const nir_shader_compiler_options *nir_options =
-      nvk_get_nir_options(&pdev->vk, stage, rstate);
-
-   unsigned char stage_sha1[SHA1_DIGEST_LENGTH];
-   vk_pipeline_hash_shader_stage(sinfo, rstate, stage_sha1);
-
-   if (cache == NULL)
-      cache = dev->mem_cache;
-
-   nir_shader *nir = vk_pipeline_cache_lookup_nir(cache, stage_sha1,
-                                                  sizeof(stage_sha1),
-                                                  nir_options, NULL,
-                                                  mem_ctx);
-   if (nir != NULL) {
-      *nir_out = nir;
-      return VK_SUCCESS;
-   }
-
-   const struct spirv_to_nir_options spirv_options =
-      nvk_get_spirv_options(&pdev->vk, stage, rstate);
-
-   VkResult result = vk_pipeline_shader_stage_to_nir(&dev->vk, sinfo,
-                                                     &spirv_options,
-                                                     nir_options,
-                                                     mem_ctx, &nir);
-   if (result != VK_SUCCESS)
-      return result;
-
-   nvk_preprocess_nir(&dev->pdev->vk, nir);
-
-   vk_pipeline_cache_add_nir(cache, stage_sha1, sizeof(stage_sha1), nir);
-
-   *nir_out = nir;
-
-   return VK_SUCCESS;
-}
-
 static inline bool
 nir_has_image_var(nir_shader *nir)
 {
@@ -493,13 +468,13 @@ nvk_shader_dump(struct nvk_shader *shader)
 static VkResult
 nvk_compile_nir_with_nak(struct nvk_physical_device *pdev,
                          nir_shader *nir,
-                         VkPipelineCreateFlagBits2KHR pipeline_flags,
+                         VkShaderCreateFlagsEXT shader_flags,
                          const struct vk_pipeline_robustness_state *rs,
                          const struct nak_fs_key *fs_key,
                          struct nvk_shader *shader)
 {
    const bool dump_asm =
-      pipeline_flags & VK_PIPELINE_CREATE_2_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_KHR;
+      shader_flags & VK_SHADER_CREATE_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_MESA;
 
    nir_variable_mode robust2_modes = 0;
    if (rs->uniform_buffers == VK_PIPELINE_ROBUSTNESS_BUFFER_BEHAVIOR_ROBUST_BUFFER_ACCESS_2_EXT)
@@ -515,38 +490,18 @@ nvk_compile_nir_with_nak(struct nvk_physical_device *pdev,
    return VK_SUCCESS;
 }
 
-struct nvk_shader *
-nvk_shader_init(struct nvk_device *dev, const void *key_data, size_t key_size)
-{
-   VK_MULTIALLOC(ma);
-   VK_MULTIALLOC_DECL(&ma, struct nvk_shader, shader, 1);
-   VK_MULTIALLOC_DECL_SIZE(&ma, char, obj_key_data, key_size);
-
-   if (!vk_multialloc_zalloc(&ma, &dev->vk.alloc,
-                             VK_SYSTEM_ALLOCATION_SCOPE_DEVICE))
-      return NULL;
-
-   memcpy(obj_key_data, key_data, key_size);
-
-   vk_pipeline_cache_object_init(&dev->vk, &shader->base,
-                                 &nvk_shader_ops, obj_key_data, key_size);
-
-   return shader;
-}
-
-VkResult
+static VkResult
 nvk_compile_nir(struct nvk_device *dev, nir_shader *nir,
-                VkPipelineCreateFlagBits2KHR pipeline_flags,
+                VkShaderCreateFlagsEXT shader_flags,
                 const struct vk_pipeline_robustness_state *rs,
                 const struct nak_fs_key *fs_key,
-                struct vk_pipeline_cache *cache,
                 struct nvk_shader *shader)
 {
    struct nvk_physical_device *pdev = nvk_device_physical(dev);
    VkResult result;
 
    if (use_nak(pdev, nir->info.stage)) {
-      result = nvk_compile_nir_with_nak(pdev, nir, pipeline_flags, rs,
+      result = nvk_compile_nir_with_nak(pdev, nir, shader_flags, rs,
                                        fs_key, shader);
    } else {
       result = nvk_cg_compile_nir(pdev, nir, fs_key, shader);
@@ -555,7 +510,7 @@ nvk_compile_nir(struct nvk_device *dev, nir_shader *nir,
       return result;
 
    if (nir->constant_data_size > 0) {
-      uint32_t data_align = nvk_min_cbuf_alignment(&dev->pdev->info);
+      uint32_t data_align = nvk_min_cbuf_alignment(&pdev->info);
       uint32_t data_size = align(nir->constant_data_size, data_align);
 
       void *data = malloc(data_size);
@@ -650,11 +605,15 @@ nvk_shader_upload(struct nvk_device *dev, struct nvk_shader *shader)
    return result;
 }
 
-void
-nvk_shader_finish(struct nvk_device *dev, struct nvk_shader *shader)
+static const struct vk_shader_ops nvk_shader_ops;
+
+static void
+nvk_shader_destroy(struct vk_device *vk_dev,
+                   struct vk_shader *vk_shader,
+                   const VkAllocationCallbacks* pAllocator)
 {
-   if (shader == NULL)
-      return;
+   struct nvk_device *dev = container_of(vk_dev, struct nvk_device, vk);
+   struct nvk_shader *shader = container_of(vk_shader, struct nvk_shader, vk);
 
    if (shader->upload_size > 0) {
       nvk_heap_free(dev, &dev->shader_heap,
@@ -671,127 +630,330 @@ nvk_shader_finish(struct nvk_device *dev, struct nvk_shader *shader)
 
    free((void *)shader->data_ptr);
 
-   vk_free(&dev->vk.alloc, shader);
+   vk_shader_free(&dev->vk, pAllocator, &shader->vk);
 }
 
-void
-nvk_hash_shader(unsigned char *hash,
-                const VkPipelineShaderStageCreateInfo *sinfo,
-                const struct vk_pipeline_robustness_state *rs,
-                bool is_multiview,
-                const struct vk_pipeline_layout *layout,
-                const struct nak_fs_key *fs_key)
+static VkResult
+nvk_compile_shader(struct nvk_device *dev,
+                   struct vk_shader_compile_info *info,
+                   const struct vk_graphics_pipeline_state *state,
+                   const VkAllocationCallbacks* pAllocator,
+                   struct vk_shader **shader_out)
 {
-   struct mesa_sha1 ctx;
+   struct nvk_shader *shader;
+   VkResult result;
+
+   /* We consume the NIR, regardless of success or failure */
+   nir_shader *nir = info->nir;
+
+   shader = vk_shader_zalloc(&dev->vk, &nvk_shader_ops, info->stage,
+                             pAllocator, sizeof(*shader));
+   if (shader == NULL) {
+      ralloc_free(nir);
+      return vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
+   }
 
-   _mesa_sha1_init(&ctx);
+   /* TODO: Multiview with ESO */
+   const bool is_multiview = state && state->rp->view_mask != 0;
 
-   unsigned char stage_sha1[SHA1_DIGEST_LENGTH];
-   vk_pipeline_hash_shader_stage(sinfo, rs, stage_sha1);
+   nvk_lower_nir(dev, nir, info->robustness, is_multiview,
+                 info->set_layout_count, info->set_layouts,
+                 &shader->cbuf_map);
 
-   _mesa_sha1_update(&ctx, stage_sha1, sizeof(stage_sha1));
+   struct nak_fs_key fs_key_tmp, *fs_key = NULL;
+   if (nir->info.stage == MESA_SHADER_FRAGMENT) {
+      nvk_populate_fs_key(&fs_key_tmp, state);
+      fs_key = &fs_key_tmp;
+   }
 
-   _mesa_sha1_update(&ctx, &is_multiview, sizeof(is_multiview));
+   result = nvk_compile_nir(dev, nir, info->flags, info->robustness,
+                            fs_key, shader);
+   ralloc_free(nir);
+   if (result != VK_SUCCESS) {
+      nvk_shader_destroy(&dev->vk, &shader->vk, pAllocator);
+      return result;
+   }
 
-   if (layout) {
-      _mesa_sha1_update(&ctx, &layout->create_flags,
-                        sizeof(layout->create_flags));
-      _mesa_sha1_update(&ctx, &layout->set_count, sizeof(layout->set_count));
-      for (int i = 0; i < layout->set_count; i++) {
-         struct nvk_descriptor_set_layout *set =
-            vk_to_nvk_descriptor_set_layout(layout->set_layouts[i]);
-         _mesa_sha1_update(&ctx, &set->vk.blake3, sizeof(set->vk.blake3));
+   result = nvk_shader_upload(dev, shader);
+   if (result != VK_SUCCESS) {
+      nvk_shader_destroy(&dev->vk, &shader->vk, pAllocator);
+      return result;
+   }
+
+   if (info->stage == MESA_SHADER_FRAGMENT) {
+      if (shader->info.fs.reads_sample_mask ||
+          shader->info.fs.uses_sample_shading) {
+         shader->min_sample_shading = 1;
+      } else if (state != NULL && state->ms != NULL &&
+                 state->ms->sample_shading_enable) {
+         shader->min_sample_shading =
+            CLAMP(state->ms->min_sample_shading, 0, 1);
+      } else {
+         shader->min_sample_shading = 0;
       }
    }
 
-   if(fs_key)
-      _mesa_sha1_update(&ctx, fs_key, sizeof(*fs_key));
+   *shader_out = &shader->vk;
 
-   _mesa_sha1_final(&ctx, hash);
+   return VK_SUCCESS;
 }
 
-static bool
-nvk_shader_serialize(struct vk_pipeline_cache_object *object,
-                     struct blob *blob);
+static VkResult
+nvk_compile_shaders(struct vk_device *vk_dev,
+                    uint32_t shader_count,
+                    struct vk_shader_compile_info *infos,
+                    const struct vk_graphics_pipeline_state *state,
+                    const VkAllocationCallbacks* pAllocator,
+                    struct vk_shader **shaders_out)
+{
+   struct nvk_device *dev = container_of(vk_dev, struct nvk_device, vk);
 
-static struct vk_pipeline_cache_object *
-nvk_shader_deserialize(struct vk_pipeline_cache *cache,
-                       const void *key_data,
-                       size_t key_size,
-                       struct blob_reader *blob);
+   for (uint32_t i = 0; i < shader_count; i++) {
+      VkResult result = nvk_compile_shader(dev, &infos[i], state,
+                                           pAllocator, &shaders_out[i]);
+      if (result != VK_SUCCESS) {
+         /* Clean up all the shaders before this point */
+         for (uint32_t j = 0; j < i; j++)
+            nvk_shader_destroy(&dev->vk, shaders_out[j], pAllocator);
 
-void
-nvk_shader_destroy(struct vk_device *_dev,
-                   struct vk_pipeline_cache_object *object)
-{
-   struct nvk_device *dev =
-      container_of(_dev, struct nvk_device, vk);
-   struct nvk_shader *shader =
-      container_of(object, struct nvk_shader, base);
+         /* Clean up all the NIR after this point */
+         for (uint32_t j = i + 1; j < shader_count; j++)
+            ralloc_free(infos[j].nir);
+
+         /* Memset the output array */
+         memset(shaders_out, 0, shader_count * sizeof(*shaders_out));
+
+         return result;
+      }
+   }
 
-   nvk_shader_finish(dev, shader);
+   return VK_SUCCESS;
 }
 
-const struct vk_pipeline_cache_object_ops nvk_shader_ops = {
-   .serialize = nvk_shader_serialize,
-   .deserialize = nvk_shader_deserialize,
-   .destroy = nvk_shader_destroy,
-};
+static VkResult
+nvk_deserialize_shader(struct vk_device *vk_dev,
+                       struct blob_reader *blob,
+                       uint32_t binary_version,
+                       const VkAllocationCallbacks* pAllocator,
+                       struct vk_shader **shader_out)
+{
+   struct nvk_device *dev = container_of(vk_dev, struct nvk_device, vk);
+   struct nvk_shader *shader;
+   VkResult result;
+
+   struct nak_shader_info info;
+   blob_copy_bytes(blob, &info, sizeof(info));
+
+   struct nvk_cbuf_map cbuf_map;
+   blob_copy_bytes(blob, &cbuf_map, sizeof(cbuf_map));
+
+   float min_sample_shading;
+   blob_copy_bytes(blob, &min_sample_shading, sizeof(min_sample_shading));
+
+   const uint32_t code_size = blob_read_uint32(blob);
+   const uint32_t data_size = blob_read_uint32(blob);
+   if (blob->overrun)
+      return vk_error(dev, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+
+   shader = vk_shader_zalloc(&dev->vk, &nvk_shader_ops, info.stage,
+                             pAllocator, sizeof(*shader));
+   if (shader == NULL)
+      return vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
+
+   shader->info = info;
+   shader->cbuf_map = cbuf_map;
+   shader->min_sample_shading = min_sample_shading;
+   shader->code_size = code_size;
+   shader->data_size = data_size;
+
+   shader->code_ptr = malloc(code_size);
+   if (shader->code_ptr == NULL) {
+      nvk_shader_destroy(&dev->vk, &shader->vk, pAllocator);
+      return vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
+   }
+
+   shader->data_ptr = malloc(data_size);
+   if (shader->data_ptr == NULL) {
+      nvk_shader_destroy(&dev->vk, &shader->vk, pAllocator);
+      return vk_error(dev, VK_ERROR_OUT_OF_HOST_MEMORY);
+   }
+
+   blob_copy_bytes(blob, (void *)shader->code_ptr, shader->code_size);
+   blob_copy_bytes(blob, (void *)shader->data_ptr, shader->data_size);
+   if (blob->overrun) {
+      nvk_shader_destroy(&dev->vk, &shader->vk, pAllocator);
+      return vk_error(dev, VK_ERROR_INCOMPATIBLE_SHADER_BINARY_EXT);
+   }
+
+   result = nvk_shader_upload(dev, shader);
+   if (result != VK_SUCCESS) {
+      nvk_shader_destroy(&dev->vk, &shader->vk, pAllocator);
+      return result;
+   }
+
+   *shader_out = &shader->vk;
+
+   return VK_SUCCESS;
+}
 
 static bool
-nvk_shader_serialize(struct vk_pipeline_cache_object *object,
+nvk_shader_serialize(struct vk_device *vk_dev,
+                     const struct vk_shader *vk_shader,
                      struct blob *blob)
 {
-   struct nvk_shader *shader =
-      container_of(object, struct nvk_shader, base);
+   struct nvk_shader *shader = container_of(vk_shader, struct nvk_shader, vk);
+
+   /* We can't currently cache assmbly */
+   if (shader->nak != NULL && shader->nak->asm_str != NULL)
+      return false;
 
    blob_write_bytes(blob, &shader->info, sizeof(shader->info));
    blob_write_bytes(blob, &shader->cbuf_map, sizeof(shader->cbuf_map));
+   blob_write_bytes(blob, &shader->min_sample_shading,
+                    sizeof(shader->min_sample_shading));
+
    blob_write_uint32(blob, shader->code_size);
-   blob_write_bytes(blob, shader->code_ptr, shader->code_size);
    blob_write_uint32(blob, shader->data_size);
+   blob_write_bytes(blob, shader->code_ptr, shader->code_size);
    blob_write_bytes(blob, shader->data_ptr, shader->data_size);
 
-   return true;
+   return !blob->out_of_memory;
 }
 
-static struct vk_pipeline_cache_object *
-nvk_shader_deserialize(struct vk_pipeline_cache *cache,
-                       const void *key_data,
-                       size_t key_size,
-                       struct blob_reader *blob)
+#define WRITE_STR(field, ...) ({                               \
+   memset(field, 0, sizeof(field));                            \
+   UNUSED int i = snprintf(field, sizeof(field), __VA_ARGS__); \
+   assert(i > 0 && i < sizeof(field));                         \
+})
+
+static VkResult
+nvk_shader_get_executable_properties(
+   UNUSED struct vk_device *device,
+   const struct vk_shader *vk_shader,
+   uint32_t *executable_count,
+   VkPipelineExecutablePropertiesKHR *properties)
 {
-   struct nvk_device *dev =
-      container_of(cache->base.device, struct nvk_device, vk);
-   struct nvk_shader *shader =
-      nvk_shader_init(dev, key_data, key_size);
+   struct nvk_shader *shader = container_of(vk_shader, struct nvk_shader, vk);
+   VK_OUTARRAY_MAKE_TYPED(VkPipelineExecutablePropertiesKHR, out,
+                          properties, executable_count);
+
+   vk_outarray_append_typed(VkPipelineExecutablePropertiesKHR, &out, props) {
+      props->stages = mesa_to_vk_shader_stage(shader->info.stage);
+      props->subgroupSize = 32;
+      WRITE_STR(props->name, "%s",
+                _mesa_shader_stage_to_string(shader->info.stage));
+      WRITE_STR(props->description, "%s shader",
+                _mesa_shader_stage_to_string(shader->info.stage));
+   }
 
-   if (!shader)
-      return NULL;
+   return vk_outarray_status(&out);
+}
+
+static VkResult
+nvk_shader_get_executable_statistics(
+   UNUSED struct vk_device *device,
+   const struct vk_shader *vk_shader,
+   uint32_t executable_index,
+   uint32_t *statistic_count,
+   VkPipelineExecutableStatisticKHR *statistics)
+{
+   struct nvk_shader *shader = container_of(vk_shader, struct nvk_shader, vk);
+   VK_OUTARRAY_MAKE_TYPED(VkPipelineExecutableStatisticKHR, out,
+                          statistics, statistic_count);
+
+   assert(executable_index == 0);
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Code Size");
+      WRITE_STR(stat->description,
+                "Size of the compiled shader binary, in bytes");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->code_size;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "Number of GPRs");
+      WRITE_STR(stat->description, "Number of GPRs used by this pipeline");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.num_gprs;
+   }
+
+   vk_outarray_append_typed(VkPipelineExecutableStatisticKHR, &out, stat) {
+      WRITE_STR(stat->name, "SLM Size");
+      WRITE_STR(stat->description,
+                "Size of shader local (scratch) memory, in bytes");
+      stat->format = VK_PIPELINE_EXECUTABLE_STATISTIC_FORMAT_UINT64_KHR;
+      stat->value.u64 = shader->info.slm_size;
+   }
+
+   return vk_outarray_status(&out);
+}
 
-   blob_copy_bytes(blob, &shader->info, sizeof(shader->info));
-   blob_copy_bytes(blob, &shader->cbuf_map, sizeof(shader->cbuf_map));
+static bool
+write_ir_text(VkPipelineExecutableInternalRepresentationKHR* ir,
+              const char *data)
+{
+   ir->isText = VK_TRUE;
 
-   shader->code_size = blob_read_uint32(blob);
-   void *code_ptr = malloc(shader->code_size);
-   if (!code_ptr)
-      goto fail;
+   size_t data_len = strlen(data) + 1;
 
-   blob_copy_bytes(blob, code_ptr, shader->code_size);
-   shader->code_ptr = code_ptr;
+   if (ir->pData == NULL) {
+      ir->dataSize = data_len;
+      return true;
+   }
 
-   shader->data_size = blob_read_uint32(blob);
-   void *data_ptr = malloc(shader->data_size);
-   if (!data_ptr)
-      goto fail;
+   strncpy(ir->pData, data, ir->dataSize);
+   if (ir->dataSize < data_len)
+      return false;
 
-   blob_copy_bytes(blob, data_ptr, shader->data_size);
-   shader->data_ptr = data_ptr;
+   ir->dataSize = data_len;
+   return true;
+}
 
-   return &shader->base;
+static VkResult
+nvk_shader_get_executable_internal_representations(
+   UNUSED struct vk_device *device,
+   const struct vk_shader *vk_shader,
+   uint32_t executable_index,
+   uint32_t *internal_representation_count,
+   VkPipelineExecutableInternalRepresentationKHR *internal_representations)
+{
+   struct nvk_shader *shader = container_of(vk_shader, struct nvk_shader, vk);
+   VK_OUTARRAY_MAKE_TYPED(VkPipelineExecutableInternalRepresentationKHR, out,
+                          internal_representations,
+                          internal_representation_count);
+   bool incomplete_text = false;
+
+   assert(executable_index == 0);
+
+   if (shader->nak != NULL && shader->nak->asm_str != NULL) {
+      vk_outarray_append_typed(VkPipelineExecutableInternalRepresentationKHR, &out, ir) {
+         WRITE_STR(ir->name, "NAK assembly");
+         WRITE_STR(ir->description, "NAK assembly");
+         if (!write_ir_text(ir, shader->nak->asm_str))
+            incomplete_text = true;
+      }
+   }
 
-fail:
-   /* nvk_shader_destroy frees both shader and shader->xfb */
-   nvk_shader_destroy(cache->base.device, &shader->base);
-   return NULL;
+   return incomplete_text ? VK_INCOMPLETE : vk_outarray_status(&out);
 }
+
+static const struct vk_shader_ops nvk_shader_ops = {
+   .destroy = nvk_shader_destroy,
+   .serialize = nvk_shader_serialize,
+   .get_executable_properties = nvk_shader_get_executable_properties,
+   .get_executable_statistics = nvk_shader_get_executable_statistics,
+   .get_executable_internal_representations =
+      nvk_shader_get_executable_internal_representations,
+};
+
+const struct vk_device_shader_ops nvk_device_shader_ops = {
+   .get_nir_options = nvk_get_nir_options,
+   .get_spirv_options = nvk_get_spirv_options,
+   .preprocess_nir = nvk_preprocess_nir,
+   .hash_graphics_state = nvk_hash_graphics_state,
+   .compile = nvk_compile_shaders,
+   .deserialize = nvk_deserialize_shader,
+   .cmd_set_dynamic_graphics_state = vk_cmd_set_dynamic_graphics_state,
+   .cmd_bind_shaders = nvk_cmd_bind_shaders,
+};
diff --git a/src/nouveau/vulkan/nvk_shader.h b/src/nouveau/vulkan/nvk_shader.h
index 08128bb95bceb..9e6e93e6f9500 100644
--- a/src/nouveau/vulkan/nvk_shader.h
+++ b/src/nouveau/vulkan/nvk_shader.h
@@ -14,6 +14,8 @@
 #include "nir.h"
 #include "nouveau_bo.h"
 
+#include "vk_shader.h"
+
 struct nak_shader_bin;
 struct nvk_device;
 struct nvk_physical_device;
@@ -57,11 +59,14 @@ struct nvk_cbuf_map {
 };
 
 struct nvk_shader {
-   struct vk_pipeline_cache_object base;
+   struct vk_shader vk;
 
    struct nak_shader_info info;
    struct nvk_cbuf_map cbuf_map;
 
+   /* Only relevant for fragment shaders */
+   float min_sample_shading;
+
    struct nak_shader_bin *nak;
    const void *code_ptr;
    uint32_t code_size;
@@ -84,11 +89,7 @@ struct nvk_shader {
    uint64_t data_addr;
 };
 
-static inline bool
-nvk_shader_is_enabled(const struct nvk_shader *shader)
-{
-   return shader && shader->upload_size > 0;
-}
+extern const struct vk_device_shader_ops nvk_device_shader_ops;
 
 VkShaderStageFlags nvk_nak_stages(const struct nv_device_info *info);
 
@@ -115,18 +116,6 @@ nvk_nir_lower_descriptors(nir_shader *nir,
                           uint32_t set_layout_count,
                           struct vk_descriptor_set_layout * const *set_layouts,
                           struct nvk_cbuf_map *cbuf_map_out);
-
-VkResult
-nvk_shader_stage_to_nir(struct nvk_device *dev,
-                        const VkPipelineShaderStageCreateInfo *sinfo,
-                        const struct vk_pipeline_robustness_state *rstate,
-                        struct vk_pipeline_cache *cache,
-                        void *mem_ctx, struct nir_shader **nir_out);
-
-void
-nvk_populate_fs_key(struct nak_fs_key *key,
-                    const struct vk_graphics_pipeline_state *state);
-
 void
 nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
               const struct vk_pipeline_robustness_state *rs,
@@ -135,37 +124,9 @@ nvk_lower_nir(struct nvk_device *dev, nir_shader *nir,
               struct vk_descriptor_set_layout * const *set_layouts,
               struct nvk_cbuf_map *cbuf_map_out);
 
-VkResult
-nvk_compile_nir(struct nvk_device *dev, nir_shader *nir,
-                VkPipelineCreateFlagBits2KHR pipeline_flags,
-                const struct vk_pipeline_robustness_state *rstate,
-                const struct nak_fs_key *fs_key,
-                struct vk_pipeline_cache *cache,
-                struct nvk_shader *shader);
-
 VkResult
 nvk_shader_upload(struct nvk_device *dev, struct nvk_shader *shader);
 
-struct nvk_shader *
-nvk_shader_init(struct nvk_device *dev, const void *key_data, size_t key_size);
-
-extern const struct vk_pipeline_cache_object_ops nvk_shader_ops;
-
-void
-nvk_shader_finish(struct nvk_device *dev, struct nvk_shader *shader);
-
-void
-nvk_hash_shader(unsigned char *hash,
-                const VkPipelineShaderStageCreateInfo *sinfo,
-                const struct vk_pipeline_robustness_state *rstate,
-                bool is_multiview,
-                const struct vk_pipeline_layout *layout,
-                const struct nak_fs_key *fs_key);
-
-void
-nvk_shader_destroy(struct vk_device *dev,
-                   struct vk_pipeline_cache_object *object);
-
 /* Codegen wrappers.
  *
  * TODO: Delete these once NAK supports everything.
-- 
GitLab


From 65ffe926cc21c05a662b9f6380b1e0934d7a65b6 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Tue, 9 Jan 2024 18:42:54 -0600
Subject: [PATCH 20/22] nvk: Advertise VK_KHR_graphics_pipeline_library

Closes: https://gitlab.freedesktop.org/mesa/mesa/-/issues/9635
---
 docs/features.txt                        | 2 +-
 src/nouveau/vulkan/nvk_physical_device.c | 8 ++++++++
 2 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/docs/features.txt b/docs/features.txt
index e17976d906450..1b13d7166984c 100644
--- a/docs/features.txt
+++ b/docs/features.txt
@@ -585,7 +585,7 @@ Khronos extensions that are not part of any Vulkan version:
   VK_EXT_fragment_shader_interlock                      DONE (anv, radv/gfx9+, vn)
   VK_EXT_global_priority                                DONE (anv, hasvk, radv, tu)
   VK_EXT_global_priority_query                          DONE (anv, hasvk, radv, tu)
-  VK_EXT_graphics_pipeline_library                      DONE (anv, lvp, radv, tu, vn)
+  VK_EXT_graphics_pipeline_library                      DONE (anv, lvp, nvk, radv, tu, vn)
   VK_EXT_headless_surface                               DONE (anv, dzn, hasvk, lvp, nvk, panvk, pvr, radv, tu, v3dv, vn)
   VK_EXT_image_2d_view_of_3d                            DONE (anv, hasvk, lvp, nvk, radv, tu, vn)
   VK_EXT_image_compression_control                      DONE (radv)
diff --git a/src/nouveau/vulkan/nvk_physical_device.c b/src/nouveau/vulkan/nvk_physical_device.c
index dc79f0fc4fb47..cf6fc0056ed4f 100644
--- a/src/nouveau/vulkan/nvk_physical_device.c
+++ b/src/nouveau/vulkan/nvk_physical_device.c
@@ -177,6 +177,7 @@ nvk_get_device_extensions(const struct nvk_instance *instance,
       .EXT_extended_dynamic_state2 = true,
       .EXT_extended_dynamic_state3 = true,
       .EXT_external_memory_dma_buf = true,
+      .EXT_graphics_pipeline_library = true,
       .EXT_host_query_reset = true,
       .EXT_image_2d_view_of_3d = true,
       .EXT_image_robustness = true,
@@ -464,6 +465,9 @@ nvk_get_device_features(const struct nv_device_info *info,
       .extendedDynamicState3RepresentativeFragmentTestEnable = false,
       .extendedDynamicState3ShadingRateImageEnable = false,
 
+      /* VK_EXT_graphics_pipeline_library */
+      .graphicsPipelineLibrary = true,
+
       /* VK_EXT_image_2d_view_of_3d */
       .image2DViewOf3D = true,
       .sampler2DViewOf3D = true,
@@ -805,6 +809,10 @@ nvk_get_device_properties(const struct nvk_instance *instance,
       /* VK_EXT_extended_dynamic_state3 */
       .dynamicPrimitiveTopologyUnrestricted = true,
 
+      /* VK_EXT_graphics_pipeline_library */
+      .graphicsPipelineLibraryFastLinking = true,
+      .graphicsPipelineLibraryIndependentInterpolationDecoration = true,
+
       /* VK_KHR_line_rasterization */
       .lineSubPixelPrecisionBits = 8,
 
-- 
GitLab


From 723cae3b56c2de59c346fa0f0f8be22e2cafe920 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Mon, 8 Jan 2024 16:23:24 -0600
Subject: [PATCH 21/22] nvk: Advertise VK_EXT_shader_object

Closes: https://gitlab.freedesktop.org/mesa/mesa/-/issues/9648
---
 docs/features.txt                        | 2 +-
 src/nouveau/vulkan/nvk_physical_device.c | 8 ++++++++
 2 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/docs/features.txt b/docs/features.txt
index 1b13d7166984c..f8a5b06574aed 100644
--- a/docs/features.txt
+++ b/docs/features.txt
@@ -618,7 +618,7 @@ Khronos extensions that are not part of any Vulkan version:
   VK_EXT_shader_atomic_float                            DONE (anv, hasvk, lvp, radv)
   VK_EXT_shader_atomic_float2                           DONE (anv, lvp, radv)
   VK_EXT_shader_image_atomic_int64                      DONE (nvk, radv)
-  VK_EXT_shader_object                                  DONE (lvp)
+  VK_EXT_shader_object                                  DONE (lvp, nvk)
   VK_EXT_shader_stencil_export                          DONE (anv, lvp, radv, tu, vn)
   VK_EXT_shader_subgroup_ballot                         DONE (anv, dzn, hasvk, lvp, nvk, radv, vn)
   VK_EXT_shader_subgroup_vote                           DONE (anv, dzn, hasvk, lvp, nvk, radv)
diff --git a/src/nouveau/vulkan/nvk_physical_device.c b/src/nouveau/vulkan/nvk_physical_device.c
index cf6fc0056ed4f..aad9b52e31c35 100644
--- a/src/nouveau/vulkan/nvk_physical_device.c
+++ b/src/nouveau/vulkan/nvk_physical_device.c
@@ -209,6 +209,7 @@ nvk_get_device_extensions(const struct nvk_instance *instance,
                                        nvk_use_nak(info),
       .EXT_shader_demote_to_helper_invocation = true,
       .EXT_shader_module_identifier = true,
+      .EXT_shader_object = true,
       .EXT_shader_subgroup_ballot = true,
       .EXT_shader_subgroup_vote = true,
       .EXT_shader_viewport_index_layer = info->cls_eng3d >= MAXWELL_B,
@@ -527,6 +528,9 @@ nvk_get_device_features(const struct nv_device_info *info,
       /* VK_EXT_shader_module_identifier */
       .shaderModuleIdentifier = true,
 
+      /* VK_EXT_shader_object */
+      .shaderObject = true,
+
       /* VK_EXT_texel_buffer_alignment */
       .texelBufferAlignment = true,
 
@@ -854,6 +858,9 @@ nvk_get_device_properties(const struct nvk_instance *instance,
       .sampleLocationSubPixelBits = 4,
       .variableSampleLocations = true,
 
+      /* VK_EXT_shader_object */
+      .shaderBinaryVersion = 0,
+
       /* VK_EXT_transform_feedback */
       .maxTransformFeedbackStreams = 4,
       .maxTransformFeedbackBuffers = 4,
@@ -924,6 +931,7 @@ nvk_physical_device_init_pipeline_cache(struct nvk_physical_device *pdev)
 
    STATIC_ASSERT(SHA1_DIGEST_LENGTH >= VK_UUID_SIZE);
    memcpy(pdev->vk.properties.pipelineCacheUUID, sha, VK_UUID_SIZE);
+   memcpy(pdev->vk.properties.shaderBinaryUUID, sha, VK_UUID_SIZE);
 
 #ifdef ENABLE_SHADER_CACHE
    char renderer[10];
-- 
GitLab


From 6540561f3ed3df2b1ac018751a8023418d3cc309 Mon Sep 17 00:00:00 2001
From: Faith Ekstrand <faith.ekstrand@collabora.com>
Date: Thu, 11 Jan 2024 19:13:53 -0600
Subject: [PATCH 22/22] WIP: vulkan: Add a lightweight linking mechanism

---
 src/nouveau/vulkan/nvk_shader.c  |  3 ++-
 src/vulkan/runtime/vk_pipeline.c | 21 +++++++++++++++++--
 src/vulkan/runtime/vk_shader.c   | 22 +++++++++++++------
 src/vulkan/runtime/vk_shader.h   | 36 +++++++++++++++++++++++++++++++-
 4 files changed, 72 insertions(+), 10 deletions(-)

diff --git a/src/nouveau/vulkan/nvk_shader.c b/src/nouveau/vulkan/nvk_shader.c
index 181205d9ba57e..12e950564688e 100644
--- a/src/nouveau/vulkan/nvk_shader.c
+++ b/src/nouveau/vulkan/nvk_shader.c
@@ -171,7 +171,8 @@ nvk_get_spirv_options(struct vk_physical_device *vk_pdev,
 }
 
 static void
-nvk_preprocess_nir(struct vk_physical_device *vk_pdev, nir_shader *nir)
+nvk_preprocess_nir(struct vk_physical_device *vk_pdev, nir_shader *nir,
+                   UNUSED struct vk_shader_link_state *link_state)
 {
    const struct nvk_physical_device *pdev =
       container_of(vk_pdev, struct nvk_physical_device, vk);
diff --git a/src/vulkan/runtime/vk_pipeline.c b/src/vulkan/runtime/vk_pipeline.c
index 28c66dd6f0fff..794fc3dd31752 100644
--- a/src/vulkan/runtime/vk_pipeline.c
+++ b/src/vulkan/runtime/vk_pipeline.c
@@ -601,6 +601,8 @@ struct vk_pipeline_precomp_shader {
    /* Tessellation info if the shader is a tessellation shader */
    struct vk_pipeline_tess_info tess;
 
+   struct vk_shader_link_state link_state;
+
    /* Hash of the vk_pipeline_precomp_shader
     *
     * This is the hash of the final compiled NIR together with tess info and
@@ -641,6 +643,7 @@ static struct vk_pipeline_precomp_shader *
 vk_pipeline_precomp_shader_create(struct vk_device *device,
                                   const void *key_data, size_t key_size,
                                   const struct vk_pipeline_robustness_state *rs,
+                                  const struct vk_shader_link_state *link_state,
                                   nir_shader *nir)
 {
    struct blob blob;
@@ -667,6 +670,7 @@ vk_pipeline_precomp_shader_create(struct vk_device *device,
 
    shader->stage = nir->info.stage;
    shader->rs = *rs;
+   shader->link_state = *link_state;
 
    vk_pipeline_gather_nir_tess_info(nir, &shader->tess);
 
@@ -696,6 +700,7 @@ vk_pipeline_precomp_shader_serialize(struct vk_pipeline_cache_object *obj,
    blob_write_uint32(blob, shader->stage);
    blob_write_bytes(blob, &shader->rs, sizeof(shader->rs));
    blob_write_bytes(blob, &shader->tess, sizeof(shader->tess));
+   blob_write_bytes(blob, &shader->link_state, sizeof(shader->link_state));
    blob_write_bytes(blob, shader->blake3, sizeof(shader->blake3));
    blob_write_uint64(blob, shader->nir_blob.size);
    blob_write_bytes(blob, shader->nir_blob.data, shader->nir_blob.size);
@@ -727,6 +732,7 @@ vk_pipeline_precomp_shader_deserialize(struct vk_pipeline_cache *cache,
    shader->stage = blob_read_uint32(blob);
    blob_copy_bytes(blob, &shader->rs, sizeof(shader->rs));
    blob_copy_bytes(blob, &shader->tess, sizeof(shader->tess));
+   blob_copy_bytes(blob, &shader->link_state, sizeof(shader->link_state));
    blob_copy_bytes(blob, shader->blake3, sizeof(shader->blake3));
 
    uint64_t nir_size = blob_read_uint64(blob);
@@ -833,13 +839,14 @@ vk_pipeline_precompile_shader(struct vk_device *device,
    if (result != VK_SUCCESS)
       return result;
 
+   struct vk_shader_link_state link_state = { .data = { 0, } };
    if (ops->preprocess_nir != NULL)
-      ops->preprocess_nir(device->physical, nir);
+      ops->preprocess_nir(device->physical, nir, &link_state);
 
    struct vk_pipeline_precomp_shader *shader =
       vk_pipeline_precomp_shader_create(device, stage_sha1,
                                         sizeof(stage_sha1),
-                                        &rs, nir);
+                                        &rs, &link_state, nir);
    ralloc_free(nir);
    if (shader == NULL)
       return vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
@@ -1108,6 +1115,13 @@ vk_graphics_pipeline_compile_shaders(struct vk_device *device,
       vk_pipeline_tess_info_merge(&tess_info, &tes_precomp->tess);
    }
 
+   struct vk_shader_link_state link_state = { .data = { 0, } };
+   for (uint32_t i = 0; i < stage_count; i++) {
+      struct vk_pipeline_precomp_shader *precomp = stages[i].precomp;
+      for (uint32_t j = 0; j < ARRAY_SIZE(link_state.data); j++)
+         link_state.data[j] |= precomp->link_state.data[j];
+   }
+
    struct mesa_blake3 blake3_ctx;
    _mesa_blake3_init(&blake3_ctx);
    for (uint32_t i = 0; i < pipeline->set_layout_count; i++) {
@@ -1184,6 +1198,8 @@ vk_graphics_pipeline_compile_shaders(struct vk_device *device,
                          VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT))
          _mesa_blake3_update(&blake3_ctx, &tess_info, sizeof(tess_info));
 
+      _mesa_blake3_update(&blake3_ctx, &link_state, sizeof(link_state));
+
       /* The set of geometry stages used together is used to generate the
        * nextStage mask as well as VK_SHADER_CREATE_NO_TASK_SHADER_BIT_EXT.
        */
@@ -1329,6 +1345,7 @@ vk_graphics_pipeline_compile_shaders(struct vk_device *device,
             .next_stage_mask = next_stage,
             .nir = nir,
             .robustness = &stage->precomp->rs,
+            .link_state = &link_state,
             .set_layout_count = pipeline->set_layout_count,
             .set_layouts = pipeline->set_layouts,
             .push_constant_range_count = push_range != NULL,
diff --git a/src/vulkan/runtime/vk_shader.c b/src/vulkan/runtime/vk_shader.c
index 2a52ec372b14e..c99f8c8083677 100644
--- a/src/vulkan/runtime/vk_shader.c
+++ b/src/vulkan/runtime/vk_shader.c
@@ -106,7 +106,8 @@ cmp_stage_idx(const void *_a, const void *_b)
 static nir_shader *
 vk_shader_to_nir(struct vk_device *device,
                  const VkShaderCreateInfoEXT *info,
-                 const struct vk_pipeline_robustness_state *rs)
+                 const struct vk_pipeline_robustness_state *rs,
+                 struct vk_shader_link_state *link_state)
 {
    const struct vk_device_shader_ops *ops = device->shader_ops;
 
@@ -133,7 +134,7 @@ vk_shader_to_nir(struct vk_device *device,
       return NULL;
 
    if (ops->preprocess_nir != NULL)
-      ops->preprocess_nir(device->physical, nir);
+      ops->preprocess_nir(device->physical, nir, link_state);
 
    return nir;
 }
@@ -147,6 +148,7 @@ vk_shader_compile_info_init(struct vk_shader_compile_info *info,
                             struct set_layouts *set_layouts,
                             const VkShaderCreateInfoEXT *vk_info,
                             const struct vk_pipeline_robustness_state *rs,
+                            const struct vk_shader_link_state *link_state,
                             nir_shader *nir)
 {
    for (uint32_t sl = 0; sl < vk_info->setLayoutCount; sl++) {
@@ -160,6 +162,7 @@ vk_shader_compile_info_init(struct vk_shader_compile_info *info,
       .next_stage_mask = vk_info->nextStage,
       .nir = nir,
       .robustness = rs,
+      .link_state = link_state,
       .set_layout_count = vk_info->setLayoutCount,
       .set_layouts = set_layouts->set_layouts,
       .push_constant_range_count = vk_info->pushConstantRangeCount,
@@ -449,7 +452,7 @@ vk_common_CreateShadersEXT(VkDevice _device,
                .idx = i,
             };
          } else {
-            nir_shader *nir = vk_shader_to_nir(device, vk_info, &rs);
+            nir_shader *nir = vk_shader_to_nir(device, vk_info, &rs, NULL);
             if (nir == NULL) {
                result = vk_errorf(device, VK_ERROR_UNKNOWN,
                                   "Failed to compile shader to NIR");
@@ -459,7 +462,7 @@ vk_common_CreateShadersEXT(VkDevice _device,
             struct vk_shader_compile_info info;
             struct set_layouts set_layouts;
             vk_shader_compile_info_init(&info, &set_layouts,
-                                        vk_info, &rs, nir);
+                                        vk_info, &rs, NULL, nir);
 
             struct vk_shader *shader;
             result = ops->compile(device, 1, &info, NULL /* state */,
@@ -491,18 +494,25 @@ vk_common_CreateShadersEXT(VkDevice _device,
       /* Memset for easy error handling */
       memset(infos, 0, sizeof(infos));
 
+      struct vk_shader_link_state link_state = { .data = { 0, } };
+
       for (uint32_t l = 0; l < linked_count; l++) {
          const VkShaderCreateInfoEXT *vk_info = &pCreateInfos[linked[l].idx];
 
-         nir_shader *nir = vk_shader_to_nir(device, vk_info, &rs);
+         struct vk_shader_link_state shader_link_state = { .data = { 0, } };
+         nir_shader *nir = vk_shader_to_nir(device, vk_info, &rs,
+                                            &shader_link_state);
          if (nir == NULL) {
             result = vk_errorf(device, VK_ERROR_UNKNOWN,
                                "Failed to compile shader to NIR");
             break;
          }
 
+         for (uint32_t j = 0; j < ARRAY_SIZE(link_state.data); j++)
+            link_state.data[j] |= shader_link_state.data[j];
+
          vk_shader_compile_info_init(&infos[l], &set_layouts[l],
-                                     vk_info, &rs, nir);
+                                     vk_info, &rs, &link_state, nir);
       }
 
       if (result == VK_SUCCESS) {
diff --git a/src/vulkan/runtime/vk_shader.h b/src/vulkan/runtime/vk_shader.h
index 67f69fff301ed..7ed5fc6aa4c06 100644
--- a/src/vulkan/runtime/vk_shader.h
+++ b/src/vulkan/runtime/vk_shader.h
@@ -49,6 +49,37 @@ int vk_shader_cmp_graphics_stages(gl_shader_stage a, gl_shader_stage b);
 
 #define VK_SHADER_CREATE_CAPTURE_INTERNAL_REPRESENTATIONS_BIT_MESA 0x1000
 
+/** Link state for shader compilation
+ *
+ * The vk_shader_link_state struct provides a very light-weight linking
+ * mechanism even when full linking isn't required or requested.  As part of
+ * the preprocess stage, the driver can output any state it wants to this
+ * opaque blob.  All of the vk_shader_link_states from all of the shaders
+ * involved in the pipeline are then ORed together to form the final link
+ * state that gets passed in via vk_shader_compile_info.
+ *
+ * The advantage of this sort of light-weight link over full linking is that
+ * light-weight linking is still fairly likely to hit the cache with different
+ * combinations of the same shaders.  For instance, if all a fragment shader
+ * needs to know is whether it is used with the classic 3D pipeline vs.
+ * task/mesh (as is the case on Intel), a single bit set by the vertex shader
+ * and a second bit set by the mesh shader are enough to communicate that.
+ * The fragment shader will then be re-usable with any set of legacy
+ * vertex/tessellation/geometry shaders.  A second example is AGX where the
+ * geometry pipeline needs to know the interpolation qualifiers used by the
+ * fragment shader.  In the common case where nothing is flat, the same vertex
+ * shader can be used with any number of fragment shaders.
+ *
+ * It is the responsibility of the driver to gracefully handle missing link
+ * state.  This can be accomplished, for instance, by adding a bitmask to the
+ * top of the link state that contains the set of stages whose data has been
+ * added to it.  If each stage sets its own bit, the final ORed state will
+ * contain a bitmask of available stages.
+ */
+struct vk_shader_link_state {
+   uint64_t data[2];
+};
+
 struct vk_shader_compile_info {
    gl_shader_stage stage;
    VkShaderCreateFlagsEXT flags;
@@ -57,6 +88,8 @@ struct vk_shader_compile_info {
 
    const struct vk_pipeline_robustness_state *robustness;
 
+   const struct vk_shader_link_state *link_state;
+
    uint32_t set_layout_count;
    struct vk_descriptor_set_layout * const *set_layouts;
 
@@ -192,7 +225,8 @@ struct vk_device_shader_ops {
     * not any enabled device features or pipeline state.  This allows us to
     * potentially cache this shader and re-use it across pipelines.
     */
-   void (*preprocess_nir)(struct vk_physical_device *device, nir_shader *nir);
+   void (*preprocess_nir)(struct vk_physical_device *device, nir_shader *nir,
+                          struct vk_shader_link_state *link_state_out);
 
    /** True if the driver wants geometry stages linked
     *
-- 
GitLab

